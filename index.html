<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv For Reinfocement Learning</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv-Reinforcement Learning
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-27T00:00:00Z">2024-03-27</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mini-Gemini: Mining the Potential of Multi-modality Vision Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18814v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18814v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, Jiaya Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce Mini-Gemini, a simple and effective framework
enhancing multi-modality Vision Language Models (VLMs). Despite the
advancements in VLMs facilitating basic visual dialog and reasoning, a
performance gap persists compared to advanced models like GPT-4 and Gemini. We
try to narrow the gap by mining the potential of VLMs for better performance
and any-to-any workflow from three aspects, i.e., high-resolution visual
tokens, high-quality data, and VLM-guided generation. To enhance visual tokens,
we propose to utilize an additional visual encoder for high-resolution
refinement without increasing the visual token count. We further construct a
high-quality dataset that promotes precise image comprehension and
reasoning-based generation, expanding the operational scope of current VLMs. In
general, Mini-Gemini further mines the potential of VLMs and empowers current
frameworks with image understanding, reasoning, and generation simultaneously.
Mini-Gemini supports a series of dense and MoE Large Language Models (LLMs)
from 2B to 34B. It is demonstrated to achieve leading performance in several
zero-shot benchmarks and even surpasses the developed private models. Code and
models are available at https://github.com/dvlab-research/MiniGemini.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and models are available at
  https://github.com/dvlab-research/MiniGemini</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth
  Estimation <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18807v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18807v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suraj Patni, Aradhye Agarwal, Chetan Arora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the absence of parallax cues, a learning-based single image depth
estimation (SIDE) model relies heavily on shading and contextual cues in the
image. While this simplicity is attractive, it is necessary to train such
models on large and varied datasets, which are difficult to capture. It has
been shown that using embeddings from pre-trained foundational models, such as
CLIP, improves zero shot transfer in several applications. Taking inspiration
from this, in our paper we explore the use of global image priors generated
from a pre-trained ViT model to provide more detailed contextual information.
We argue that the embedding vector from a ViT model, pre-trained on a large
dataset, captures greater relevant information for SIDE than the usual route of
generating pseudo image captions, followed by CLIP based text embeddings. Based
on this idea, we propose a new SIDE model using a diffusion backbone which is
conditioned on ViT embeddings. Our proposed design establishes a new
state-of-the-art (SOTA) for SIDE on NYUv2 dataset, achieving Abs Rel error of
0.059(14% improvement) compared to 0.069 by the current SOTA (VPD). And on
KITTI dataset, achieving Sq Rel error of 0.139 (2% improvement) compared to
0.142 by the current SOTA (GEDepth). For zero-shot transfer with a model
trained on NYUv2, we report mean relative improvement of (20%, 23%, 81%, 25%)
over NeWCRFs on (Sun-RGBD, iBims1, DIODE, HyperSim) datasets, compared to (16%,
18%, 45%, 9%) by ZoeDepth. The code is available at
https://github.com/Aradhye2002/EcoDepth.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long-form factuality in large language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, Cosmo Du, Quoc V. Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often generate content that contains factual
errors when responding to fact-seeking prompts on open-ended topics. To
benchmark a model's long-form factuality in open domains, we first use GPT-4 to
generate LongFact, a prompt set comprising thousands of questions spanning 38
topics. We then propose that LLM agents can be used as automated evaluators for
long-form factuality through a method which we call Search-Augmented Factuality
Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into
a set of individual facts and to evaluate the accuracy of each fact using a
multi-step reasoning process comprising sending search queries to Google Search
and determining whether a fact is supported by the search results. Furthermore,
we propose extending F1 score as an aggregated metric for long-form factuality.
To do so, we balance the percentage of supported facts in a response
(precision) with the percentage of provided facts relative to a hyperparameter
representing a user's preferred response length (recall).
  Empirically, we demonstrate that LLM agents can achieve superhuman rating
performance - on a set of ~16k individual facts, SAFE agrees with crowdsourced
human annotators 72% of the time, and on a random subset of 100 disagreement
cases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times
cheaper than human annotators. We also benchmark thirteen language models on
LongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding
that larger language models generally achieve better long-form factuality.
LongFact, SAFE, and all experimental code are available at
https://github.com/google-deepmind/long-form-factuality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gamba: Marry Gaussian Splatting with Mamba for single view 3D
  reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18795v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18795v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiuhong Shen, Xuanyu Yi, Zike Wu, Pan Zhou, Hanwang Zhang, Shuicheng Yan, Xinchao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We tackle the challenge of efficiently reconstructing a 3D asset from a
single image with growing demands for automated 3D content creation pipelines.
Previous methods primarily rely on Score Distillation Sampling (SDS) and Neural
Radiance Fields (NeRF). Despite their significant success, these approaches
encounter practical limitations due to lengthy optimization and considerable
memory usage. In this report, we introduce Gamba, an end-to-end amortized 3D
reconstruction model from single-view images, emphasizing two main insights:
(1) 3D representation: leveraging a large number of 3D Gaussians for an
efficient 3D Gaussian splatting process; (2) Backbone design: introducing a
Mamba-based sequential network that facilitates context-dependent reasoning and
linear scalability with the sequence (token) length, accommodating a
substantial number of Gaussians. Gamba incorporates significant advancements in
data preprocessing, regularization design, and training methodologies. We
assessed Gamba against existing optimization-based and feed-forward 3D
generation approaches using the real-world scanned OmniObject3D dataset. Here,
Gamba demonstrates competitive generation capabilities, both qualitatively and
quantitatively, while achieving remarkable speed, approximately 0.6 second on a
single NVIDIA A100 GPU.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ImageNet-D: Benchmarking Neural Network Robustness on Diffusion
  Synthetic Object <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18775v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18775v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenshuang Zhang, Fei Pan, Junmo Kim, In So Kweon, Chengzhi Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We establish rigorous benchmarks for visual perception robustness. Synthetic
images such as ImageNet-C, ImageNet-9, and Stylized ImageNet provide specific
type of evaluation over synthetic corruptions, backgrounds, and textures, yet
those robustness benchmarks are restricted in specified variations and have low
synthetic quality. In this work, we introduce generative model as a data source
for synthesizing hard images that benchmark deep models' robustness. Leveraging
diffusion models, we are able to generate images with more diversified
backgrounds, textures, and materials than any prior work, where we term this
benchmark as ImageNet-D. Experimental results show that ImageNet-D results in a
significant accuracy drop to a range of vision models, from the standard ResNet
visual classifier to the latest foundation models like CLIP and MiniGPT-4,
significantly reducing their accuracy by up to 60\%. Our work suggests that
diffusion models can be an effective source to test vision models. The code and
dataset are available at https://github.com/chenshuang-zhang/imagenet_d.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Superior Parallel Big Data Clustering through Competitive Stochastic
  <span class="highlight-title">Sample</span> Size Optimization in Big-means 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rustam Mussabayev, Ravil Mussabayev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel K-means clustering algorithm, an advancement on
the conventional Big-means methodology. The proposed method efficiently
integrates parallel processing, stochastic sampling, and competitive
optimization to create a scalable variant designed for big data applications.
It addresses scalability and computation time challenges typically faced with
traditional techniques. The algorithm adjusts sample sizes dynamically for each
worker during execution, optimizing performance. Data from these sample sizes
are continually analyzed, facilitating the identification of the most efficient
configuration. By incorporating a competitive element among workers using
different sample sizes, efficiency within the Big-means algorithm is further
stimulated. In essence, the algorithm balances computational time and
clustering quality by employing a stochastic, competitive sampling strategy in
a parallel computing setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ModaLink: Unifying Modalities for Efficient Image-to-PointCloud Place
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18762v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18762v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weidong Xie, Lun Luo, Nanfei Ye, Yi Ren, Shaoyi Du, Minhang Wang, Jintao Xu, Rui Ai, Weihao Gu, Xieyuanli Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Place recognition is an important task for robots and autonomous cars to
localize themselves and close loops in pre-built maps. While single-modal
sensor-based methods have shown satisfactory performance, cross-modal place
recognition that retrieving images from a point-cloud database remains a
challenging problem. Current cross-modal methods transform images into 3D
points using depth estimation for modality conversion, which are usually
computationally intensive and need expensive labeled data for depth
supervision. In this work, we introduce a fast and lightweight framework to
encode images and point clouds into place-distinctive descriptors. We propose
an effective Field of View (FoV) transformation module to convert point clouds
into an analogous modality as images. This module eliminates the necessity for
depth estimation and helps subsequent modules achieve real-time performance. We
further design a non-negative factorization-based encoder to extract mutually
consistent semantic features between point clouds and images. This encoder
yields more distinctive global descriptors for retrieval. Experimental results
on the KITTI dataset show that our proposed methods achieve state-of-the-art
performance while running in real time. Additional evaluation on the HAOMO
dataset covering a 17 km trajectory further shows the practical generalization
capabilities. We have released the implementation of our methods as open source
at: https://github.com/haomo-ai/ModaLink.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 11 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detection of subclinical atherosclerosis by image-based deep learning on
  chest x-ray 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guglielmo Gallone, Francesco Iodice, Alberto Presta, Davide Tore, Ovidio de Filippo, Michele Visciano, Carlo Alberto Barbano, Alessandro Serafini, Paola Gorrini, Alessandro Bruno, Walter Grosso Marra, James Hughes, Mario Iannaccone, Paolo Fonio, Attilio Fiandrotti, Alessandro Depaoli, Marco Grangetto, Gaetano Maria de Ferrari, Fabrizio D'Ascenzo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aims. To develop a deep-learning based system for recognition of subclinical
atherosclerosis on a plain frontal chest x-ray. Methods and Results. A
deep-learning algorithm to predict coronary artery calcium (CAC) score (the
AI-CAC model) was developed on 460 chest x-ray (80% training cohort, 20%
internal validation cohort) of primary prevention patients (58.4% male, median
age 63 [51-74] years) with available paired chest x-ray and chest computed
tomography (CT) indicated for any clinical reason and performed within 3
months. The CAC score calculated on chest CT was used as ground truth. The
model was validated on an temporally-independent cohort of 90 patients from the
same institution (external validation). The diagnostic accuracy of the AI-CAC
model assessed by the area under the curve (AUC) was the primary outcome.
Overall, median AI-CAC score was 35 (0-388) and 28.9% patients had no AI-CAC.
AUC of the AI-CAC model to identify a CAC>0 was 0.90 in the internal validation
cohort and 0.77 in the external validation cohort. Sensitivity was consistently
above 92% in both cohorts. In the overall cohort (n=540), among patients with
AI-CAC=0, a single ASCVD event occurred, after 4.3 years. Patients with
AI-CAC>0 had significantly higher Kaplan Meier estimates for ASCVD events
(13.5% vs. 3.4%, log-rank=0.013). Conclusion. The AI-CAC model seems to
accurately detect subclinical atherosclerosis on chest x-ray with elevated
sensitivity, and to predict ASCVD events with elevated negative predictive
value. Adoption of the AI-CAC model to refine CV risk stratification or as an
opportunistic screening tool requires prospective evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to European Heart Journal - Cardiovascular Imaging Added
  also the additional material 44 pages (30 main paper, 14 additional
  material), 14 figures (5 main manuscript, 9 additional material)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Many-Objective Evolutionary Influence Maximization: Balancing Spread,
  Budget, Fairness, and Time <span class="chip">GECCO
  24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18755v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18755v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elia Cunegatti, Leonardo Lucio Custode, Giovanni Iacca
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Influence Maximization (IM) problem seeks to discover the set of nodes in
a graph that can spread the information propagation at most. This problem is
known to be NP-hard, and it is usually studied by maximizing the influence
(spread) and, optionally, optimizing a second objective, such as minimizing the
seed set size or maximizing the influence fairness. However, in many practical
scenarios multiple aspects of the IM problem must be optimized at the same
time. In this work, we propose a first case study where several IM-specific
objective functions, namely budget, fairness, communities, and time, are
optimized on top of the maximization of influence and minimization of the seed
set size. To this aim, we introduce MOEIM (Many-Objective Evolutionary
Algorithm for Influence Maximization) a Multi-Objective Evolutionary Algorithm
(MOEA) based on NSGA-II incorporating graph-aware operators and a smart
initialization. We compare MOEIM in two experimental settings, including a
total of nine graph datasets, two heuristic methods, a related MOEA, and a
state-of-the-art Deep Learning approach. The experiments show that MOEIM
overall outperforms the competitors in most of the tested many-objective
settings. To conclude, we also investigate the correlation between the
objectives, leading to novel insights into the topic. The codebase is available
at https://github.com/eliacunegatti/MOEIM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in Genetic and Evolutionary Computation Conference (GECCO
  24 Companion), July 14 18, 2024, Melbourne, VIC, Australia. ACM, New York,
  NY, USA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding the Learning Dynamics of Alignment with Human Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shawn Im, Yixuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligning large language models (LLMs) with human intentions has become a
critical task for safely deploying models in real-world systems. While existing
alignment approaches have seen empirical success, theoretically understanding
how these methods affect model behavior remains an open question. Our work
provides an initial attempt to theoretically analyze the learning dynamics of
human preference alignment. We formally show how the distribution of preference
datasets influences the rate of model updates and provide rigorous guarantees
on the training accuracy. Our theory also reveals an intricate phenomenon where
the optimization is prone to prioritizing certain behaviors with higher
preference distinguishability. We empirically validate our findings on
contemporary LLMs and alignment tasks, reinforcing our theoretical insights and
shedding light on considerations for future alignment approaches. Disclaimer:
This paper contains potentially offensive text; reader discretion is advised.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Manufacturing Quality Prediction Models through the
  Integration of Explainability Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18731v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18731v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dennis Gross, Helge Spieker, Arnaud Gotlieb, Ricardo Knoblauch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research presents a method that utilizes explainability techniques to
amplify the performance of machine learning (ML) models in forecasting the
quality of milling processes, as demonstrated in this paper through a
manufacturing use case. The methodology entails the initial training of ML
models, followed by a fine-tuning phase where irrelevant features identified
through explainability methods are eliminated. This procedural refinement
results in performance enhancements, paving the way for potential reductions in
manufacturing costs and a better understanding of the trained ML models. This
study highlights the usefulness of explainability techniques in both explaining
and optimizing predictive models in the manufacturing realm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Probabilistic Model Checking of Stochastic <span class="highlight-title">Reinforcement</span> Learning
  Policies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18725v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18725v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dennis Gross, Helge Spieker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a method to verify stochastic reinforcement learning (RL)
policies. This approach is compatible with any RL algorithm as long as the
algorithm and its corresponding environment collectively adhere to the Markov
property. In this setting, the future state of the environment should depend
solely on its current state and the action executed, independent of any
previous states or actions. Our method integrates a verification technique,
referred to as model checking, with RL, leveraging a Markov decision process, a
trained RL policy, and a probabilistic computation tree logic (PCTL) formula to
build a formal model that can be subsequently verified via the model checker
Storm. We demonstrate our method's applicability across multiple benchmarks,
comparing it to baseline methods called deterministic safety estimates and
naive monolithic model checking. Our results show that our method is suited to
verify stochastic RL policies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-Supervised Learning for Deep Causal Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasin Ibrahim, Hermione Warr, Konstantinos Kamnitsas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing models that can answer questions of the form "How would $x$ change
if $y$ had been $z$?" is fundamental for advancing medical image analysis.
Training causal generative models that address such counterfactual questions,
though, currently requires that all relevant variables have been observed and
that corresponding labels are available in training data. However, clinical
data may not have complete records for all patients and state of the art causal
generative models are unable to take full advantage of this. We thus develop,
for the first time, a semi-supervised deep causal generative model that
exploits the causal relationships between variables to maximise the use of all
available data. We explore this in the setting where each sample is either
fully labelled or fully unlabelled, as well as the more clinically realistic
case of having different labels missing for each sample. We leverage techniques
from causal inference to infer missing values and subsequently generate
realistic counterfactuals, even for samples with incomplete labels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Hallucinations in Large Vision-Language Models with
  Instruction Contrastive Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18715v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18715v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xintong Wang, Jingheng Pan, Liang Ding, Chris Biemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (LVLMs) are increasingly adept at generating
contextually detailed and coherent responses from visual inputs. However, their
application in multimodal decision-making and open-ended generation is hindered
by a notable rate of hallucinations, where generated text inaccurately
represents the visual contents. To address this issue, this paper introduces
the Instruction Contrastive Decoding (ICD) method, a novel approach designed to
reduce hallucinations during LVLM inference. Our method is inspired by our
observation that what we call disturbance instructions significantly exacerbate
hallucinations in multimodal fusion modules. ICD contrasts distributions from
standard and instruction disturbance, thereby increasing alignment uncertainty
and effectively subtracting hallucinated concepts from the original
distribution. Through comprehensive experiments on discriminative benchmarks
(POPE and MME) and a generative benchmark (LLaVa-Bench), we demonstrate that
ICD significantly mitigates both object-level and attribute-level
hallucinations. Moreover, our method not only addresses hallucinations but also
significantly enhances the general perception and recognition capabilities of
LVLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAT-NGP : Unleashing Neural Graphics Primitives for Fast Relightable
  Transient-Free 3D reconstruction from Satellite Imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18711v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18711v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Camille Billouard, Dawa Derksen, Emmanuelle Sarrazin, Bruno Vallet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current stereo-vision pipelines produce high accuracy 3D reconstruction when
using multiple pairs or triplets of satellite images. However, these pipelines
are sensitive to the changes between images that can occur as a result of
multi-date acquisitions. Such variations are mainly due to variable shadows,
reflexions and transient objects (cars, vegetation). To take such changes into
account, Neural Radiance Fields (NeRF) have recently been applied to multi-date
satellite imagery. However, Neural methods are very compute-intensive, taking
dozens of hours to learn, compared with minutes for standard stereo-vision
pipelines. Following the ideas of Instant Neural Graphics Primitives we propose
to use an efficient sampling strategy and multi-resolution hash encoding to
accelerate the learning. Our model, Satellite Neural Graphics Primitives
(SAT-NGP) decreases the learning time to 15 minutes while maintaining the
quality of the 3D reconstruction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, 1 table; Accepted to International Geoscience and
  Remote Sensing Symposium (IGARSS) 2024; Code available at
  https://github.com/Ellimac0/SAT-NGP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contrastive Learning with Orthonormal Anchors (CLOA) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huanran Li, Daniel Pimentel-Alarcón
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study focuses on addressing the instability issues prevalent in
contrastive learning, specifically examining the InfoNCE loss function and its
derivatives. We reveal a critical observation that these loss functions exhibit
a restrictive behavior, leading to a convergence phenomenon where embeddings
tend to merge into a singular point. This "over-fusion" effect detrimentally
affects classification accuracy in subsequent supervised-learning tasks.
Through theoretical analysis, we demonstrate that embeddings, when equalized or
confined to a rank-1 linear subspace, represent a local minimum for InfoNCE. In
response to this challenge, our research introduces an innovative strategy that
leverages the same or fewer labeled data than typically used in the fine-tuning
phase. The loss we proposed, Orthonormal Anchor Regression Loss, is designed to
disentangle embedding clusters, significantly enhancing the distinctiveness of
each embedding while simultaneously ensuring their aggregation into dense,
well-defined clusters. Our method demonstrates remarkable improvements with
just a fraction of the conventional label requirements, as evidenced by our
results on CIFAR10 and CIFAR100 datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Annolid: Annotate, Segment, and Track Anything You Need 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18690v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18690v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Yang, Thomas A. Cleland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Annolid is a deep learning-based software package designed for the
segmentation, labeling, and tracking of research targets within video files,
focusing primarily on animal behavior analysis. Based on state-of-the-art
instance segmentation methods, Annolid now harnesses the Cutie video object
segmentation model to achieve resilient, markerless tracking of multiple
animals from single annotated frames, even in environments in which they may be
partially or entirely concealed by environmental features or by one another.
Our integration of Segment Anything and Grounding-DINO strategies additionally
enables the automatic masking and segmentation of recognizable animals and
objects by text command, removing the need for manual annotation. Annolid's
comprehensive approach to object segmentation flexibly accommodates a broad
spectrum of behavior analysis applications, enabling the classification of
diverse behavioral states such as freezing, digging, pup huddling, and social
interactions in addition to the tracking of animals and their body parts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TransFusion: Contrastive Learning with Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huanran Li, Daniel Pimentel-Alarcón
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel framework, TransFusion, designed to make the
process of contrastive learning more analytical and explainable. TransFusion
consists of attention blocks whose softmax being replaced by ReLU, and its
final block's weighted-sum operation is truncated to leave the adjacency matrix
as the output. The model is trained by minimizing the Jensen-Shannon Divergence
between its output and the target affinity matrix, which indicates whether each
pair of samples belongs to the same or different classes. The main contribution
of TransFusion lies in defining a theoretical limit for answering two
fundamental questions in the field: the maximum level of data augmentation and
the minimum batch size required for effective contrastive learning.
Furthermore, experimental results indicate that TransFusion successfully
extracts features that isolate clusters from complex real-world data, leading
to improved classification accuracy in downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 4 figures,</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aiming for Relevance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18668v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18668v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bar Eini Porat, Danny Eytan, Uri Shalit
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vital signs are crucial in intensive care units (ICUs). They are used to
track the patient's state and to identify clinically significant changes.
Predicting vital sign trajectories is valuable for early detection of adverse
events. However, conventional machine learning metrics like RMSE often fail to
capture the true clinical relevance of such predictions. We introduce novel
vital sign prediction performance metrics that align with clinical contexts,
focusing on deviations from clinical norms, overall trends, and trend
deviations. These metrics are derived from empirical utility curves obtained in
a previous study through interviews with ICU clinicians. We validate the
metrics' usefulness using simulated and real clinical datasets (MIMIC and
eICU). Furthermore, we employ these metrics as loss functions for neural
networks, resulting in models that excel in predicting clinically significant
events. This research paves the way for clinically relevant machine learning
model evaluation and optimization, promising to improve ICU patient care. 10
pages, 9 figures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 9 figures, AMIA Informatics 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ INEXA: Interactive and Explainable Process Model Abstraction Through
  Object-Centric Process Mining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18659v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18659v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Janik-Vasily Benzin, Gyunam Park, Juergen Mangler, Stefanie Rinderle-Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Process events are recorded by multiple information systems at different
granularity levels. Based on the resulting event logs, process models are
discovered at different granularity levels, as well. Events stored at a
fine-grained granularity level, for example, may hinder the discovered process
model to be displayed due the high number of resulting model elements. The
discovered process model of a real-world manufacturing process, for example,
consists of 1,489 model elements and over 2,000 arcs. Existing process model
abstraction techniques could help reducing the size of the model, but would
disconnect it from the underlying event log. Existing event abstraction
techniques do neither support the analysis of mixed granularity levels, nor
interactive exploration of a suitable granularity level. To enable the
exploration of discovered process models at different granularity levels, we
propose INEXA, an interactive, explainable process model abstraction method
that keeps the link to the event log. As a starting point, INEXA aggregates
large process models to a "displayable" size, e.g., for the manufacturing use
case to a process model with 58 model elements. Then, the process analyst can
explore granularity levels interactively, while applied abstractions are
automatically traced in the event log for explainability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spikewhisper: Temporal Spike Backdoor Attacks on Federated Neuromorphic
  Learning over Low-power Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18607v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18607v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanqing Fu, Gaolei Li, Jun Wu, Jianhua Li, Xi Lin, Kai Zhou, Yuchen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated neuromorphic learning (FedNL) leverages event-driven spiking neural
networks and federated learning frameworks to effectively execute intelligent
analysis tasks over amounts of distributed low-power devices but also perform
vulnerability to poisoning attacks. The threat of backdoor attacks on
traditional deep neural networks typically comes from time-invariant data.
However, in FedNL, unknown threats may be hidden in time-varying spike signals.
In this paper, we start to explore a novel vulnerability of FedNL-based systems
with the concept of time division multiplexing, termed Spikewhisper, which
allows attackers to evade detection as much as possible, as multiple malicious
clients can imperceptibly poison with different triggers at different
timeslices. In particular, the stealthiness of Spikewhisper is derived from the
time-domain divisibility of global triggers, in which each malicious client
pastes only one local trigger to a certain timeslice in the neuromorphic
sample, and also the polarity and motion of each local trigger can be
configured by attackers. Extensive experiments based on two different
neuromorphic datasets demonstrate that the attack success rate of Spikewispher
is higher than the temporally centralized attacks. Besides, it is validated
that the effect of Spikewispher is sensitive to the trigger duration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAP: Retrieval-Augmented Planner for Adaptive Procedure Planning in
  Instructional Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Zare, Yulei Niu, Hammad Ayyubi, Shih-fu Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Procedure Planning in instructional videos entails generating a sequence of
action steps based on visual observations of the initial and target states.
Despite the rapid progress in this task, there remain several critical
challenges to be solved: (1) Adaptive procedures: Prior works hold an
unrealistic assumption that the number of action steps is known and fixed,
leading to non-generalizable models in real-world scenarios where the sequence
length varies. (2) Temporal relation: Understanding the step temporal relation
knowledge is essential in producing reasonable and executable plans. (3)
Annotation cost: Annotating instructional videos with step-level labels (i.e.,
timestamp) or sequence-level labels (i.e., action category) is demanding and
labor-intensive, limiting its generalizability to large-scale datasets.In this
work, we propose a new and practical setting, called adaptive procedure
planning in instructional videos, where the procedure length is not fixed or
pre-determined. To address these challenges we introduce Retrieval-Augmented
Planner (RAP) model. Specifically, for adaptive procedures, RAP adaptively
determines the conclusion of actions using an auto-regressive model
architecture. For temporal relation, RAP establishes an external memory module
to explicitly retrieve the most relevant state-action pairs from the training
videos and revises the generated procedures. To tackle high annotation cost,
RAP utilizes a weakly-supervised learning manner to expand the training dataset
to other task-relevant, unannotated videos by generating pseudo labels for
action steps. Experiments on CrossTask and COIN benchmarks show the superiority
of RAP over traditional fixed-length models, establishing it as a strong
baseline solution for adaptive procedure planning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 6 figures, 12 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Homogeneous Tokenizer Matters: Homogeneous Visual Tokenizer for Remote
  Sensing Image Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18593v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18593v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Run Shao, Zhaoyang Zhang, Chao Tao, Yunsheng Zhang, Chengli Peng, Haifeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The tokenizer, as one of the fundamental components of large models, has long
been overlooked or even misunderstood in visual tasks. One key factor of the
great comprehension power of the large language model is that natural language
tokenizers utilize meaningful words or subwords as the basic elements of
language. In contrast, mainstream visual tokenizers, represented by patch-based
methods such as Patch Embed, rely on meaningless rectangular patches as basic
elements of vision, which cannot serve as effectively as words or subwords in
language. Starting from the essence of the tokenizer, we defined semantically
independent regions (SIRs) for vision. We designed a simple HOmogeneous visual
tOKenizer: HOOK. HOOK mainly consists of two modules: the Object Perception
Module (OPM) and the Object Vectorization Module (OVM). To achieve homogeneity,
the OPM splits the image into 4*4 pixel seeds and then utilizes the attention
mechanism to perceive SIRs. The OVM employs cross-attention to merge seeds
within the same SIR. To achieve adaptability, the OVM defines a variable number
of learnable vectors as cross-attention queries, allowing for the adjustment of
token quantity. We conducted experiments on the NWPU-RESISC45, WHU-RS19
classification dataset, and GID5 segmentation dataset for sparse and dense
tasks. The results demonstrate that the visual tokens obtained by HOOK
correspond to individual objects, which demonstrates homogeneity. HOOK
outperformed Patch Embed by 6\% and 10\% in the two tasks and achieved
state-of-the-art performance compared to the baselines used for comparison.
Compared to Patch Embed, which requires more than one hundred tokens for one
image, HOOK requires only 6 and 8 tokens for sparse and dense tasks,
respectively, resulting in efficiency improvements of 1.5 to 2.8 times. The
code is available at https://github.com/GeoX-Lab/Hook.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 8 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-Informed Graph Neural Networks for Water Distribution Systems <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18570v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18570v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Inaam Ashraf, Janine Strotherm, Luca Hermes, Barbara Hammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Water distribution systems (WDS) are an integral part of critical
infrastructure which is pivotal to urban development. As 70% of the world's
population will likely live in urban environments in 2050, efficient simulation
and planning tools for WDS play a crucial role in reaching UN's sustainable
developmental goal (SDG) 6 - "Clean water and sanitation for all". In this
realm, we propose a novel and efficient machine learning emulator, more
precisely, a physics-informed deep learning (DL) model, for hydraulic state
estimation in WDS. Using a recursive approach, our model only needs a few graph
convolutional neural network (GCN) layers and employs an innovative algorithm
based on message passing. Unlike conventional machine learning tasks, the model
uses hydraulic principles to infer two additional hydraulic state features in
the process of reconstructing the available ground truth feature in an
unsupervised manner. To the best of our knowledge, this is the first DL
approach to emulate the popular hydraulic simulator EPANET, utilizing no
additional information. Like most DL models and unlike the hydraulic simulator,
our model demonstrates vastly faster emulation times that do not increase
drastically with the size of the WDS. Moreover, we achieve high accuracy on the
ground truth and very similar results compared to the hydraulic simulator as
demonstrated through experiments on five real-world WDS datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of the paper with the same title published at
  Proceedings of the AAAI Conference on Artificial Intelligence 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PDNNet: PDN-Aware GNN-CNN Heterogeneous Network for Dynamic IR Drop
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Zhao, Zhuomin Chai, Xun Jiang, Yibo Lin, Runsheng Wang, Ru Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  IR drop on the power delivery network (PDN) is closely related to PDN's
configuration and cell current consumption. As the integrated circuit (IC)
design is growing larger, dynamic IR drop simulation becomes computationally
unaffordable and machine learning based IR drop prediction has been explored as
a promising solution. Although CNN-based methods have been adapted to IR drop
prediction task in several works, the shortcomings of overlooking PDN
configuration is non-negligible. In this paper, we consider not only how to
properly represent cell-PDN relation, but also how to model IR drop following
its physical nature in the feature aggregation procedure. Thus, we propose a
novel graph structure, PDNGraph, to unify the representations of the PDN
structure and the fine-grained cell-PDN relation. We further propose a
dual-branch heterogeneous network, PDNNet, incorporating two parallel GNN-CNN
branches to favorably capture the above features during the learning process.
Several key designs are presented to make the dynamic IR drop prediction highly
effective and interpretable. We are the first work to apply graph structure to
deep-learning based dynamic IR drop prediction method. Experiments show that
PDNNet outperforms the state-of-the-art CNN-based methods by up to 39.3%
reduction in prediction error and achieves 545x speedup compared to the
commercial tool, which demonstrates the superiority of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Architecture Search for Sentence Classification with BERT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18547v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18547v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Kenneweg, Sarah Schröder, Barbara Hammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre training of language models on large text corpora is common practice in
Natural Language Processing. Following, fine tuning of these models is
performed to achieve the best results on a variety of tasks. In this paper we
question the common practice of only adding a single output layer as a
classification head on top of the network. We perform an AutoML search to find
architectures that outperform the current single layer at only a small compute
cost. We validate our classification architecture on a variety of NLP
benchmarks from the GLUE dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Heatmap-Guided 6-Dof Grasp Detection in Cluttered Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18546v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18546v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siang Chen, Wei Tang, Pengwei Xie, Wenming Yang, Guijin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fast and robust object grasping in clutter is a crucial component of
robotics. Most current works resort to the whole observed point cloud for 6-Dof
grasp generation, ignoring the guidance information excavated from global
semantics, thus limiting high-quality grasp generation and real-time
performance. In this work, we show that the widely used heatmaps are
underestimated in the efficiency of 6-Dof grasp generation. Therefore, we
propose an effective local grasp generator combined with grasp heatmaps as
guidance, which infers in a global-to-local semantic-to-point way.
Specifically, Gaussian encoding and the grid-based strategy are applied to
predict grasp heatmaps as guidance to aggregate local points into graspable
regions and provide global semantic information. Further, a novel non-uniform
anchor sampling mechanism is designed to improve grasp accuracy and diversity.
Benefiting from the high-efficiency encoding in the image space and focusing on
points in local graspable regions, our framework can perform high-quality grasp
detection in real-time and achieve state-of-the-art results. In addition, real
robot experiments demonstrate the effectiveness of our method with a success
rate of 94% and a clutter completion rate of 100%. Our code is available at
https://github.com/THU-VCLab/HGGD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extensive results on GraspNet-1B dataset</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Path Towards Legal Autonomy: An interoperable and explainable approach
  to extracting, transforming, loading and computing legal information using
  large language models, expert systems and Bayesian networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Axel Constant, Hannes Westermann, Bryan Wilson, Alex Kiefer, Ines Hipolito, Sylvain Pronovost, Steven Swanson, Mahault Albarracin, Maxwell J. D. Ramstead
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Legal autonomy - the lawful activity of artificial intelligence agents - can
be achieved in one of two ways. It can be achieved either by imposing
constraints on AI actors such as developers, deployers and users, and on AI
resources such as data, or by imposing constraints on the range and scope of
the impact that AI agents can have on the environment. The latter approach
involves encoding extant rules concerning AI driven devices into the software
of AI agents controlling those devices (e.g., encoding rules about limitations
on zones of operations into the agent software of an autonomous drone device).
This is a challenge since the effectivity of such an approach requires a method
of extracting, loading, transforming and computing legal information that would
be both explainable and legally interoperable, and that would enable AI agents
to reason about the law. In this paper, we sketch a proof of principle for such
a method using large language models (LLMs), expert legal systems known as
legal decision paths, and Bayesian networks. We then show how the proposed
method could be applied to extant regulation in matters of autonomous cars,
such as the California Vehicle Code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Behavior-Based Recommendation System for E-commerce 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18536v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18536v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reza Barzegar Nozari, Mahdi Divsalar, Sepehr Akbarzadeh Abkenar, Mohammadreza Fadavi Amiri, Ali Divsalar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The majority of existing recommender systems rely on user ratings, which are
limited by the lack of user collaboration and the sparsity problem. To address
these issues, this study proposes a behavior-based recommender system that
leverages customers' natural behaviors, such as browsing and clicking, on
e-commerce platforms. The proposed recommendation system involves clustering
active customers, determining neighborhoods, collecting similar users,
calculating product reputation based on similar users, and recommending
high-reputation products. To overcome the complexity of customer behaviors and
traditional clustering methods, an unsupervised clustering approach based on
product categories is developed to enhance the recommendation methodology. This
study makes notable contributions in several aspects. Firstly, a groundbreaking
behavior-based recommendation methodology is developed, incorporating customer
behavior to generate accurate and tailored recommendations leading to improved
customer satisfaction and engagement. Secondly, an original unsupervised
clustering method, focusing on product categories, enables more precise
clustering and facilitates accurate recommendations. Finally, an approach to
determine neighborhoods for active customers within clusters is established,
ensuring grouping of customers with similar behavioral patterns to enhance
recommendation accuracy and relevance. The proposed recommendation methodology
and clustering method contribute to improved recommendation performance,
offering valuable insights for researchers and practitioners in the field of
e-commerce recommendation systems. Additionally, the proposed method
outperforms benchmark methods in experiments conducted using a behavior dataset
from the well-known e-commerce site Alibaba.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Line Search Methods for Large Scale Neural Network Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18519v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18519v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Kenneweg, Tristan Kenneweg, Barbara Hammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent studies, line search methods have shown significant improvements in
the performance of traditional stochastic gradient descent techniques,
eliminating the need for a specific learning rate schedule. In this paper, we
identify existing issues in state-of-the-art line search methods, propose
enhancements, and rigorously evaluate their effectiveness. We test these
methods on larger datasets and more complex data domains than before.
Specifically, we improve the Armijo line search by integrating the momentum
term from ADAM in its search direction, enabling efficient large-scale
training, a task that was previously prone to failure using Armijo line search
methods. Our optimization approach outperforms both the previous Armijo
implementation and tuned learning rate schedules for Adam. Our evaluation
focuses on Transformers and CNNs in the domains of NLP and image data. Our work
is publicly available as a Python package, which provides a hyperparameter free
Pytorch optimizer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Faster Convergence for Transformer Fine-tuning with Line Search Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Kenneweg, Leonardo Galli, Tristan Kenneweg, Barbara Hammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works have shown that line search methods greatly increase performance
of traditional stochastic gradient descent methods on a variety of datasets and
architectures [1], [2]. In this work we succeed in extending line search
methods to the novel and highly popular Transformer architecture and dataset
domains in natural language processing. More specifically, we combine the
Armijo line search with the Adam optimizer and extend it by subdividing the
networks architecture into sensible units and perform the line search
separately on these local units. Our optimization method outperforms the
traditional Adam optimizer and achieves significant performance improvements
for small data sets or small training budgets, while performing equal or better
for other tested cases. Our work is publicly available as a python package,
which provides a hyperparameter-free pytorch optimizer that is compatible with
arbitrary network architectures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Impact of Employing Weather Forecast Data as Input to the Estimation of
  Evapotranspiration by Deep Neural Network Models <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18489v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18489v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro J. Vaz, Gabriela Schütz, Carlos Guerrero, Pedro J. S. Cardoso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reference Evapotranspiration (ET0) is a key parameter for designing smart
irrigation scheduling, since it is related by a coefficient to the water needs
of a crop. The United Nations Food and Agriculture Organization, proposed a
standard method for ET0 computation (FAO56PM), based on the parameterization of
the Penman-Monteith equation, that is widely adopted in the literature. To
compute ET0 using the FAO56-PM method, four main weather parameters are needed:
temperature, humidity, wind, and solar radiation (SR). One way to make daily
ET0 estimations for future days is to use freely available weather forecast
services (WFSs), where many meteorological parameters are estimated up to the
next 15 days. A problem with this method is that currently, SR is not provided
as a free forecast parameter on most of those online services or, normally,
such forecasts present a financial cost penalty. For this reason, several ET0
estimation models using machine and deep learning were developed and presented
in the literature, that use as input features a reduced set of carefully
selected weather parameters, that are compatible with common freely available
WFSs. However, most studies on this topic have only evaluated model performance
using data from weather stations (WSs), without considering the effect of using
weather forecast data. In this study, the performance of authors' previous
models is evaluated when using weather forecast data from two online WFSs, in
the following scenarios: (i) direct ET0 estimation by an ANN model, and (ii)
estimate SR by ANN model, and then use that estimation for ET0 computation,
using the FAO56-PM method. Employing data collected from two WFSs and a WS
located in Vale do Lobo, Portugal, the latter approach achieved the best
result, with a coefficient of determination (R2) ranging between 0.893 and
0.667, when considering forecasts up to 15 days.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A partial version of the work submitted to ESRE/INTERNATIONAL
  CONFERENCE ON ENVIRONMENTAL SCIENCES AND RENEWABLE ENERGY</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthesizing EEG Signals from Event-Related Potential Paradigms with
  Conditional Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guido Klein, Pierre Guetschel, Gianluigi Silvestri, Michael Tangermann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data scarcity in the brain-computer interface field can be alleviated through
the use of generative models, specifically diffusion models. While diffusion
models have previously been successfully applied to electroencephalogram (EEG)
data, existing models lack flexibility w.r.t.~sampling or require alternative
representations of the EEG data. To overcome these limitations, we introduce a
novel approach to conditional diffusion models that utilizes classifier-free
guidance to directly generate subject-, session-, and class-specific EEG data.
In addition to commonly used metrics, domain-specific metrics are employed to
evaluate the specificity of the generated samples. The results indicate that
the proposed model can generate EEG data that resembles real data for each
subject, session, and class.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to 9th Graz BCI conference, 6 pages, 3 figures, first
  figure is split into two subfigures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Density-guided Translator Boosts Synthetic-to-Real Unsupervised Domain
  Adaptive Segmentation of 3D Point Clouds <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18469v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18469v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhimin Yuan, Wankang Zeng, Yanfei Su, Weiquan Liu, Ming Cheng, Yulan Guo, Cheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D synthetic-to-real unsupervised domain adaptive segmentation is crucial to
annotating new domains. Self-training is a competitive approach for this task,
but its performance is limited by different sensor sampling patterns (i.e.,
variations in point density) and incomplete training strategies. In this work,
we propose a density-guided translator (DGT), which translates point density
between domains, and integrates it into a two-stage self-training pipeline
named DGT-ST. First, in contrast to existing works that simultaneously conduct
data generation and feature/output alignment within unstable adversarial
training, we employ the non-learnable DGT to bridge the domain gap at the input
level. Second, to provide a well-initialized model for self-training, we
propose a category-level adversarial network in stage one that utilizes the
prototype to prevent negative transfer. Finally, by leveraging the designs
above, a domain-mixed self-training method with source-aware consistency loss
is proposed in stage two to narrow the domain gap further. Experiments on two
synthetic-to-real segmentation tasks (SynLiDAR $\rightarrow$ semanticKITTI and
SynLiDAR $\rightarrow$ semanticPOSS) demonstrate that DGT-ST outperforms
state-of-the-art methods, achieving 9.4$\%$ and 4.3$\%$ mIoU improvements,
respectively. Code is available at \url{https://github.com/yuan-zm/DGT-ST}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoBOS: Constraint-Based Online Scheduler for Human-Robot Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18459v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18459v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marina Ionova, Jan Kristof Behrens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assembly processes involving humans and robots are challenging scenarios
because the individual activities and access to shared workspace have to be
coordinated. Fixed robot programs leave no room to diverge from a fixed
protocol. Working on such a process can be stressful for the user and lead to
ineffective behavior or failure. We propose a novel approach of online
constraint-based scheduling in a reactive execution control framework
facilitating behavior trees called CoBOS. This allows the robot to adapt to
uncertain events such as delayed activity completions and activity selection
(by the human). The user will experience less stress as the robotic coworkers
adapt their behavior to best complement the human-selected activities to
complete the common task. In addition to the improved working conditions, our
algorithm leads to increased efficiency, even in highly uncertain scenarios. We
evaluate our algorithm using a probabilistic simulation study with 56000
experiments. We outperform all baselines by a margin of 4-10%. Initial real
robot experiments using a Franka Emika Panda robot and human tracking based on
HTC Vive VR gloves look promising.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoRAST: Towards Foundation Model-Powered Correlated Data Analysis in
  Resource-Constrained CPS and IoT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Hu, Jinhang Zuo, Alanis Zhao, Bob Iannucci, Carlee Joe-Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models (FMs) emerge as a promising solution to harness distributed
and diverse environmental data by leveraging prior knowledge to understand the
complicated temporal and spatial correlations within heterogeneous datasets.
Unlike distributed learning frameworks such as federated learning, which often
struggle with multimodal data, FMs can transform diverse inputs into
embeddings. This process facilitates the integration of information from
various modalities and the application of prior learning to new domains.
However, deploying FMs in resource-constrained edge systems poses significant
challenges. To this end, we introduce CoRAST, a novel learning framework that
utilizes FMs for enhanced analysis of distributed, correlated heterogeneous
data. Utilizing a server-based FM, CoRAST can exploit existing environment
information to extract temporal, spatial, and cross-modal correlations among
sensor data. This enables CoRAST to offer context-aware insights for localized
client tasks through FM-powered global representation learning. Our evaluation
on real-world weather dataset demonstrates CoRAST's ability to exploit
correlated heterogeneous data through environmental representation learning to
reduce the forecast errors by up to 50.3% compared to the baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted and to be published in 2024 IEEE International Workshop on
  Foundation Models for Cyber-Physical Systems & Internet of Things (FMSys)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ U-Sketch: An Efficient Approach for Sketch to Image Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilias Mitsouras, Eleftherios Tsonis, Paraskevi Tzouveli, Athanasios Voulodimos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have demonstrated remarkable performance in text-to-image
synthesis, producing realistic and high resolution images that faithfully
adhere to the corresponding text-prompts. Despite their great success, they
still fall behind in sketch-to-image synthesis tasks, where in addition to
text-prompts, the spatial layout of the generated images has to closely follow
the outlines of certain reference sketches. Employing an MLP latent edge
predictor to guide the spatial layout of the synthesized image by predicting
edge maps at each denoising step has been recently proposed. Despite yielding
promising results, the pixel-wise operation of the MLP does not take into
account the spatial layout as a whole, and demands numerous denoising
iterations to produce satisfactory images, leading to time inefficiency. To
this end, we introduce U-Sketch, a framework featuring a U-Net type latent edge
predictor, which is capable of efficiently capturing both local and global
features, as well as spatial correlations between pixels. Moreover, we propose
the addition of a sketch simplification network that offers the user the choice
of preprocessing and simplifying input sketches for enhanced outputs. The
experimental results, corroborated by user feedback, demonstrate that our
proposed U-Net latent edge predictor leads to more realistic results, that are
better aligned with the spatial outlines of the reference sketches, while
drastically reducing the number of required denoising steps and, consequently,
the overall execution time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18421v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18421v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elliot Bolton, Abhinav Venigalla, Michihiro Yasunaga, David Hall, Betty Xiong, Tony Lee, Roxana Daneshjou, Jonathan Frankle, Percy Liang, Michael Carbin, Christopher D. Manning
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Models such as GPT-4 and Med-PaLM 2 have demonstrated impressive performance
on a wide variety of biomedical NLP tasks. However, these models have hundreds
of billions of parameters, are computationally expensive to run, require users
to send their input data over the internet, and are trained on unknown data
sources. Can smaller, more targeted models compete? To address this question,
we build and release BioMedLM, a 2.7 billion parameter GPT-style autoregressive
model trained exclusively on PubMed abstracts and full articles. When
fine-tuned, BioMedLM can produce strong multiple-choice biomedical
question-answering results competitive with much larger models, such as
achieving a score of 57.3% on MedMCQA (dev) and 69.0% on the MMLU Medical
Genetics exam. BioMedLM can also be fine-tuned to produce useful answers to
patient questions on medical topics. This demonstrates that smaller models can
potentially serve as transparent, privacy-preserving, economical and
environmentally friendly foundations for particular NLP applications, such as
in biomedicine. The model is available on the Hugging Face Hub:
https://huggingface.co/stanford-crfm/BioMedLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Channel-ensemble Approach: Unbiased and Low-variance Pseudo-labels is
  Critical for Semi-supervised Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18407v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18407v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Wu, Junbiao Pang, Baochang Zhang, Qingming Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised learning (SSL) is a practical challenge in computer vision.
Pseudo-label (PL) methods, e.g., FixMatch and FreeMatch, obtain the State Of
The Art (SOTA) performances in SSL. These approaches employ a
threshold-to-pseudo-label (T2L) process to generate PLs by truncating the
confidence scores of unlabeled data predicted by the self-training method.
However, self-trained models typically yield biased and high-variance
predictions, especially in the scenarios when a little labeled data are
supplied. To address this issue, we propose a lightweight channel-based
ensemble method to effectively consolidate multiple inferior PLs into the
theoretically guaranteed unbiased and low-variance one. Importantly, our
approach can be readily extended to any SSL framework, such as FixMatch or
FreeMatch. Experimental results demonstrate that our method significantly
outperforms state-of-the-art techniques on CIFAR10/100 in terms of
effectiveness and efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering
  Using a VLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18406v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18406v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonkyun Kim, Changin Choi, Wonseok Lee, Wonjong Rhee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stimulated by the sophisticated reasoning capabilities of recent Large
Language Models (LLMs), a variety of strategies for bridging video modality
have been devised. A prominent strategy involves Video Language Models
(VideoLMs), which train a learnable interface with video data to connect
advanced vision encoders with LLMs. Recently, an alternative strategy has
surfaced, employing readily available foundation models, such as VideoLMs and
LLMs, across multiple stages for modality bridging. In this study, we introduce
a simple yet novel strategy where only a single Vision Language Model (VLM) is
utilized. Our starting point is the plain insight that a video comprises a
series of images, or frames, interwoven with temporal information. The essence
of video comprehension lies in adeptly managing the temporal aspects along with
the spatial details of each frame. Initially, we transform a video into a
single composite image by arranging multiple frames in a grid layout. The
resulting single image is termed as an image grid. This format, while
maintaining the appearance of a solitary image, effectively retains temporal
information within the grid structure. Therefore, the image grid approach
enables direct application of a single high-performance VLM without
necessitating any video-data training. Our extensive experimental analysis
across ten zero-shot video question answering benchmarks, including five
open-ended and five multiple-choice benchmarks, reveals that the proposed Image
Grid Vision Language Model (IG-VLM) surpasses the existing methods in nine out
of ten benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code is available at https://github.com/imagegridworth/IG-VLM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Large Language Models for Relevance Judgments in Legal Case
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18405v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18405v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengjie Ma, Chong Chen, Qi Chu, Jiaxin Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collecting relevant judgments for legal case retrieval is a challenging and
time-consuming task. Accurately judging the relevance between two legal cases
requires a considerable effort to read the lengthy text and a high level of
domain expertise to extract Legal Facts and make juridical judgments. With the
advent of advanced large language models, some recent studies have suggested
that it is promising to use LLMs for relevance judgment. Nonetheless, the
method of employing a general large language model for reliable relevance
judgments in legal case retrieval is yet to be thoroughly explored. To fill
this research gap, we devise a novel few-shot workflow tailored to the relevant
judgment of legal cases. The proposed workflow breaks down the annotation
process into a series of stages, imitating the process employed by human
annotators and enabling a flexible integration of expert reasoning to enhance
the accuracy of relevance judgments. By comparing the relevance judgments of
LLMs and human experts, we empirically show that we can obtain reliable
relevance judgments with the proposed workflow. Furthermore, we demonstrate the
capacity to augment existing legal case retrieval models through the synthesis
of data generated by the large language model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Colour and Brush Stroke Pattern Recognition in Abstract Art using
  Modified Deep Convolutional Generative Adversarial Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18397v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18397v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Srinitish Srinivasan, Varenya Pathak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Abstract Art is an immensely popular, discussed form of art that often has
the ability to depict the emotions of an artist. Many researchers have made
attempts to study abstract art in the form of edge detection, brush stroke and
emotion recognition algorithms using machine and deep learning. This papers
describes the study of a wide distribution of abstract paintings using
Generative Adversarial Neural Networks(GAN). GANs have the ability to learn and
reproduce a distribution enabling researchers and scientists to effectively
explore and study the generated image space. However, the challenge lies in
developing an efficient GAN architecture that overcomes common training
pitfalls. This paper addresses this challenge by introducing a modified-DCGAN
(mDCGAN) specifically designed for high-quality artwork generation. The
approach involves a thorough exploration of the modifications made, delving
into the intricate workings of DCGANs, optimisation techniques, and
regularisation methods aimed at improving stability and realism in art
generation enabling effective study of generated patterns. The proposed mDCGAN
incorporates meticulous adjustments in layer configurations and architectural
choices, offering tailored solutions to the unique demands of art generation
while effectively combating issues like mode collapse and gradient vanishing.
Further this paper explores the generated latent space by performing random
walks to understand vector relationships between brush strokes and colours in
the abstract art space and a statistical analysis of unstable outputs after a
certain period of GAN training and compare its significant difference. These
findings validate the effectiveness of the proposed approach, emphasising its
potential to revolutionise the field of digital art generation and digital art
ecosystem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 5 tables, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FTBC: Forward Temporal Bias Correction for Optimizing ANN-SNN Conversion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18388v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18388v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofeng Wu, Velibor Bojkovic, Bin Gu, Kun Suo, Kai Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking Neural Networks (SNNs) offer a promising avenue for energy-efficient
computing compared with Artificial Neural Networks (ANNs), closely mirroring
biological neural processes. However, this potential comes with inherent
challenges in directly training SNNs through spatio-temporal backpropagation --
stemming from the temporal dynamics of spiking neurons and their discrete
signal processing -- which necessitates alternative ways of training, most
notably through ANN-SNN conversion. In this work, we introduce a lightweight
Forward Temporal Bias Correction (FTBC) technique, aimed at enhancing
conversion accuracy without the computational overhead. We ground our method on
provided theoretical findings that through proper temporal bias calibration the
expected error of ANN-SNN conversion can be reduced to be zero after each time
step. We further propose a heuristic algorithm for finding the temporal bias
only in the forward pass, thus eliminating the computational burden of
backpropagation and we evaluate our method on CIFAR-10/100 and ImageNet
datasets, achieving a notable increase in accuracy on all datasets. Codes are
released at a GitHub repository.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Multi-modal Models are Good Class-Incremental Learners <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18383v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18383v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xusheng Cao, Haori Lu, Linlan Huang, Xialei Liu, Ming-Ming Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In class-incremental learning (CIL) scenarios, the phenomenon of catastrophic
forgetting caused by the classifier's bias towards the current task has long
posed a significant challenge. It is mainly caused by the characteristic of
discriminative models. With the growing popularity of the generative
multi-modal models, we would explore replacing discriminative models with
generative ones for CIL. However, transitioning from discriminative to
generative models requires addressing two key challenges. The primary challenge
lies in transferring the generated textual information into the classification
of distinct categories. Additionally, it requires formulating the task of CIL
within a generative framework. To this end, we propose a novel generative
multi-modal model (GMM) framework for class-incremental learning. Our approach
directly generates labels for images using an adapted generative model. After
obtaining the detailed text, we use a text encoder to extract text features and
employ feature matching to determine the most similar label as the
classification prediction. In the conventional CIL settings, we achieve
significantly better results in long-sequence task scenarios. Under the
Few-shot CIL setting, we have improved by at least 14\% accuracy over all the
current state-of-the-art methods with significantly less forgetting. Our code
is available at \url{https://github.com/DoubleClass/GMM}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Attributed Text Generation of Large Language Models via
  Preference Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18381v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18381v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongfang Li, Zetian Sun, Baotian Hu, Zhenyu Liu, Xinshuo Hu, Xuebo Liu, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have been widely adopted in natural language
processing, yet they face the challenge of generating unreliable content.
Recent works aim to reduce misinformation and hallucinations by resorting to
attribution as a means to provide evidence (i.e., citations). However, current
attribution methods usually focus on the retrieval stage and automatic
evaluation that neglect mirroring the citation mechanisms in human scholarly
writing to bolster credibility. In this paper, we address these challenges by
modelling the attribution task as preference learning and introducing an
Automatic Preference Optimization (APO) framework. First, we create a curated
collection for post-training with 6,330 examples by collecting and filtering
from existing datasets. Second, considering the high cost of labelling
preference data, we further propose an automatic method to synthesize
attribution preference data resulting in 95,263 pairs. Moreover, inspired by
the human citation process, we further propose a progressive preference
optimization method by leveraging fine-grained information. Extensive
experiments on three datasets (i.e., ASQA, StrategyQA, and ELI5) demonstrate
that APO achieves state-of-the-art citation F1 with higher answer quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 15 tables, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IIP-Mixer:Intra-Inter Patch Mixing Architecture for Battery Remaining
  Useful Life Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18379v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18379v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangzai Ye, Li Feng, Jianlan Guo, Yuqiang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately estimating the Remaining Useful Life (RUL) of lithium-ion
batteries is crucial for maintaining the safe and stable operation of
rechargeable battery management systems. However, this task is often
challenging due to the complex temporal dynamics involved. Recently,
attention-based networks, such as Transformers and Informer, have been the
popular architecture in time series forecasting. Despite their effectiveness,
these models with abundant parameters necessitate substantial training time to
unravel temporal patterns. To tackle these challenges, we propose a simple
MLP-Mixer-based architecture named 'Intra-Inter Patch Mixer' (IIP-Mixer), which
is an architecture based exclusively on multi-layer perceptrons (MLPs),
extracting information by mixing operations along both intra-patch and
inter-patch dimensions for battery RUL prediction. The proposed IIP-Mixer
comprises parallel dual-head mixer layers: the intra-patch mixing MLP,
capturing local temporal patterns in the short-term period, and the inter-patch
mixing MLP, capturing global temporal patterns in the long-term period.
Notably, to address the varying importance of features in RUL prediction, we
introduce a weighted loss function in the MLP-Mixer-based architecture, marking
the first time such an approach has been employed. Our experiments demonstrate
that IIP-Mixer achieves competitive performance in battery RUL prediction,
outperforming other popular time-series frameworks
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intent-Aware DRL-Based Uplink Dynamic Scheduler for 5G-NR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18364v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18364v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Salwa Mostafa, Mateus P. Mota, Alvaro Valcarce, Mehdi Bennis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the problem of supporting Industrial Internet of Things user
equipment (IIoT UEs) with intent (i.e., requested quality of service (QoS)) and
random traffic arrival. A deep reinforcement learning (DRL) based centralized
dynamic scheduler for time-frequency resources is proposed to learn how to
schedule the available communication resources among the IIoT UEs. The proposed
scheduler leverages an RL framework to adapt to the dynamic changes in the
wireless communication system and traffic arrivals. Moreover, a graph-based
reduction scheme is proposed to reduce the state and action space of the RL
framework to allow fast convergence and a better learning strategy. Simulation
results demonstrate the effectiveness of the proposed intelligent scheduler in
guaranteeing the expressed intent of IIoT UEs compared to several traditional
scheduling schemes, such as round-robin, semi-static, and heuristic approaches.
The proposed scheduler also outperforms the contention-free and
contention-based schemes in maximizing the number of successfully computed
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating Diverse Agricultural Data for Vision-Based Farming
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mikolaj Cieslak, Umabharathi Govindarajan, Alejandro Garcia, Anuradha Chandrashekar, Torsten Hädrich, Aleksander Mendoza-Drosik, Dominik L. Michels, Sören Pirk, Chia-Chun Fu, Wojciech Pałubicki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a specialized procedural model for generating synthetic
agricultural scenes, focusing on soybean crops, along with various weeds. This
model is capable of simulating distinct growth stages of these plants, diverse
soil conditions, and randomized field arrangements under varying lighting
conditions. The integration of real-world textures and environmental factors
into the procedural generation process enhances the photorealism and
applicability of the synthetic data. Our dataset includes 12,000 images with
semantic labels, offering a comprehensive resource for computer vision tasks in
precision agriculture, such as semantic segmentation for autonomous weed
control. We validate our model's effectiveness by comparing the synthetic data
against real agricultural images, demonstrating its potential to significantly
augment training data for machine learning models in agriculture. This approach
not only provides a cost-effective solution for generating high-quality,
diverse data but also addresses specific needs in agricultural vision tasks
that are not fully covered by general-purpose models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Quantum Fuzzy-based Approach for Real-Time Detection of Solar Coronal
  Holes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanmoy Bandyopadhyay, Suman Kundu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The detection and analysis of the solar coronal holes (CHs) is an important
field of study in the domain of solar physics. Mainly, it is required for the
proper prediction of the geomagnetic storms which directly or indirectly affect
various space and ground-based systems. For the detection of CHs till date, the
solar scientist depends on manual hand-drawn approaches. However, with the
advancement of image processing technologies, some automated image segmentation
methods have been used for the detection of CHs. In-spite of this, fast and
accurate detection of CHs are till a major issues. Here in this work, a novel
quantum computing-based fast fuzzy c-mean technique has been developed for fast
detection of the CHs region. The task has been carried out in two stages, in
first stage the solar image has been segmented using a quantum computing based
fast fuzzy c-mean (QCFFCM) and in the later stage the CHs has been extracted
out from the segmented image based on image morphological operation. In the
work, quantum computing has been used to optimize the cost function of the fast
fuzzy c-mean (FFCM) algorithm, where quantum approximate optimization algorithm
(QAOA) has been used to optimize the quadratic part of the cost function. The
proposed method has been tested for 193 \AA{} SDO/AIA full-disk solar image
datasets and has been compared with the existing techniques. The outcome shows
the comparable performance of the proposed method with the existing one within
a very lesser time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LC-LLM: Explainable Lane-Change Intention and Trajectory Predictions
  with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18344v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18344v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingxing Peng, Xusen Guo, Xianda Chen, Meixin Zhu, Kehua Chen,  Hao,  Yang, Xuesong Wang, Yinhai Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To ensure safe driving in dynamic environments, autonomous vehicles should
possess the capability to accurately predict the lane change intentions of
surrounding vehicles in advance and forecast their future trajectories.
Existing motion prediction approaches have ample room for improvement,
particularly in terms of long-term prediction accuracy and interpretability. In
this paper, we address these challenges by proposing LC-LLM, an explainable
lane change prediction model that leverages the strong reasoning capabilities
and self-explanation abilities of Large Language Models (LLMs). Essentially, we
reformulate the lane change prediction task as a language modeling problem,
processing heterogeneous driving scenario information in natural language as
prompts for input into the LLM and employing a supervised fine-tuning technique
to tailor the LLM specifically for our lane change prediction task. This allows
us to utilize the LLM's powerful common sense reasoning abilities to understand
complex interactive information, thereby improving the accuracy of long-term
predictions. Furthermore, we incorporate explanatory requirements into the
prompts in the inference stage. Therefore, our LC-LLM model not only can
predict lane change intentions and trajectories but also provides explanations
for its predictions, enhancing the interpretability. Extensive experiments on
the large-scale highD dataset demonstrate the superior performance and
interpretability of our LC-LLM in lane change prediction task. To the best of
our knowledge, this is the first attempt to utilize LLMs for predicting lane
change behavior. Our study shows that LLMs can encode comprehensive interaction
information for driving behavior understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ mALBERT: Is a Compact Multilingual BERT Model Still Worth It? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christophe Servan, Sahar Ghannay, Sophie Rosset
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Within the current trend of Pretained Language Models (PLM), emerge more and
more criticisms about the ethical andecological impact of such models. In this
article, considering these critical remarks, we propose to focus on
smallermodels, such as compact models like ALBERT, which are more ecologically
virtuous than these PLM. However,PLMs enable huge breakthroughs in Natural
Language Processing tasks, such as Spoken and Natural LanguageUnderstanding,
classification, Question--Answering tasks. PLMs also have the advantage of
being multilingual, and,as far as we know, a multilingual version of compact
ALBERT models does not exist. Considering these facts, wepropose the free
release of the first version of a multilingual compact ALBERT model,
pre-trained using Wikipediadata, which complies with the ethical aspect of such
a language model. We also evaluate the model against classicalmultilingual PLMs
in classical NLP tasks. Finally, this paper proposes a rare study on the
subword tokenizationimpact on language performances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 2024 Joint International Conference on Computational Linguistics,
  Language Resources and Evaluation, May 2024, Torino, Italy</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can LLMs Converse Formally? Automatically Assessing LLMs in Translating
  and Interpreting Formal Specifications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18327v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18327v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rushang Karia, Daksh Dobhal, Daniel Bramblett, Pulkit Verma, Siddharth Srivastava
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stakeholders often describe system requirements using natural language which
are then converted to formal syntax by a domain-expert leading to increased
design costs. This paper assesses the capabilities of Large Language Models
(LLMs) in converting between natural language descriptions and formal
specifications. Existing work has evaluated the capabilities of LLMs in
generating formal syntax such as source code but such experiments are typically
hand-crafted and use problems that are likely to be in the training set of
LLMs, and often require human-annotated datasets. We propose an approach that
can use two copies of an LLM in conjunction with an off-the-shelf verifier to
automatically evaluate its translation abilities without any additional human
input. Our approach generates formal syntax using language grammars to
automatically generate a dataset. We conduct an empirical evaluation to measure
the accuracy of this translation task and show that SOTA LLMs cannot adequately
solve this task, limiting their current utility in the design of complex
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chinese Offensive Language Detection:Current Status and Future
  Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18314v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18314v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunze Xiao, Houda Bouamor, Wajdi Zaghouani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the considerable efforts being made to monitor and regulate
user-generated content on social media platforms, the pervasiveness of
offensive language, such as hate speech or cyberbullying, in the digital space
remains a significant challenge. Given the importance of maintaining a
civilized and respectful online environment, there is an urgent and growing
need for automatic systems capable of detecting offensive speech in real time.
However, developing effective systems for processing languages such as Chinese
presents a significant challenge, owing to the language's complex and nuanced
nature, which makes it difficult to process automatically. This paper provides
a comprehensive overview of offensive language detection in Chinese, examining
current benchmarks and approaches and highlighting specific models and tools
for addressing the unique challenges of detecting offensive language in this
complex language. The primary objective of this survey is to explore the
existing techniques and identify potential avenues for further research that
can address the cultural and linguistic complexities of Chinese.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A thermodynamically consistent physics-informed deep learning material
  model for short fiber/polymer nanocomposites 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18310v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18310v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Betim Bahtiri, Behrouz Arash, Sven Scheffler, Maximilian Jux, Raimund Rolfes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work proposes a physics-informed deep learning (PIDL)-based constitutive
model for investigating the viscoelastic-viscoplastic behavior of short
fiber-reinforced nanoparticle-filled epoxies under various ambient conditions.
The deep-learning model is trained to enforce thermodynamic principles, leading
to a thermodynamically consistent constitutive model. To accomplish this, a
long short-term memory network is combined with a feed-forward neural network
to predict internal variables required for characterizing the internal
dissipation of the nanocomposite materials. In addition, another feed-forward
neural network is used to indicate the free-energy function, which enables
defining the thermodynamic state of the entire system. The PIDL model is
initially developed for the three-dimensional case by generating synthetic data
from a classical constitutive model. The model is then trained by extracting
the data directly from cyclic loading-unloading experimental tests. Numerical
examples show that the PIDL model can accurately predict the mechanical
behavior of epoxy-based nanocomposites for different volume fractions of fibers
and nanoparticles under various hygrothermal conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2305.08102</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Recommender System for NFT Collectibles with Item Feature <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18305v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18305v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minjoo Choi, Seonmi Kim, Yejin Kim, Youngbin Lee, Joohwan Hong, Yongjae Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems have been actively studied and applied in various domains
to deal with information overload. Although there are numerous studies on
recommender systems for movies, music, and e-commerce, comparatively less
attention has been paid to the recommender system for NFTs despite the
continuous growth of the NFT market. This paper presents a recommender system
for NFTs that utilizes a variety of data sources, from NFT transaction records
to external item features, to generate precise recommendations that cater to
individual preferences. We develop a data-efficient graph-based recommender
system to efficiently capture the complex relationship between each item and
users and generate node(item) embeddings which incorporate both node feature
information and graph structure. Furthermore, we exploit inputs beyond
user-item interactions, such as image feature, text feature, and price feature.
Numerical experiments verify the performance of the graph-based recommender
system improves significantly after utilizing all types of item features as
side information, thereby outperforming all other baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the AAAI 2023 Bridge on AI for Financial Services
  (https://sites.google.com/view/aaai-ai-fin/home)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Selective Mixup Fine-Tuning for Optimizing Non-Decomposable Objectives <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shrinivas Ramasubramanian, Harsh Rangwani, Sho Takemori, Kunal Samanta, Yuhei Umeda, Venkatesh Babu Radhakrishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise in internet usage has led to the generation of massive amounts of
data, resulting in the adoption of various supervised and semi-supervised
machine learning algorithms, which can effectively utilize the colossal amount
of data to train models. However, before deploying these models in the real
world, these must be strictly evaluated on performance measures like worst-case
recall and satisfy constraints such as fairness. We find that current
state-of-the-art empirical techniques offer sub-optimal performance on these
practical, non-decomposable performance objectives. On the other hand, the
theoretical techniques necessitate training a new model from scratch for each
performance objective. To bridge the gap, we propose SelMix, a selective
mixup-based inexpensive fine-tuning technique for pre-trained models, to
optimize for the desired objective. The core idea of our framework is to
determine a sampling distribution to perform a mixup of features between
samples from particular classes such that it optimizes the given objective. We
comprehensively evaluate our technique against the existing empirical and
theoretically principled methods on standard benchmark datasets for imbalanced
classification. We find that proposed SelMix fine-tuning significantly improves
the performance for various practical non-decomposable objectives across
benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024 SpotLight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GeNet: A Graph Neural Network-based Anti-noise Task-Oriented Semantic
  Communication Paradigm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18296v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18296v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunhang Zheng, Kechao Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional approaches to semantic communication tasks rely on the knowledge
of the signal-to-noise ratio (SNR) to mitigate channel noise. However, these
methods necessitate training under specific SNR conditions, entailing
considerable time and computational resources. In this paper, we propose GeNet,
a Graph Neural Network (GNN)-based paradigm for semantic communication aimed at
combating noise, thereby facilitating Task-Oriented Communication (TOC). We
propose a novel approach where we first transform the input data image into
graph structures. Then we leverage a GNN-based encoder to extract semantic
information from the source data. This extracted semantic information is then
transmitted through the channel. At the receiver's end, a GNN-based decoder is
utilized to reconstruct the relevant semantic information from the source data
for TOC. Through experimental evaluation, we show GeNet's effectiveness in
anti-noise TOC while decoupling the SNR dependency. We further evaluate GeNet's
performance by varying the number of nodes, revealing its versatility as a new
paradigm for semantic communication. Additionally, we show GeNet's robustness
to geometric transformations by testing it with different rotation angles,
without resorting to data augmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few-Shot Recalibration of Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Lisa Li, Urvashi Khandelwal, Kelvin Guu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has uncovered promising ways to extract well-calibrated
confidence estimates from language models (LMs), where the model's confidence
score reflects how likely it is to be correct. However, while LMs may appear
well-calibrated over broad distributions, this often hides significant
miscalibration within narrower slices (e.g., systemic over-confidence in math
can balance out systemic under-confidence in history, yielding perfect
calibration in aggregate). To attain well-calibrated confidence estimates for
any slice of a distribution, we propose a new framework for few-shot
slice-specific recalibration. Specifically, we train a recalibration model that
takes in a few unlabeled examples from any given slice and predicts a curve
that remaps confidence scores to be more accurate for that slice. Our trained
model can recalibrate for arbitrary new slices, without using any labeled data
from that slice. This enables us to identify domain-specific confidence
thresholds above which the LM's predictions can be trusted, and below which it
should abstain. Experiments show that our few-shot recalibrator consistently
outperforms existing calibration methods, for instance improving calibration
error for PaLM2-Large on MMLU by 16%, as compared to temperature scaling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identification and Uses of Deep Learning Backbones via Pattern Mining <span class="chip">SDM24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Livanos, Ian Davidson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning is extensively used in many areas of data mining as a black-box
method with impressive results. However, understanding the core mechanism of
how deep learning makes predictions is a relatively understudied problem. Here
we explore the notion of identifying a backbone of deep learning for a given
group of instances. A group here can be instances of the same class or even
misclassified instances of the same class. We view each instance for a given
group as activating a subset of neurons and attempt to find a subgraph of
neurons associated with a given concept/group. We formulate this problem as a
set cover style problem and show it is intractable and presents a highly
constrained integer linear programming (ILP) formulation. As an alternative, we
explore a coverage-based heuristic approach related to pattern mining, and show
it converges to a Pareto equilibrium point of the ILP formulation.
Experimentally we explore these backbones to identify mistakes and improve
performance, explanation, and visualization. We demonstrate application-based
results using several challenging data sets, including Bird Audio Detection
(BAD) Challenge and Labeled Faces in the Wild (LFW), as well as the classic
MNIST data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures, published SIAM SDM24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DSF-GAN: DownStream Feedback Generative Adversarial Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oriel Perets, Nadav Rappoport
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Utility and privacy are two crucial measurements of the quality of synthetic
tabular data. While significant advancements have been made in privacy
measures, generating synthetic samples with high utility remains challenging.
To enhance the utility of synthetic samples, we propose a novel architecture
called the DownStream Feedback Generative Adversarial Network (DSF-GAN). This
approach incorporates feedback from a downstream prediction model during
training to augment the generator's loss function with valuable information.
Thus, DSF-GAN utilizes a downstream prediction task to enhance the utility of
synthetic samples. To evaluate our method, we tested it using two popular
datasets. Our experiments demonstrate improved model performance when training
on synthetic samples generated by DSF-GAN, compared to those generated by the
same GAN architecture without feedback. The evaluation was conducted on the
same validation set comprising real samples. All code and datasets used in this
research will be made openly available for ease of reproduction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Generative Class Incremental Learning Performance with Model
  Forgetting Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taro Togo, Ren Togo, Keisuke Maeda, Takahiro Ogawa, Miki Haseyama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents a novel approach to Generative Class Incremental Learning
(GCIL) by introducing the forgetting mechanism, aimed at dynamically managing
class information for better adaptation to streaming data. GCIL is one of the
hot topics in the field of computer vision, and this is considered one of the
crucial tasks in society, specifically the continual learning of generative
models. The ability to forget is a crucial brain function that facilitates
continual learning by selectively discarding less relevant information for
humans. However, in the field of machine learning models, the concept of
intentionally forgetting has not been extensively investigated. In this study
we aim to bridge this gap by incorporating the forgetting mechanisms into GCIL,
thereby examining their impact on the models' ability to learn in continual
learning. Through our experiments, we have found that integrating the
forgetting mechanisms significantly enhances the models' performance in
acquiring new knowledge, underscoring the positive role that strategic
forgetting plays in the process of continual learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Manipulating Neural Path Planners via Slight Perturbations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18256v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18256v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zikang Xiong, Suresh Jagannathan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-driven neural path planners are attracting increasing interest in the
robotics community. However, their neural network components typically come as
black boxes, obscuring their underlying decision-making processes. Their
black-box nature exposes them to the risk of being compromised via the
insertion of hidden malicious behaviors. For example, an attacker may hide
behaviors that, when triggered, hijack a delivery robot by guiding it to a
specific (albeit wrong) destination, trapping it in a predefined region, or
inducing unnecessary energy expenditure by causing the robot to repeatedly
circle a region. In this paper, we propose a novel approach to specify and
inject a range of hidden malicious behaviors, known as backdoors, into neural
path planners. Our approach provides a concise but flexible way to define these
behaviors, and we show that hidden behaviors can be triggered by slight
perturbations (e.g., inserting a tiny unnoticeable object), that can
nonetheless significantly compromise their integrity. We also discuss potential
techniques to identify these backdoors aimed at alleviating such risks. We
demonstrate our approach on both sampling-based and search-based neural path
planners.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18252v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18252v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwu Zhong, Zi-Yuan Hu, Michael R. Lyu, Liwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual representation learning has been a cornerstone in computer vision,
evolving from supervised learning with human-annotated labels to aligning
image-text pairs from the Internet. Despite recent advancements in multi-modal
large language models (MLLMs), the visual representations they rely on, such as
CLIP embeddings, often lack access to external world knowledge critical for
real-world visual reasoning. In this work, we propose Visual Table, a novel
visual representation tailored for MLLMs. It provides hierarchical text
descriptions of holistic visual scenes, consisting of a scene description and
multiple object-centric descriptions that encompass categories, attributes, and
knowledge at instance level. We further develop a scalable generator for visual
table generation and train it on small-scale annotations from GPT4V. Extensive
evaluations demonstrate that, with generated visual tables as additional visual
representations, our model can consistently outperform the state-of-the-art
(SOTA) MLLMs across diverse benchmarks. When visual tables serve as standalone
visual representations, our model can closely match or even beat the SOTA MLLMs
that are built on CLIP visual embeddings. Our code is available at
https://github.com/LaVi-Lab/Visual-Table.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://github.com/LaVi-Lab/Visual-Table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Conversational Question Answering with Fine-Grained
  Retrieval-Augmentation and Self-Check 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18243v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18243v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linhao Ye, Zhikai Lei, Jianghao Yin, Qin Chen, Jie Zhou, Liang He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) aims to generate more reliable and
accurate responses, by augmenting large language models (LLMs) with the
external vast and dynamic knowledge. Most previous work focuses on using RAG
for single-round question answering, while how to adapt RAG to the complex
conversational setting wherein the question is interdependent on the preceding
context is not well studied. In this paper, we propose a conversation-level RAG
approach, which incorporates fine-grained retrieval augmentation and self-check
for conversational question answering (CQA). In particular, our approach
consists of three components, namely conversational question refiner,
fine-grained retriever and self-check based response generator, which work
collaboratively for question understanding and relevant information acquisition
in conversational settings. Extensive experiments demonstrate the great
advantages of our approach over the state-of-the-art baselines. Moreover, we
also release a Chinese CQA dataset with new features including reformulated
question, extracted keyword, retrieved paragraphs and their helpfulness, which
facilitates further researches in RAG enhanced CQA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeuSDFusion: A Spatial-Aware Generative Model for 3D Shape Completion,
  Reconstruction, and Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18241v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18241v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruikai Cui, Weizhe Liu, Weixuan Sun, Senbo Wang, Taizhang Shang, Yang Li, Xibin Song, Han Yan, Zhennan Wu, Shenzhou Chen, Hongdong Li, Pan Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D shape generation aims to produce innovative 3D content adhering to
specific conditions and constraints. Existing methods often decompose 3D shapes
into a sequence of localized components, treating each element in isolation
without considering spatial consistency. As a result, these approaches exhibit
limited versatility in 3D data representation and shape generation, hindering
their ability to generate highly diverse 3D shapes that comply with the
specified constraints. In this paper, we introduce a novel spatial-aware 3D
shape generation framework that leverages 2D plane representations for enhanced
3D shape modeling. To ensure spatial coherence and reduce memory usage, we
incorporate a hybrid shape representation technique that directly learns a
continuous signed distance field representation of the 3D shape using
orthogonal 2D planes. Additionally, we meticulously enforce spatial
correspondences across distinct planes using a transformer-based autoencoder
structure, promoting the preservation of spatial relationships in the generated
3D shapes. This yields an algorithm that consistently outperforms
state-of-the-art 3D shape generation methods on various tasks, including
unconditional shape generation, multi-modal shape completion, single-view
reconstruction, and text-to-shape synthesis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models Need Consultants for Reasoning: Becoming an Expert
  in a Complex Human System Through Behavior Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18230v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18230v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuwen Wang, Shirong Zeng, Cheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs), in conjunction with various reasoning
reinforcement methodologies, have demonstrated remarkable capabilities
comparable to humans in fields such as mathematics, law, coding, common sense,
and world knowledge. In this paper, we delve into the reasoning abilities of
LLMs within complex human systems. We propose a novel reasoning framework,
termed ``Mosaic Expert Observation Wall'' (MEOW) exploiting
generative-agents-based simulation technique. In the MEOW framework, simulated
data are utilized to train an expert model concentrating ``experience'' about a
specific task in each independent time of simulation. It is the accumulated
``experience'' through the simulation that makes for an expert on a task in a
complex human system. We conduct the experiments within a communication game
that mirrors real-world security scenarios. The results indicate that our
proposed methodology can cooperate with existing methodologies to enhance the
reasoning abilities of LLMs in complex human systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Transformer-Based Framework for Payload Malware Detection and
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyle Stein, Arash Mahyari, Guillermo Francia III, Eman El-Sheikh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As malicious cyber threats become more sophisticated in breaching computer
networks, the need for effective intrusion detection systems (IDSs) becomes
crucial. Techniques such as Deep Packet Inspection (DPI) have been introduced
to allow IDSs analyze the content of network packets, providing more context
for identifying potential threats. IDSs traditionally rely on using
anomaly-based and signature-based detection techniques to detect unrecognized
and suspicious activity. Deep learning techniques have shown great potential in
DPI for IDSs due to their efficiency in learning intricate patterns from the
packet content being transmitted through the network. In this paper, we propose
a revolutionary DPI algorithm based on transformers adapted for the purpose of
detecting malicious traffic with a classifier head. Transformers learn the
complex content of sequence data and generalize them well to similar scenarios
thanks to their self-attention mechanism. Our proposed method uses the raw
payload bytes that represent the packet contents and is deployed as
man-in-the-middle. The payload bytes are used to detect malicious packets and
classify their types. Experimental results on the UNSW-NB15 and CIC-IOT23
datasets demonstrate that our transformer-based model is effective in
distinguishing malicious from benign traffic in the test dataset, attaining an
average accuracy of 79\% using binary classification and 72\% on the
multi-classification experiment, both using solely payload bytes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Two-Dimensional to Three-Dimensional Environment with Q-Learning:
  Modeling Autonomous Navigation with <span class="highlight-title">Reinforcement</span> Learning and no Libraries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18219v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18219v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ergon Cugler de Moraes Silva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) algorithms have become indispensable tools in
artificial intelligence, empowering agents to acquire optimal decision-making
policies through interactions with their environment and feedback mechanisms.
This study explores the performance of RL agents in both two-dimensional (2D)
and three-dimensional (3D) environments, aiming to research the dynamics of
learning across different spatial dimensions. A key aspect of this
investigation is the absence of pre-made libraries for learning, with the
algorithm developed exclusively through computational mathematics. The
methodological framework centers on RL principles, employing a Q-learning agent
class and distinct environment classes tailored to each spatial dimension. The
research aims to address the question: How do reinforcement learning agents
adapt and perform in environments of varying spatial dimensions, particularly
in 2D and 3D settings? Through empirical analysis, the study evaluates agents'
learning trajectories and adaptation processes, revealing insights into the
efficacy of RL algorithms in navigating complex, multi-dimensional spaces.
Reflections on the findings prompt considerations for future research,
particularly in understanding the dynamics of learning in higher-dimensional
environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Large Language Models for Fuzzy String Matching in Political
  Science 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18218v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18218v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fuzzy string matching remains a key issue when political scientists combine
data from different sources. Existing matching methods invariably rely on
string distances, such as Levenshtein distance and cosine similarity. As such,
they are inherently incapable of matching strings that refer to the same entity
with different names such as ''JP Morgan'' and ''Chase Bank'', ''DPRK'' and
''North Korea'', ''Chuck Fleischmann (R)'' and ''Charles Fleischmann (R)''. In
this letter, we propose to use large language models to entirely sidestep this
problem in an easy and intuitive manner. Extensive experiments show that our
proposed methods can improve the state of the art by as much as 39% in terms of
average precision while being substantially easier and more intuitive to use by
political scientists. Moreover, our results are robust against various
temperatures. We further note that enhanced prompting can lead to additional
performance improvements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 2 figures, 1 table;</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Preference-Based Planning in Stochastic Environments: From
  Partially-Ordered Temporal Goals to Most Preferred Policies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18212v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18212v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hazhar Rahmani, Abhishek N. Kulkarni, Jie Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human preferences are not always represented via complete linear orders: It
is natural to employ partially-ordered preferences for expressing incomparable
outcomes. In this work, we consider decision-making and probabilistic planning
in stochastic systems modeled as Markov decision processes (MDPs), given a
partially ordered preference over a set of temporally extended goals.
Specifically, each temporally extended goal is expressed using a formula in
Linear Temporal Logic on Finite Traces (LTL$_f$). To plan with the partially
ordered preference, we introduce order theory to map a preference over temporal
goals to a preference over policies for the MDP. Accordingly, a most preferred
policy under a stochastic ordering induces a stochastic nondominated
probability distribution over the finite paths in the MDP. To synthesize a most
preferred policy, our technical approach includes two key steps. In the first
step, we develop a procedure to transform a partially ordered preference over
temporal goals into a computational model, called preference automaton, which
is a semi-automaton with a partial order over acceptance conditions. In the
second step, we prove that finding a most preferred policy is equivalent to
computing a Pareto-optimal policy in a multi-objective MDP that is constructed
from the original MDP, the preference automaton, and the chosen stochastic
ordering relation. Throughout the paper, we employ running examples to
illustrate the proposed preference specification and solution approaches. We
demonstrate the efficacy of our algorithm using these examples, providing
detailed analysis, and then discuss several potential future directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2209.12267</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long and Short-Term Constraints Driven Safe <span class="highlight-title">Reinforcement</span> Learning for
  Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuemin Hu, Pan Chen, Yijun Wen, Bo Tang, Long Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) has been widely used in decision-making tasks,
but it cannot guarantee the agent's safety in the training process due to the
requirements of interaction with the environment, which seriously limits its
industrial applications such as autonomous driving. Safe RL methods are
developed to handle this issue by constraining the expected safety violation
costs as a training objective, but they still permit unsafe state occurrence,
which is unacceptable in autonomous driving tasks. Moreover, these methods are
difficult to achieve a balance between the cost and return expectations, which
leads to learning performance degradation for the algorithms. In this paper, we
propose a novel algorithm based on the long and short-term constraints (LSTC)
for safe RL. The short-term constraint aims to guarantee the short-term state
safety that the vehicle explores, while the long-term constraint ensures the
overall safety of the vehicle throughout the decision-making process. In
addition, we develop a safe RL method with dual-constraint optimization based
on the Lagrange multiplier to optimize the training process for end-to-end
autonomous driving. Comprehensive experiments were conducted on the MetaDrive
simulator. Experimental results demonstrate that the proposed method achieves
higher safety in continuous state and action tasks, and exhibits higher
exploration performance in long-distance decision-making tasks compared with
state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Evolutionary Network Architecture Search Framework with Adaptive
  Multimodal Fusion for Hand Gesture Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhang Xia, Shihao Song, Zhanglu Hou, Junwen Xu, Juan Zou, Yuan Liu, Shengxiang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hand gesture recognition (HGR) based on multimodal data has attracted
considerable attention owing to its great potential in applications. Various
manually designed multimodal deep networks have performed well in multimodal
HGR (MHGR), but most of existing algorithms require a lot of expert experience
and time-consuming manual trials. To address these issues, we propose an
evolutionary network architecture search framework with the adaptive multimodel
fusion (AMF-ENAS). Specifically, we design an encoding space that
simultaneously considers fusion positions and ratios of the multimodal data,
allowing for the automatic construction of multimodal networks with different
architectures through decoding. Additionally, we consider three input streams
corresponding to intra-modal surface electromyography (sEMG), intra-modal
accelerometer (ACC), and inter-modal sEMG-ACC. To automatically adapt to
various datasets, the ENAS framework is designed to automatically search a MHGR
network with appropriate fusion positions and ratios. To the best of our
knowledge, this is the first time that ENAS has been utilized in MHGR to tackle
issues related to the fusion position and ratio of multimodal data.
Experimental results demonstrate that AMF-ENAS achieves state-of-the-art
performance on the Ninapro DB2, DB3, and DB7 datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Privacy Protection Capabilities of Chinese Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18205v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18205v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqi Yang, Xiaowen Huang, Jitao Sang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs), renowned for their impressive capabilities in
various tasks, have significantly advanced artificial intelligence. Yet, these
advancements have raised growing concerns about privacy and security
implications. To address these issues and explain the risks inherent in these
models, we have devised a three-tiered progressive framework tailored for
evaluating privacy in language systems. This framework consists of
progressively complex and in-depth privacy test tasks at each tier. Our primary
objective is to comprehensively evaluate the sensitivity of large language
models to private information, examining how effectively they discern, manage,
and safeguard sensitive data in diverse scenarios. This systematic evaluation
helps us understand the degree to which these models comply with privacy
protection guidelines and the effectiveness of their inherent safeguards
against privacy breaches. Our observations indicate that existing Chinese large
language models universally show privacy protection shortcomings. It seems that
at the moment this widespread issue is unavoidable and may pose corresponding
privacy risks in applications based on these models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EndToEndML: An Open-Source End-to-End Pipeline for Machine Learning
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18203v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18203v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nisha Pillai, Athish Ram Das, Moses Ayoola, Ganga Gireesan, Bindu Nanduri, Mahalingam Ramkumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence (AI) techniques are widely applied in the life
sciences. However, applying innovative AI techniques to understand and
deconvolute biological complexity is hindered by the learning curve for life
science scientists to understand and use computing languages. An open-source,
user-friendly interface for AI models, that does not require programming skills
to analyze complex biological data will be extremely valuable to the
bioinformatics community. With easy access to different sequencing technologies
and increased interest in different 'omics' studies, the number of biological
datasets being generated has increased and analyzing these high-throughput
datasets is computationally demanding. The majority of AI libraries today
require advanced programming skills as well as machine learning, data
preprocessing, and visualization skills. In this research, we propose a
web-based end-to-end pipeline that is capable of preprocessing, training,
evaluating, and visualizing machine learning (ML) models without manual
intervention or coding expertise. By integrating traditional machine learning
and deep neural network models with visualizations, our library assists in
recognizing, classifying, clustering, and predicting a wide range of
multi-modal, multi-sensor datasets, including images, languages, and
one-dimensional numerical data, for drug discovery, pathogen classification,
and medical diagnostics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2024 7th International Conference on Information and Computer
  Technologies (ICICT)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Looking Beyond What You See: An Empirical Analysis on Subgroup
  Intersectional Fairness for Multi-label Chest X-ray Classification Using
  Social Determinants of Racial Health Inequities <span class="chip">ICCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18196v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18196v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dana Moukheiber, Saurabh Mahindre, Lama Moukheiber, Mira Moukheiber, Mingchen Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There has been significant progress in implementing deep learning models in
disease diagnosis using chest X- rays. Despite these advancements, inherent
biases in these models can lead to disparities in prediction accuracy across
protected groups. In this study, we propose a framework to achieve accurate
diagnostic outcomes and ensure fairness across intersectional groups in
high-dimensional chest X- ray multi-label classification. Transcending
traditional protected attributes, we consider complex interactions within
social determinants, enabling a more granular benchmark and evaluation of
fairness. We present a simple and robust method that involves retraining the
last classification layer of pre-trained models using a balanced dataset across
groups. Additionally, we account for fairness constraints and integrate
class-balanced fine-tuning for multi-label settings. The evaluation of our
method on the MIMIC-CXR dataset demonstrates that our framework achieves an
optimal tradeoff between accuracy and fairness compared to baseline methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV CVAMD 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SCANet: Correcting LEGO Assembly Errors with Self-Correct Assembly
  Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18195v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18195v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Wan, Kaichen Zhou, jinhong Chen, Hao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous assembly in robotics and 3D vision presents significant
challenges, particularly in ensuring assembly correctness. Presently,
predominant methods such as MEPNet focus on assembling components based on
manually provided images. However, these approaches often fall short in
achieving satisfactory results for tasks requiring long-term planning.
Concurrently, we observe that integrating a self-correction module can
partially alleviate such issues. Motivated by this concern, we introduce the
single-step assembly error correction task, which involves identifying and
rectifying misassembled components. To support research in this area, we
present the LEGO Error Correction Assembly Dataset (LEGO-ECA), comprising
manual images for assembly steps and instances of assembly failures.
Additionally, we propose the Self-Correct Assembly Network (SCANet), a novel
method to address this task. SCANet treats assembled components as queries,
determining their correctness in manual images and providing corrections when
necessary. Finally, we utilize SCANet to correct the assembly results of
MEPNet. Experimental results demonstrate that SCANet can identify and correct
MEPNet's misassembled results, significantly improving the correctness of
assembly. Our code and dataset are available at
https://github.com/Yaser-wyx/SCANet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can AI Models Appreciate Document Aesthetics? An <span class="highlight-title">Exploration</span> of
  Legibility and Layout Quality in Relation to Prediction Confidence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18183v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18183v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hsiu-Wei Yang, Abhinav Agrawal, Pavlos Fragkogiannis, Shubham Nitin Mulay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A well-designed document communicates not only through its words but also
through its visual eloquence. Authors utilize aesthetic elements such as
colors, fonts, graphics, and layouts to shape the perception of information.
Thoughtful document design, informed by psychological insights, enhances both
the visual appeal and the comprehension of the content. While state-of-the-art
document AI models demonstrate the benefits of incorporating layout and image
data, it remains unclear whether the nuances of document aesthetics are
effectively captured. To bridge the gap between human cognition and AI
interpretation of aesthetic elements, we formulated hypotheses concerning AI
behavior in document understanding tasks, specifically anchored in document
design principles. With a focus on legibility and layout quality, we tested
four aspects of aesthetic effects: noise, font-size contrast, alignment, and
complexity, on model confidence using correlational analysis. The results and
observations highlight the value of model analysis rooted in document design
theories. Our work serves as a trailhead for further studies and we advocate
for continued research in this topic to deepen our understanding of how AI
interprets document aesthetics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mechanisms of non-factual hallucinations in language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18167v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18167v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Yu, Meng Cao, Jackie Chi Kit Cheung, Yue Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art language models (LMs) sometimes generate non-factual
hallucinations that misalign with world knowledge. Despite extensive efforts to
detect and mitigate hallucinations, understanding their internal mechanisms
remains elusive. Our study investigates the mechanistic causes of
hallucination, specifically non-factual ones where the LM incorrectly predicts
object attributes in response to subject-relation queries. With causal
mediation analysis and embedding space projection, we identify two general
mechanistic causes of hallucinations shared across LMs of various scales and
designs: 1) insufficient subject attribute knowledge in lower layer MLPs, and
2) failing to select the correct object attribute in upper layer attention
heads and MLPs. These two mechanisms exhibit varying degrees of subject-object
association, predictive uncertainty and perturbation robustness. Additionally,
we scrutinize LM pre-training checkpoints, revealing distinct learning dynamics
for the two mechanistic causes of hallucinations. We also highlight how
attribution features from our causal analysis can effectively construct
hallucination detectors. Our work proposes a mechanistic understanding of LM
factual errors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is Modularity Transferable? A Case Study through the Lens of Knowledge
  Distillation <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18804v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18804v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mateusz Klimaszewski, Piotr Andruszkiewicz, Alexandra Birch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of Modular Deep Learning showcases its potential in various Natural
Language Processing applications. Parameter-efficient fine-tuning (PEFT)
modularity has been shown to work for various use cases, from domain adaptation
to multilingual setups. However, all this work covers the case where the
modular components are trained and deployed within one single Pre-trained
Language Model (PLM). This model-specific setup is a substantial limitation on
the very modularity that modular architectures are trying to achieve. We ask
whether current modular approaches are transferable between models and whether
we can transfer the modules from more robust and larger PLMs to smaller ones.
In this work, we aim to fill this gap via a lens of Knowledge Distillation,
commonly used for model compression, and present an extremely straightforward
approach to transferring pre-trained, task-specific PEFT modules between
same-family PLMs. Moreover, we propose a method that allows the transfer of
modules between incompatible PLMs without any change in the inference
complexity. The experiments on Named Entity Recognition, Natural Language
Inference, and Paraphrase Identification tasks over multiple languages and PEFT
methods showcase the initial potential of transferable modularity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Projective Methods for Mitigating Gender Bias in Pre-trained Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18803v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18803v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hillary Dawkins, Isar Nejadgholi, Daniel Gillis, Judi McCuaig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mitigation of gender bias in NLP has a long history tied to debiasing static
word embeddings. More recently, attention has shifted to debiasing pre-trained
language models. We study to what extent the simplest projective debiasing
methods, developed for word embeddings, can help when applied to BERT's
internal representations. Projective methods are fast to implement, use a small
number of saved parameters, and make no updates to the existing model
parameters. We evaluate the efficacy of the methods in reducing both intrinsic
bias, as measured by BERT's next sentence prediction task, and in mitigating
observed bias in a downstream setting when fine-tuned. To this end, we also
provide a critical analysis of a popular gender-bias assessment test for
quantifying intrinsic bias, resulting in an enhanced test set and new bias
measures. We find that projective methods can be effective at both intrinsic
bias and downstream bias mitigation, but that the two outcomes are not
necessarily correlated. This finding serves as a warning that intrinsic bias
test sets, based either on language modeling tasks or next sentence prediction,
should not be the only benchmark in developing a debiased language model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a World-English Language Model for On-Device Virtual Assistants <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18783v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18783v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rricha Jalota, Lyan Verwimp, Markus Nussbaum-Thom, Amr Mousa, Arturo Argueta, Youssef Oualil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Network Language Models (NNLMs) for Virtual Assistants (VAs) are
generally language-, region-, and in some cases, device-dependent, which
increases the effort to scale and maintain them. Combining NNLMs for one or
more of the categories is one way to improve scalability. In this work, we
combine regional variants of English to build a ``World English'' NNLM for
on-device VAs. In particular, we investigate the application of adapter
bottlenecks to model dialect-specific characteristics in our existing
production NNLMs {and enhance the multi-dialect baselines}. We find that
adapter modules are more effective in modeling dialects than specializing
entire sub-networks. Based on this insight and leveraging the design of our
production models, we introduce a new architecture for World English NNLM that
meets the accuracy, latency, and memory constraints of our single-dialect
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CheckEval: Robust Evaluation Framework using Large Language Model via
  Checklist 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18771v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18771v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukyung Lee, Joonghoon Kim, Jaehee Kim, Hyowon Cho, Pilsung Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce CheckEval, a novel evaluation framework using Large Language
Models, addressing the challenges of ambiguity and inconsistency in current
evaluation methods. CheckEval addresses these challenges by dividing evaluation
criteria into detailed sub-aspects and constructing a checklist of Boolean
questions for each, simplifying the evaluation. This approach not only renders
the process more interpretable but also significantly enhances the robustness
and reliability of results by focusing on specific evaluation dimensions.
Validated through a focused case study using the SummEval benchmark, CheckEval
indicates a strong correlation with human judgments. Furthermore, it
demonstrates a highly consistent Inter-Annotator Agreement. These findings
highlight the effectiveness of CheckEval for objective, flexible, and precise
evaluations. By offering a customizable and interactive framework, CheckEval
sets a new standard for the use of LLMs in evaluation, responding to the
evolving needs of the field and establishing a clear method for future
LLM-based evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>HEAL at CHI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved Neural Protoform Reconstruction via Reflex Prediction <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18769v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18769v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Lu, Jingzhi Wang, David R. Mortensen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Protolanguage reconstruction is central to historical linguistics. The
comparative method, one of the most influential theoretical and methodological
frameworks in the history of the language sciences, allows linguists to infer
protoforms (reconstructed ancestral words) from their reflexes (related modern
words) based on the assumption of regular sound change. Not surprisingly,
numerous computational linguists have attempted to operationalize comparative
reconstruction through various computational models, the most successful of
which have been supervised encoder-decoder models, which treat the problem of
predicting protoforms given sets of reflexes as a sequence-to-sequence problem.
We argue that this framework ignores one of the most important aspects of the
comparative method: not only should protoforms be inferable from cognate sets
(sets of related reflexes) but the reflexes should also be inferable from the
protoforms. Leveraging another line of research -- reflex prediction -- we
propose a system in which candidate protoforms from a reconstruction model are
reranked by a reflex prediction model. We show that this more complete
implementation of the comparative method allows us to surpass state-of-the-art
protoform reconstruction methods on three of four Chinese and Romance datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CYCLE: Learning to Self-Refine the Code Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18746v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18746v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangruibo Ding, Marcus J. Min, Gail Kaiser, Baishakhi Ray
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained code language models have achieved promising performance in code
generation and improved the programming efficiency of human developers.
However, their self-refinement capability is typically overlooked by the
existing evaluations of code LMs, which focus only on the accuracy of the
one-time prediction. For the cases when code LMs fail to implement the correct
program, developers actually find it hard to debug and fix the faulty
prediction since it is not written by the developers themselves. Unfortunately,
our study reveals that code LMs cannot efficiently self-refine their faulty
generations as well.
  In this paper, we propose CYCLE framework, learning to self-refine the faulty
generation according to the available feedback, such as the execution results
reported by the test suites. We evaluate CYCLE on three popular code generation
benchmarks, HumanEval, MBPP, and APPS. The results reveal that CYCLE
successfully maintains, sometimes improves, the quality of one-time code
generation, while significantly improving the self-refinement capability of
code LMs. We implement four variants of CYCLE with varied numbers of parameters
across 350M, 1B, 2B, and 3B, and the experiments show that CYCLE consistently
boosts the code generation performance, by up to 63.5%, across benchmarks and
varied model sizes. We also notice that CYCLE outperforms code LMs that have
3$\times$ more parameters in self-refinement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready for OOPSLA'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Invalsi Benchmark: measuring Language Models Mathematical and
  Language understanding in Italian 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18697v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18697v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Esuli, Giovanni Puccetti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Italian is by all metrics a high resource language, currently, there
are isn't a Language Model pre-trained exclusively in this language. This
results in a lower number of available benchmarks to evaluate the performance
of language models in Italian.
  This work presents two new benchmarks to evaluate the models performance on
mathematical understanding and language understanding in Italian. These
benchmarks are based on real tests that are undertaken by students of age
between 11 and 18 within the Italian school system and have therefore been
validated by several experts in didactics and pedagogy.
  To validate this dataset we evaluate the performance of 9 language models
that are the best performing when writing in Italian, including our own
fine-tuned models. We show that this is a challenging benchmark where current
language models are bound by 60\% accuracy.
  We believe that the release of this dataset paves the way for improving
future models mathematical and language understanding in Italian.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Laws For Dense Retrieval <span class="chip">SIGIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18684v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18684v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Fang, Jingtao Zhan, Qingyao Ai, Jiaxin Mao, Weihang Su, Jia Chen, Yiqun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling up neural models has yielded significant advancements in a wide array
of tasks, particularly in language generation. Previous studies have found that
the performance of neural models frequently adheres to predictable scaling
laws, correlated with factors such as training set size and model size. This
insight is invaluable, especially as large-scale experiments grow increasingly
resource-intensive. Yet, such scaling law has not been fully explored in dense
retrieval due to the discrete nature of retrieval metrics and complex
relationships between training data and model sizes in retrieval tasks. In this
study, we investigate whether the performance of dense retrieval models follows
the scaling law as other neural models. We propose to use contrastive
log-likelihood as the evaluation metric and conduct extensive experiments with
dense retrieval models implemented with different numbers of parameters and
trained with different amounts of annotated data. Results indicate that, under
our settings, the performance of dense retrieval models follows a precise
power-law scaling related to the model size and the number of annotations.
Additionally, we examine scaling with prevalent data augmentation methods to
assess the impact of annotation quality, and apply the scaling law to find the
best resource allocation strategy under a budget constraint. We believe that
these insights will significantly contribute to understanding the scaling
effect of dense retrieval models and offer meaningful guidance for future
research endeavors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SIGIR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NL-ITI: Optimizing Probing and Intervention for Improvement of ITI
  Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18680v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18680v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakub Hoscilowicz, Adam Wiacek, Jan Chojnacki, Adam Cieslak, Leszek Michon, Vitalii Urbanevych, Artur Janicki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLM) are prone to returning false information. It
constitutes one of major challenges in the AI field. In our work, we explore
paradigm introduced by Inference-Time-Intervention (ITI). In first stage, it
identifies attention heads, which contain the highest amount of desired type of
knowledge (e.g., truthful). Afterwards, during inference, LLM activations are
shifted for chosen subset of attention heads. We further improved the ITI
framework by introducing a nonlinear probing and multi-token intervention -
Non-Linear ITI (NL-ITI). NL-ITI is tested on diverse multiple-choice
benchmarks, including TruthfulQA, on which we report around 14% MC1 metric
improvement with respect to the baseline ITI results. NL-ITI achieves also
encouraging results on other testsets - on Business Ethics subdomain of MMLU,
around 18% MC1 improvement over baseline LLaMA2-7B. Additionally, NL-ITI
performs better while being less invasive in the behavior of LLM at the same
time (as measured by Kullback-Leibler divergence).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at https://github.com/Samsung/NL-ITI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fact Checking Beyond Training Set <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18671v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18671v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Payam Karisani, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating the veracity of everyday claims is time consuming and in some
cases requires domain expertise. We empirically demonstrate that the commonly
used fact checking pipeline, known as the retriever-reader, suffers from
performance deterioration when it is trained on the labeled data from one
domain and used in another domain. Afterwards, we delve into each component of
the pipeline and propose novel algorithms to address this problem. We propose
an adversarial algorithm to make the retriever component robust against
distribution shift. Our core idea is to initially train a bi-encoder on the
labeled source data, and then, to adversarially train two separate document and
claim encoders using unlabeled target data. We then focus on the reader
component and propose to train it such that it is insensitive towards the order
of claims and evidence documents. Our empirical evaluations support the
hypothesis that such a reader shows a higher robustness against distribution
shift. To our knowledge, there is no publicly available multi-topic fact
checking dataset. Thus, we propose a simple automatic method to re-purpose two
well-known fact checking datasets. We then construct eight fact checking
scenarios from these datasets, and compare our model to a set of strong
baseline models, including recent domain adaptation models that use GPT4 for
generating synthetic data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Content Recommendation: Knowledge Graph-Based Semantic
  Contrastive Learning for Diversity and Cold-Start Users <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18667v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18667v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yejin Kim, Scott Rome, Kevin Foley, Mayur Nankani, Rimon Melamed, Javier Morales, Abhay Yadav, Maria Peifer, Sardar Hamidian, H. Howie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Addressing the challenges related to data sparsity, cold-start problems, and
diversity in recommendation systems is both crucial and demanding. Many current
solutions leverage knowledge graphs to tackle these issues by combining both
item-based and user-item collaborative signals. A common trend in these
approaches focuses on improving ranking performance at the cost of escalating
model complexity, reducing diversity, and complicating the task. It is
essential to provide recommendations that are both personalized and diverse,
rather than solely relying on achieving high rank-based performance, such as
Click-through Rate, Recall, etc. In this paper, we propose a hybrid multi-task
learning approach, training on user-item and item-item interactions. We apply
item-based contrastive learning on descriptive text, sampling positive and
negative pairs based on item metadata. Our approach allows the model to better
understand the relationships between entities within the knowledge graph by
utilizing semantic information from text. It leads to more accurate, relevant,
and diverse user recommendations and a benefit that extends even to cold-start
users who have few interactions with items. We perform extensive experiments on
two widely used datasets to validate the effectiveness of our approach. Our
findings demonstrate that jointly training user-item interactions and
item-based signals using synopsis text is highly effective. Furthermore, our
results provide evidence that item-based contrastive learning enhances the
quality of entity embeddings, as indicated by metrics such as uniformity and
alignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SDSAT: Accelerating LLM Inference through Speculative Decoding with
  Semantic Adaptive Tokens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18647v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18647v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengbo Liu, Yong Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an acceleration scheme for large language models (LLMs) through
Speculative Decoding with Semantic Adaptive Tokens (SDSAT). The primary
objective of this design is to enhance the LLM model's ability to generate
draft tokens more accurately without compromising the model's accuracy. The
core strategies involve: 1) Fine-tune the model by incorporating semantic
adaptive tokens that possess flexible decoding capabilities without changing
its structure, allowing them to generate high-quality draft tokens. 2) By
employing a training method that does not affect the standard tokens, the model
can acquire parallel decoding abilities atop its original framework with
minimal training overhead. 3) We have designed the "two-step-draft-then-verify"
generation strategies using both greedy search and nucleus sampling.
Experiments conducted on the CodeLlama-13B and 7B models have yielded speed
increases of over 3.5X and 3.0X, respectively. Please refer to
https://github.com/hasuoshenyun/SDSAT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vulnerability Detection with Code Language Models: How Far Are We? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18624v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18624v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangruibo Ding, Yanjun Fu, Omniyyah Ibrahim, Chawin Sitawarin, Xinyun Chen, Basel Alomair, David Wagner, Baishakhi Ray, Yizheng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the context of the rising interest in code language models (code LMs) and
vulnerability detection, we study the effectiveness of code LMs for detecting
vulnerabilities. Our analysis reveals significant shortcomings in existing
vulnerability datasets, including poor data quality, low label accuracy, and
high duplication rates, leading to unreliable model performance in realistic
vulnerability detection scenarios. Additionally, the evaluation methods used
with these datasets are not representative of real-world vulnerability
detection.
  To address these challenges, we introduce PrimeVul, a new dataset for
training and evaluating code LMs for vulnerability detection. PrimeVul
incorporates a novel set of data labeling techniques that achieve comparable
label accuracy to human-verified benchmarks while significantly expanding the
dataset. It also implements a rigorous data de-duplication and chronological
data splitting strategy to mitigate data leakage issues, alongside introducing
more realistic evaluation metrics and settings. This comprehensive approach
aims to provide a more accurate assessment of code LMs' performance in
real-world conditions.
  Evaluating code LMs on PrimeVul reveals that existing benchmarks
significantly overestimate the performance of these models. For instance, a
state-of-the-art 7B model scored 68.26% F1 on BigVul but only 3.09% F1 on
PrimeVul. Attempts to improve performance through advanced training techniques
and larger models like GPT-3.5 and GPT-4 were unsuccessful, with results akin
to random guessing in the most stringent settings. These findings underscore
the considerable gap between current capabilities and the practical
requirements for deploying code LMs in security roles, highlighting the need
for more innovative research in this domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">survey</span> on learning models of spiking neural membrane systems and
  spiking neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prithwineel Paul, Petr Sosik, Lucie Ciencialova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking neural networks (SNN) are a biologically inspired model of neural
networks with certain brain-like properties. In the past few decades, this
model has received increasing attention in computer science community, owing
also to the successful phenomenon of deep learning. In SNN, communication
between neurons takes place through the spikes and spike trains. This
differentiates these models from the ``standard'' artificial neural networks
(ANN) where the frequency of spikes is replaced by real-valued signals. Spiking
neural P systems (SNPS) can be considered a branch of SNN based more on the
principles of formal automata, with many variants developed within the
framework of the membrane computing theory. In this paper, we first briefly
compare structure and function, advantages and drawbacks of SNN and SNPS. A key
part of the article is a survey of recent results and applications of machine
learning and deep learning models of both SNN and SNPS formalisms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Debiasing Sentence Embedders through Contrastive Word Pairs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18555v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18555v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Kenneweg, Sarah Schröder, Alexander Schulz, Barbara Hammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the last years, various sentence embedders have been an integral part in
the success of current machine learning approaches to Natural Language
Processing (NLP). Unfortunately, multiple sources have shown that the bias,
inherent in the datasets upon which these embedding methods are trained, is
learned by them. A variety of different approaches to remove biases in
embeddings exists in the literature. Most of these approaches are applicable to
word embeddings and in fewer cases to sentence embeddings. It is problematic
that most debiasing approaches are directly transferred from word embeddings,
therefore these approaches fail to take into account the nonlinear nature of
sentence embedders and the embeddings they produce. It has been shown in
literature that bias information is still present if sentence embeddings are
debiased using such methods. In this contribution, we explore an approach to
remove linear and nonlinear bias information for NLP solutions, without
impacting downstream performance. We compare our approach to common debiasing
methods on classical bias metrics and on bias metrics which take nonlinear
information into account.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attention-aware semantic relevance predicting Chinese sentence reading 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18542v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18542v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, several influential computational models and metrics have
been proposed to predict how humans comprehend and process sentence. One
particularly promising approach is contextual semantic similarity. Inspired by
the attention algorithm in Transformer and human memory mechanisms, this study
proposes an ``attention-aware'' approach for computing contextual semantic
relevance. This new approach takes into account the different contributions of
contextual parts and the expectation effect, allowing it to incorporate
contextual information fully. The attention-aware approach also facilitates the
simulation of existing reading models and evaluate them. The resulting
``attention-aware'' metrics of semantic relevance can more accurately predict
fixation durations in Chinese reading tasks recorded in an eye-tracking corpus
than those calculated by existing approaches. The study's findings further
provide strong support for the presence of semantic preview benefits in Chinese
naturalistic reading. Furthermore, the attention-aware metrics of semantic
relevance, being memory-based, possess high interpretability from both
linguistic and cognitive standpoints, making them a valuable computational tool
for modeling eye-movements in reading and further gaining insight into the
process of language comprehension. Our approach underscores the potential of
these metrics to advance our comprehension of how humans understand and process
language, ultimately leading to a better understanding of language
comprehension and processing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Plays a Pivotal Role in the Object-Attribute Compositional
  Generalization of CLIP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reza Abbasi, Mohammad Samiei, Mohammad Hossein Rohban, Mahdieh Soleymani Baghshah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models, such as CLIP, have shown promising
Out-of-Distribution (OoD) generalization under various types of distribution
shifts. Recent studies attempted to investigate the leading cause of this
capability. In this work, we follow the same path, but focus on a specific type
of OoD data - images with novel compositions of attribute-object pairs - and
study whether such models can successfully classify those images into
composition classes. We carefully designed an authentic image test dataset
called ImageNet-AO, consisting of attributes for objects that are unlikely
encountered in the CLIP training sets. We found that CLIPs trained with large
datasets such as OpenAI CLIP, LAION-400M, and LAION-2B show orders-of-magnitude
improvement in effective compositional OoD generalization compared to both
supervised models and CLIPs trained with smaller datasets, such as CC-12M and
YFCC-15M. Our results provide evidence that the scale and diversity of training
data and language supervision play a key role in unlocking the compositional
generalization abilities of vision-language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Oral accepted at OODCV 2023(http://www.ood-cv.org)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AcTED: Automatic Acquisition of Typical Event Duration for
  Semi-supervised Temporal Commonsense QA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Virgo, Fei Cheng, Lis Kanashiro Pereira, Masayuki Asahara, Ichiro Kobayashi, Sadao Kurohashi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a voting-driven semi-supervised approach to automatically acquire
the typical duration of an event and use it as pseudo-labeled data. The human
evaluation demonstrates that our pseudo labels exhibit surprisingly high
accuracy and balanced coverage. In the temporal commonsense QA task,
experimental results show that using only pseudo examples of 400 events, we
achieve performance comparable to the existing BERT-based weakly supervised
approaches that require a significant amount of training examples. When
compared to the RoBERTa baselines, our best approach establishes
state-of-the-art performance with a 7% improvement in Exact Match.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Language Beat Numerical Regression? Language-Based Multimodal
  Trajectory Prediction <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Inhwan Bae, Junoh Lee, Hae-Gon Jeon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models have demonstrated impressive ability in context understanding
and generative performance. Inspired by the recent success of language
foundation models, in this paper, we propose LMTraj (Language-based Multimodal
Trajectory predictor), which recasts the trajectory prediction task into a sort
of question-answering problem. Departing from traditional numerical regression
models, which treat the trajectory coordinate sequence as continuous signals,
we consider them as discrete signals like text prompts. Specially, we first
transform an input space for the trajectory coordinate into the natural
language space. Here, the entire time-series trajectories of pedestrians are
converted into a text prompt, and scene images are described as text
information through image captioning. The transformed numerical and image data
are then wrapped into the question-answering template for use in a language
model. Next, to guide the language model in understanding and reasoning
high-level knowledge, such as scene context and social relationships between
pedestrians, we introduce an auxiliary multi-task question and answering. We
then train a numerical tokenizer with the prompt data. We encourage the
tokenizer to separate the integer and decimal parts well, and leverage it to
capture correlations between the consecutive numbers in the language model.
Lastly, we train the language model using the numerical tokenizer and all of
the question-answer prompts. Here, we propose a beam-search-based most-likely
prediction and a temperature-based multimodal prediction to implement both
deterministic and stochastic inferences. Applying our LMTraj, we show that the
language-based model can be a powerful pedestrian trajectory predictor, and
outperforms existing numerical-based predictor methods. Code is publicly
available at https://github.com/inhwanbae/LMTrajectory .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DELTA: Pre-train a Discriminative Encoder for Legal Case Retrieval via
  Structural Word Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18435v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18435v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haitao Li, Qingyao Ai, Xinyan Han, Jia Chen, Qian Dong, Yiqun Liu, Chong Chen, Qi Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research demonstrates the effectiveness of using pre-trained language
models for legal case retrieval. Most of the existing works focus on improving
the representation ability for the contextualized embedding of the [CLS] token
and calculate relevance using textual semantic similarity. However, in the
legal domain, textual semantic similarity does not always imply that the cases
are relevant enough. Instead, relevance in legal cases primarily depends on the
similarity of key facts that impact the final judgment. Without proper
treatments, the discriminative ability of learned representations could be
limited since legal cases are lengthy and contain numerous non-key facts. To
this end, we introduce DELTA, a discriminative model designed for legal case
retrieval. The basic idea involves pinpointing key facts in legal cases and
pulling the contextualized embedding of the [CLS] token closer to the key facts
while pushing away from the non-key facts, which can warm up the case embedding
space in an unsupervised manner. To be specific, this study brings the word
alignment mechanism to the contextual masked auto-encoder. First, we leverage
shallow decoders to create information bottlenecks, aiming to enhance the
representation ability. Second, we employ the deep decoder to enable
translation between different structures, with the goal of pinpointing key
facts to enhance discriminative ability. Comprehensive experiments conducted on
publicly available legal benchmarks show that our approach can outperform
existing state-of-the-art methods in legal case retrieval. It provides a new
perspective on the in-depth understanding and processing of legal case
documents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring language relations through syntactic distances and geographic
  proximity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18430v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18430v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan De Gregorio, Raúl Toral, David Sánchez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Languages are grouped into families that share common linguistic traits.
While this approach has been successful in understanding genetic relations
between diverse languages, more analyses are needed to accurately quantify
their relatedness, especially in less studied linguistic levels such as syntax.
Here, we explore linguistic distances using series of parts of speech (POS)
extracted from the Universal Dependencies dataset. Within an
information-theoretic framework, we show that employing POS trigrams maximizes
the possibility of capturing syntactic variations while being at the same time
compatible with the amount of available data. Linguistic connections are then
established by assessing pairwise distances based on the POS distributions.
Intriguingly, our analysis reveals definite clusters that correspond to well
known language families and groups, with exceptions explained by distinct
morphological typologies. Furthermore, we obtain a significant correlation
between language similarity and geographic distance, which underscores the
influence of spatial proximity on language kinships.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TriviaHG: A <span class="highlight-title">Dataset</span> for Automatic Hint Generation from Factoid Questions <span class="chip">SIGIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18426v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18426v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jamshid Mozafari, Anubhav Jangra, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, individuals tend to engage in dialogues with Large Language Models,
seeking answers to their questions. In times when such answers are readily
accessible to anyone, the stimulation and preservation of human's cognitive
abilities, as well as the assurance of maintaining good reasoning skills by
humans becomes crucial. This study addresses such needs by proposing hints
(instead of final answers or before giving answers) as a viable solution. We
introduce a framework for the automatic hint generation for factoid questions,
employing it to construct TriviaHG, a novel large-scale dataset featuring
160,230 hints corresponding to 16,645 questions from the TriviaQA dataset.
Additionally, we present an automatic evaluation method that measures the
Convergence and Familiarity quality attributes of hints. To evaluate the
TriviaHG dataset and the proposed evaluation method, we enlisted 10 individuals
to annotate 2,791 hints and tasked 6 humans with answering questions using the
provided hints. The effectiveness of hints varied, with success rates of 96%,
78%, and 36% for questions with easy, medium, and hard answers, respectively.
Moreover, the proposed automatic evaluation methods showed a robust correlation
with annotators' results. Conclusively, the findings highlight three key
insights: the facilitative role of hints in resolving unknown questions, the
dependence of hint quality on answer difficulty, and the feasibility of
employing automatic evaluation methods for hint assessment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SIGIR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SemRoDe: Macro Adversarial Training to Learn Representations That are
  Robust to Word-Level Attacks <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18423v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18423v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian Formento, Wenjie Feng, Chuan Sheng Foo, Luu Anh Tuan, See-Kiong Ng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models (LMs) are indispensable tools for natural language processing
tasks, but their vulnerability to adversarial attacks remains a concern. While
current research has explored adversarial training techniques, their
improvements to defend against word-level attacks have been limited. In this
work, we propose a novel approach called Semantic Robust Defence (SemRoDe), a
Macro Adversarial Training strategy to enhance the robustness of LMs. Drawing
inspiration from recent studies in the image domain, we investigate and later
confirm that in a discrete data setting such as language, adversarial samples
generated via word substitutions do indeed belong to an adversarial domain
exhibiting a high Wasserstein distance from the base domain. Our method learns
a robust representation that bridges these two domains. We hypothesize that if
samples were not projected into an adversarial domain, but instead to a domain
with minimal shift, it would improve attack robustness. We align the domains by
incorporating a new distance-based objective. With this, our model is able to
learn more generalized representations by aligning the model's high-level
output features and therefore better handling unseen adversarial samples. This
method can be generalized across word embeddings, even when they share minimal
overlap at both vocabulary and word-substitution levels. To evaluate the
effectiveness of our approach, we conduct experiments on BERT and RoBERTa
models on three datasets. The results demonstrate promising state-of-the-art
robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in NAACL 2024 (Main Track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BLADE: Enhancing Black-box Large Language Models with Small
  Domain-Specific Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haitao Li, Qingyao Ai, Jia Chen, Qian Dong, Zhijing Wu, Yiqun Liu, Chong Chen, Qi Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) like ChatGPT and GPT-4 are versatile and capable
of addressing a diverse range of tasks. However, general LLMs, which are
developed on open-domain data, may lack the domain-specific knowledge essential
for tasks in vertical domains, such as legal, medical, etc. To address this
issue, previous approaches either conduct continuous pre-training with
domain-specific data or employ retrieval augmentation to support general LLMs.
Unfortunately, these strategies are either cost-intensive or unreliable in
practical applications. To this end, we present a novel framework named BLADE,
which enhances Black-box LArge language models with small Domain-spEcific
models. BLADE consists of a black-box LLM and a small domain-specific LM. The
small LM preserves domain-specific knowledge and offers specialized insights,
while the general LLM contributes robust language comprehension and reasoning
capabilities. Specifically, our method involves three steps: 1) pre-training
the small LM with domain-specific data, 2) fine-tuning this model using
knowledge instruction data, and 3) joint Bayesian optimization of the general
LLM and the small LM. Extensive experiments conducted on public legal and
medical benchmarks reveal that BLADE significantly outperforms existing
approaches. This shows the potential of BLADE as an effective and
cost-efficient solution in adapting general LLMs for vertical domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluation of Semantic Search and its Role in
  Retrieved-Augmented-Generation (RAG) for Arabic Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18350v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18350v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Mahboub, Muhy Eddin Za'ter, Bashar Alfrou, Yazan Estaitia, Adnan Jaljuli, Asma Hakouz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The latest advancements in machine learning and deep learning have brought
forth the concept of semantic similarity, which has proven immensely beneficial
in multiple applications and has largely replaced keyword search. However,
evaluating semantic similarity and conducting searches for a specific query
across various documents continue to be a complicated task. This complexity is
due to the multifaceted nature of the task, the lack of standard benchmarks,
whereas these challenges are further amplified for Arabic language. This paper
endeavors to establish a straightforward yet potent benchmark for semantic
search in Arabic. Moreover, to precisely evaluate the effectiveness of these
metrics and the dataset, we conduct our assessment of semantic search within
the framework of retrieval augmented generation (RAG).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rejection Improves Reliability: Training LLMs to Refuse Unknown
  Questions Using RL from Knowledge Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18349v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18349v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongshen Xu, Zichen Zhu, Da Ma, Situo Zhang, Shuai Fan, Lu Chen, Kai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) often generate erroneous outputs, known as
hallucinations, due to their limitations in discerning questions beyond their
knowledge scope. While addressing hallucination has been a focal point in
research, previous efforts primarily concentrate on enhancing correctness
without giving due consideration to the significance of rejection mechanisms.
In this paper, we conduct a comprehensive examination of the role of rejection,
introducing the notion of model reliability along with corresponding metrics.
These metrics measure the model's ability to provide accurate responses while
adeptly rejecting questions exceeding its knowledge boundaries, thereby
minimizing hallucinations. To improve the inherent reliability of LLMs, we
present a novel alignment framework called Reinforcement Learning from
Knowledge Feedback (RLKF). RLKF leverages knowledge feedback to dynamically
determine the model's knowledge boundary and trains a reliable reward model to
encourage the refusal of out-of-knowledge questions. Experimental results on
mathematical questions affirm the substantial efficacy of RLKF in significantly
enhancing LLM reliability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantifying and Mitigating Unimodal Biases in Multimodal Large Language
  Models: A Causal Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18346v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18346v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meiqi Chen, Yixin Cao, Yan Zhang, Chaochao Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) have facilitated the
development of Multimodal LLMs (MLLMs). Despite their impressive capabilities,
MLLMs often suffer from an over-reliance on unimodal biases (e.g., language
bias and vision bias), leading to incorrect answers in complex multimodal
tasks. To investigate this issue, we propose a causal framework to interpret
the biases in Visual Question Answering (VQA) problems. Within our framework,
we devise a causal graph to elucidate the predictions of MLLMs on VQA problems,
and assess the causal effect of biases through an in-depth causal analysis.
Motivated by the causal graph, we introduce a novel MORE dataset, consisting of
12,000 VQA instances. This dataset is designed to challenge MLLMs' abilities,
necessitating multi-hop reasoning and the surmounting of unimodal biases.
Furthermore, we propose two strategies to mitigate unimodal biases and enhance
MLLMs' reasoning capabilities, including a Decompose-Verify-Answer (DeVA)
framework for limited-access MLLMs and the refinement of open-source MLLMs
through fine-tuning. Extensive quantitative and qualitative experiments offer
valuable insights for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IterAlign: Iterative Constitutional Alignment of Large Language Models <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18341v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18341v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiusi Chen, Hongzhi Wen, Sreyashi Nag, Chen Luo, Qingyu Yin, Ruirui Li, Zheng Li, Wei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of large language models (LLMs), aligning LLMs
with human values and societal norms to ensure their reliability and safety has
become crucial. Reinforcement learning with human feedback (RLHF) and
Constitutional AI (CAI) have been proposed for LLM alignment. However, these
methods require either heavy human annotations or explicitly pre-defined
constitutions, which are labor-intensive and resource-consuming. To overcome
these drawbacks, we study constitution-based LLM alignment and propose a
data-driven constitution discovery and self-alignment framework called
IterAlign. IterAlign leverages red teaming to unveil the weaknesses of an LLM
and automatically discovers new constitutions using a stronger LLM. These
constitutions are then used to guide self-correction of the base LLM. Such a
constitution discovery pipeline can be run iteratively and automatically to
discover new constitutions that specifically target the alignment gaps in the
current LLM. Empirical results on several safety benchmark datasets and
multiple base LLMs show that IterAlign successfully improves truthfulness,
helpfulness, harmlessness and honesty, improving the LLM alignment by up to
$13.5\%$ in harmlessness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Dataset</span> for Pharmacovigilance in German, French, and Japanese:
  Annotating Adverse Drug Reactions across Languages <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18336v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18336v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lisa Raithel, Hui-Syuan Yeh, Shuntaro Yada, Cyril Grouin, Thomas Lavergne, Aurélie Névéol, Patrick Paroubek, Philippe Thomas, Tomohiro Nishiyama, Sebastian Möller, Eiji Aramaki, Yuji Matsumoto, Roland Roller, Pierre Zweigenbaum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User-generated data sources have gained significance in uncovering Adverse
Drug Reactions (ADRs), with an increasing number of discussions occurring in
the digital world. However, the existing clinical corpora predominantly revolve
around scientific articles in English. This work presents a multilingual corpus
of texts concerning ADRs gathered from diverse sources, including patient fora,
social media, and clinical reports in German, French, and Japanese. Our corpus
contains annotations covering 12 entity types, four attribute types, and 13
relation types. It contributes to the development of real-world multilingual
language models for healthcare. We provide statistics to highlight certain
challenges associated with the corpus and conduct preliminary experiments
resulting in strong baselines for extracting entities and relations between
these entities, both within and across languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual Instruction Tuning with Large Language Models for Mathematical
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18295v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18295v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongwei Zhou, Tiejun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements highlight the success of instruction tuning with large
language models (LLMs) utilizing Chain-of-Thought (CoT) data for mathematical
reasoning tasks. Despite the fine-tuned LLMs, challenges persist, such as
incorrect, missing, and redundant steps in CoT generation leading to
inaccuracies in answer predictions. To alleviate this problem, we propose a
dual instruction tuning strategy to meticulously model mathematical reasoning
from both forward and reverse directions. This involves introducing the
Intermediate Reasoning State Prediction task (forward reasoning) and the
Instruction Reconstruction task (reverse reasoning) to enhance the LLMs'
understanding and execution of instructions. Training instances for these tasks
are constructed based on existing mathematical instruction tuning datasets.
Subsequently, LLMs undergo multi-task fine-tuning using both existing
mathematical instructions and the newly created data. Comprehensive experiments
validate the effectiveness and domain generalization of the dual instruction
tuning strategy across various mathematical reasoning tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BlendX: Complex Multi-Intent Detection with Blended Patterns <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18277v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18277v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yejin Yoon, Jungyeon Lee, Kangsan Kim, Chanhee Park, Taeuk Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task-oriented dialogue (TOD) systems are commonly designed with the
presumption that each utterance represents a single intent. However, this
assumption may not accurately reflect real-world situations, where users
frequently express multiple intents within a single utterance. While there is
an emerging interest in multi-intent detection (MID), existing in-domain
datasets such as MixATIS and MixSNIPS have limitations in their formulation. To
address these issues, we present BlendX, a suite of refined datasets featuring
more diverse patterns than their predecessors, elevating both its complexity
and diversity. For dataset construction, we utilize both rule-based heuristics
as well as a generative tool -- OpenAI's ChatGPT -- which is augmented with a
similarity-driven strategy for utterance selection. To ensure the quality of
the proposed datasets, we also introduce three novel metrics that assess the
statistical properties of an utterance related to word count, conjunction use,
and pronoun usage. Extensive experiments on BlendX reveal that state-of-the-art
MID models struggle with the challenges posed by the new datasets, highlighting
the need to reexamine the current state of the MID field. The dataset is
available at https://github.com/HYU-NLP/BlendX.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RankMamba, Benchmarking Mamba's Document Ranking Performance in the Era
  of Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichao Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer structure has achieved great success in multiple applied machine
learning communities, such as natural language processing (NLP), computer
vision (CV) and information retrieval (IR). Transformer architecture's core
mechanism -- attention requires $O(n^2)$ time complexity in training and $O(n)$
time complexity in inference. Many works have been proposed to improve the
attention mechanism's scalability, such as Flash Attention and Multi-query
Attention. A different line of work aims to design new mechanisms to replace
attention. Recently, a notable model structure -- Mamba, which is based on
state space models, has achieved transformer-equivalent performance in multiple
sequence modeling tasks.
  In this work, we examine \mamba's efficacy through the lens of a classical IR
task -- document ranking. A reranker model takes a query and a document as
input, and predicts a scalar relevance score. This task demands the language
model's ability to comprehend lengthy contextual inputs and to capture the
interaction between query and document tokens. We find that (1) Mamba models
achieve competitive performance compared to transformer-based models with the
same training recipe; (2) but also have a lower training throughput in
comparison to efficient transformer implementations such as flash attention. We
hope this study can serve as a starting point to explore Mamba models in other
classical IR tasks. Our code implementation and trained checkpoints are made
public to facilitate
reproducibility.\footnote{https://github.com/zhichaoxu-shufe/RankMamba}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward Interactive Regional Understanding in Vision-Large Language
  Models <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18260v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18260v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jungbeom Lee, Sanghyuk Chun, Sangdoo Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent Vision-Language Pre-training (VLP) models have demonstrated
significant advancements. Nevertheless, these models heavily rely on image-text
pairs that capture only coarse and global information of an image, leading to a
limitation in their regional understanding ability. In this work, we introduce
\textbf{RegionVLM}, equipped with explicit regional modeling capabilities,
allowing them to understand user-indicated image regions. To achieve this, we
design a simple yet innovative architecture, requiring no modifications to the
model architecture or objective function. Additionally, we leverage a dataset
that contains a novel source of information, namely Localized Narratives, which
has been overlooked in previous VLP research. Our experiments demonstrate that
our single generalist model not only achieves an interactive dialogue system
but also exhibits superior performance on various zero-shot region
understanding tasks, without compromising its ability for global image
understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MD-PK: Metaphor Detection via Prompt Learning and Knowledge Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18253v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18253v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaidi Jia, Rongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Metaphors are ubiquitous in daily life, yet detecting them poses a
significant challenge. Previous approaches often struggled with improper
application of language rules and overlooked the issue of data sparsity. To
address these challenges, we introduce knowledge distillation and prompt
learning into metaphor detection. Specifically, we devise a prompt learning
template tailored for the metaphor detection task. By masking target words and
providing relevant prompt information, we guide the model to accurately infer
the contextual meaning of these words. This approach not only mitigates the
interference from the literal meaning of target words but also ensures the
proper utilization of MIP language rules for metaphor detection. Moreover, we
employ a teacher model equipped with prior knowledge to generate meaningful
soft labels, guiding the optimization process of the student model. The
inclusion of soft labels, akin to label smoothing, helps alleviate the model's
tendency towards over-confidence and effectively addresses the challenge of
data sparsity. Experimental results demonstrate that our proposed model
achieves state-of-the-art performance across multiple datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Since the Scientific Literature Is Multilingual, Our Models Should Be
  Too 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18251v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18251v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abteen Ebrahimi, Kenneth Church
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  English has long been assumed the $\textit{lingua franca}$ of scientific
research, and this notion is reflected in the natural language processing (NLP)
research involving scientific document representation. In this position piece,
we quantitatively show that the literature is largely multilingual and argue
that current models and benchmarks should reflect this linguistic diversity. We
provide evidence that text-based models fail to create meaningful
representations for non-English papers and highlight the negative user-facing
impacts of using English-only models non-discriminately across a multilingual
domain. We end with suggestions for the NLP community on how to improve
performance on non-English documents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Deceptive Power of LLM-Generated Fake News: A Study of
  Real-World Detection Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18249v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18249v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanshen Sun, Jianfeng He, Limeng Cui, Shuo Lei, Chang-Tien Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) have enabled the creation
of fake news, particularly in complex fields like healthcare. Studies highlight
the gap in the deceptive power of LLM-generated fake news with and without
human assistance, yet the potential of prompting techniques has not been fully
explored. Thus, this work aims to determine whether prompting strategies can
effectively narrow this gap. Current LLM-based fake news attacks require human
intervention for information gathering and often miss details and fail to
maintain context consistency. Therefore, to better understand threat tactics,
we propose a strong fake news attack method called conditional
Variational-autoencoder-Like Prompt (VLPrompt). Unlike current methods,
VLPrompt eliminates the need for additional data collection while maintaining
contextual coherence and preserving the intricacies of the original text. To
propel future research on detecting VLPrompt attacks, we created a new dataset
named VLPrompt fake news (VLPFN) containing real and fake texts. Our
experiments, including various detection methods and novel human study metrics,
were conducted to assess their performance on our dataset, yielding numerous
findings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ZAEBUC-Spoken: A Multilingual Multidialectal Arabic-English Speech
  Corpus <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Injy Hamed, Fadhl Eryani, David Palfreyman, Nizar Habash
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present ZAEBUC-Spoken, a multilingual multidialectal Arabic-English speech
corpus. The corpus comprises twelve hours of Zoom meetings involving multiple
speakers role-playing a work situation where Students brainstorm ideas for a
certain topic and then discuss it with an Interlocutor. The meetings cover
different topics and are divided into phases with different language setups.
The corpus presents a challenging set for automatic speech recognition (ASR),
including two languages (Arabic and English) with Arabic spoken in multiple
variants (Modern Standard Arabic, Gulf Arabic, and Egyptian Arabic) and English
used with various accents. Adding to the complexity of the corpus, there is
also code-switching between these languages and dialects. As part of our work,
we take inspiration from established sets of transcription guidelines to
present a set of guidelines handling issues of conversational speech,
code-switching and orthography of both languages. We further enrich the corpus
with two layers of annotations; (1) dialectness level annotation for the
portion of the corpus where mixing occurs between different variants of Arabic,
and (2) automatic morphological annotations, including tokenization,
lemmatization, and part-of-speech tagging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A 4D Hybrid Algorithm to Scale Parallel Training to Thousands of GPUs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.13525v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.13525v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddharth Singh, Prajwal Singhania, Aditya K. Ranjan, Zack Sating, Abhinav Bhatele
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large communication costs are a critical bottleneck in training
state-of-the-art neural networks on distributed systems. This paper introduces
AxoNN, a novel four-dimensional (4D) parallelization approach, inspired by
Agarwal's algorithm for matrix multiplication, for parallelizing tensor
computations in deep learning, AxoNN employs two key strategies to minimize
communication overhead. First, we optimize communication by overlapping
expensive collective operations (reduce-scatter, all-gather, all-reduce) with
computations. Our experiments with a 20-billion parameter transformer model
demonstrate that these optimizations deliver nearly 53\% improvement. Second,
we present an analytical model to assist users in identifying
communication-minimizing configurations within the vast search space defined by
our 4D algorithm. This model empowers practitioners by simplifying the tuning
process for their specific training workloads. When training an 80-billion
parameter model on 1024 GPUs of Perlmutter, AxoNN surpasses Megatron-LM, a
state-of-the-art framework, by a significant 26%. Additionally, it achieves 57%
of the theoretical peak FLOP/s.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shifting to Machine Supervision: Annotation-Efficient Semi and
  Self-Supervised Learning for Automatic Medical Image Segmentation and
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10319v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10319v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranav Singh, Raviteja Chukkapalli, Shravan Chaudhari, Luoyao Chen, Mei Chen, Jinqian Pan, Craig Smuda, Jacopo Cirrone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advancements in clinical treatment are increasingly constrained by the
limitations of supervised learning techniques, which depend heavily on large
volumes of annotated data. The annotation process is not only costly but also
demands substantial time from clinical specialists. Addressing this issue, we
introduce the S4MI (Self-Supervision and Semi-Supervision for Medical Imaging)
pipeline, a novel approach that leverages advancements in self-supervised and
semi-supervised learning. These techniques engage in auxiliary tasks that do
not require labeling, thus simplifying the scaling of machine supervision
compared to fully-supervised methods. Our study benchmarks these techniques on
three distinct medical imaging datasets to evaluate their effectiveness in
classification and segmentation tasks. Notably, we observed that self
supervised learning significantly surpassed the performance of supervised
methods in the classification of all evaluated datasets. Remarkably, the
semi-supervised approach demonstrated superior outcomes in segmentation,
outperforming fully-supervised methods while using 50% fewer labels across all
datasets. In line with our commitment to contributing to the scientific
community, we have made the S4MI code openly accessible, allowing for broader
application and further development of these methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Seventeen pages (incl. references), five figures, and one table.
  (Under Review)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Agent</span>-Pro: Learning to Evolve via <span class="highlight-title">Policy</span>-Level Reflection and
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17574v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17574v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqi Zhang, Ke Tang, Hai Wu, Mengna Wang, Yongliang Shen, Guiyang Hou, Zeqi Tan, Peng Li, Yueting Zhuang, Weiming Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models exhibit robust problem-solving capabilities for diverse
tasks. However, most LLM-based agents are designed as specific task solvers
with sophisticated prompt engineering, rather than agents capable of learning
and evolving through interactions. These task solvers necessitate manually
crafted prompts to inform task rules and regulate LLM behaviors, inherently
incapacitating to address complex dynamic scenarios e.g., large interactive
games. In light of this, we propose Agent-Pro: an LLM-based Agent with
Policy-level Reflection and Optimization that can learn a wealth of expertise
from interactive experiences and progressively elevate its behavioral policy.
Specifically, it involves a dynamic belief generation and reflection process
for policy evolution. Rather than action-level reflection, Agent-Pro
iteratively reflects on past trajectories and beliefs, fine-tuning its
irrational beliefs for a better policy. Moreover, a depth-first search is
employed for policy optimization, ensuring continual enhancement in policy
payoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em,
outperforming vanilla LLM and specialized models. Our results show Agent-Pro
can learn and evolve in complex and dynamic scenes, which also benefits
numerous LLM-based applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LLM-based Agent</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Contrast: Better Reflection Through Inconsistent Solving
  Perspectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02009v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02009v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqi Zhang, Yongliang Shen, Linjuan Wu, Qiuying Peng, Jun Wang, Yueting Zhuang, Weiming Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The reflection capacity of Large Language Model (LLM) has garnered extensive
attention. A post-hoc prompting strategy, e.g., reflexion and self-refine,
refines LLM's response based on self-evaluated or external feedback. However,
recent research indicates without external feedback, LLM's intrinsic reflection
is unstable. Our investigation unveils that the key bottleneck is the quality
of the self-evaluated feedback. We find LLMs often exhibit overconfidence or
high randomness when self-evaluate, offering stubborn or inconsistent feedback,
which causes poor reflection. To remedy this, we advocate Self-Contrast: It
adaptively explores diverse solving perspectives tailored to the request,
contrasts the differences, and summarizes these discrepancies into a checklist
which could be used to re-examine and eliminate discrepancies. Our method
endows LLM with diverse perspectives to alleviate stubborn biases. Moreover,
their discrepancies indicate potential errors or inherent uncertainties that
LLM often overlooks. Reflecting upon these can catalyze more accurate and
stable reflection. Experiments conducted on a series of reasoning and
translation tasks with different LLMs serve to underscore the effectiveness and
generality of our strategy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalization Bounds: Perspectives from Information Theory and
  PAC-Bayes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.04381v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.04381v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fredrik Hellström, Giuseppe Durisi, Benjamin Guedj, Maxim Raginsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A fundamental question in theoretical machine learning is generalization.
Over the past decades, the PAC-Bayesian approach has been established as a
flexible framework to address the generalization capabilities of machine
learning algorithms, and design new ones. Recently, it has garnered increased
interest due to its potential applicability for a variety of learning
algorithms, including deep neural networks. In parallel, an
information-theoretic view of generalization has developed, wherein the
relation between generalization and various information measures has been
established. This framework is intimately connected to the PAC-Bayesian
approach, and a number of results have been independently discovered in both
strands. In this monograph, we highlight this strong connection and present a
unified treatment of PAC-Bayesian and information-theoretic generalization
bounds. We present techniques and results that the two perspectives have in
common, and discuss the approaches and interpretations that differ. In
particular, we demonstrate how many proofs in the area share a modular
structure, through which the underlying ideas can be intuited. We pay special
attention to the conditional mutual information (CMI) framework; analytical
studies of the information complexity of learning algorithms; and the
application of the proposed methods to deep learning. This monograph is
intended to provide a comprehensive introduction to information-theoretic
generalization bounds and their connection to PAC-Bayes, serving as a
foundation from which the most recent developments are accessible. It is aimed
broadly towards researchers with an interest in generalization and theoretical
machine learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>228 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decoupled Data Consistency with Diffusion Purification for Image
  Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06054v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06054v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Soo Min Kwon, Ismail R. Alkhouri, Saiprasad Ravishankar, Qing Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have recently gained traction as a powerful class of deep
generative priors, excelling in a wide range of image restoration tasks due to
their exceptional ability to model data distributions. To solve image
restoration problems, many existing techniques achieve data consistency by
incorporating additional likelihood gradient steps into the reverse sampling
process of diffusion models. However, the additional gradient steps pose a
challenge for real-world practical applications as they incur a large
computational overhead, thereby increasing inference time. They also present
additional difficulties when using accelerated diffusion model samplers, as the
number of data consistency steps is limited by the number of reverse sampling
steps. In this work, we propose a novel diffusion-based image restoration
solver that addresses these issues by decoupling the reverse process from the
data consistency steps. Our method involves alternating between a
reconstruction phase to maintain data consistency and a refinement phase that
enforces the prior via diffusion purification. Our approach demonstrates
versatility, making it highly adaptable for efficient problem-solving in latent
space. Additionally, it reduces the necessity for numerous sampling steps
through the integration of consistency models. The efficacy of our approach is
validated through comprehensive experiments across various image restoration
tasks, including image denoising, deblurring, inpainting, and super-resolution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedSN: A Novel Federated Learning Framework over LEO Satellite Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01483v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01483v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Lin, Zhe Chen, Zihan Fang, Xianhao Chen, Xiong Wang, Yue Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, a large number of Low Earth Orbit (LEO) satellites have been
launched and deployed successfully in space by commercial companies, such as
SpaceX. Due to multimodal sensors equipped by the LEO satellites, they serve
not only for communication but also for various machine learning applications,
such as space modulation recognition, remote sensing image classification, etc.
However, the ground station (GS) may be incapable of downloading such a large
volume of raw sensing data for centralized model training due to the limited
contact time with LEO satellites (e.g. 5 minutes). Therefore, federated
learning (FL) has emerged as the promising solution to address this problem via
on-device training. Unfortunately, to enable FL on LEO satellites, we still
face three critical challenges that are i) heterogeneous computing and memory
capabilities, ii) limited uplink rate, and iii) model staleness. To this end,
we propose FedSN as a general FL framework to tackle the above challenges, and
fully explore data diversity on LEO satellites. Specifically, we first present
a novel sub-structure scheme to enable heterogeneous local model training
considering different computing, memory, and communication constraints on LEO
satellites. Additionally, we propose a pseudo-synchronous model aggregation
strategy to dynamically schedule model aggregation for compensating model
staleness. To further demonstrate the effectiveness of the FedSN, we evaluate
it using space modulation recognition and remote sensing image classification
tasks by leveraging the data from real-world satellite networks. Extensive
experimental results demonstrate that FedSN framework achieves higher accuracy,
lower computing, and communication overhead than the state-of-the-art
benchmarks and the effectiveness of each components in FedSN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nonlinear Control Allocation: A Learning Based Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.06180v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.06180v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hafiz Zeeshan Iqbal Khan, Surrayya Mobeen, Jahanzeb Rajput, Jamshed Riaz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern aircraft are designed with redundant control effectors to cater for
fault tolerance and maneuverability requirements. This leads to aircraft being
over-actuated and requires control allocation schemes to distribute the control
commands among control effectors. Traditionally, optimization-based control
allocation schemes are used; however, for nonlinear allocation problems, these
methods require large computational resources. In this work, an artificial
neural network (ANN) based nonlinear control allocation scheme is proposed. The
proposed scheme is composed of learning the inverse of the control
effectiveness map through ANN, and then implementing it as an allocator instead
of solving an online optimization problem. Stability conditions are presented
for closed-loop systems incorporating the allocator, and computational
challenges are explored with piece-wise linear effectiveness functions and
ANN-based allocators. To demonstrate the efficacy of the proposed scheme, it is
compared with a standard quadratic programming-based method for control
allocation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IEEE Conference on Decision and Control (CDC), 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03100v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03100v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeqian Ju, Yuancheng Wang, Kai Shen, Xu Tan, Detai Xin, Dongchao Yang, Yanqing Liu, Yichong Leng, Kaitao Song, Siliang Tang, Zhizheng Wu, Tao Qin, Xiang-Yang Li, Wei Ye, Shikun Zhang, Jiang Bian, Lei He, Jinyu Li, Sheng Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recent large-scale text-to-speech (TTS) models have achieved
significant progress, they still fall short in speech quality, similarity, and
prosody. Considering speech intricately encompasses various attributes (e.g.,
content, prosody, timbre, and acoustic details) that pose significant
challenges for generation, a natural idea is to factorize speech into
individual subspaces representing different attributes and generate them
individually. Motivated by it, we propose NaturalSpeech 3, a TTS system with
novel factorized diffusion models to generate natural speech in a zero-shot
way. Specifically, 1) we design a neural codec with factorized vector
quantization (FVQ) to disentangle speech waveform into subspaces of content,
prosody, timbre, and acoustic details; 2) we propose a factorized diffusion
model to generate attributes in each subspace following its corresponding
prompt. With this factorization design, NaturalSpeech 3 can effectively and
efficiently model intricate speech with disentangled subspaces in a
divide-and-conquer way. Experiments show that NaturalSpeech 3 outperforms the
state-of-the-art TTS systems on quality, similarity, prosody, and
intelligibility, and achieves on-par quality with human recordings.
Furthermore, we achieve better performance by scaling to 1B parameters and 200K
hours of training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Achieving human-level quality and naturalness on multi-speaker
  datasets (e.g., LibriSpeech) in a zero-shot way</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and
  Ethics) Evaluation: A <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.03123v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.03123v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunder Ali Khowaja, Parus Khuwaja, Kapal Dev, Weizheng Wang, Lewis Nkenyereye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ChatGPT is another large language model (LLM) vastly available for the
consumers on their devices but due to its performance and ability to converse
effectively, it has gained a huge popularity amongst research as well as
industrial community. Recently, many studies have been published to show the
effectiveness, efficiency, integration, and sentiments of chatGPT and other
LLMs. In contrast, this study focuses on the important aspects that are mostly
overlooked, i.e. sustainability, privacy, digital divide, and ethics and
suggests that not only chatGPT but every subsequent entry in the category of
conversational bots should undergo Sustainability, PrivAcy, Digital divide, and
Ethics (SPADE) evaluation. This paper discusses in detail the issues and
concerns raised over chatGPT in line with aforementioned characteristics. We
also discuss the recent EU AI Act briefly in accordance with the SPADE
evaluation. We support our hypothesis by some preliminary data collection and
visualizations along with hypothesized facts. We also suggest mitigations and
recommendations for each of the concerns. Furthermore, we also suggest some
policies and recommendations for EU AI policy act concerning ethics, digital
divide, and sustainability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 8 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Incorporating simulated spatial context information improves the
  effectiveness of contrastive learning models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.15120v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.15120v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lizhen Zhu, James Z. Wang, Wonseuk Lee, Brad Wyble
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual learning often occurs in a specific context, where an agent acquires
skills through exploration and tracking of its location in a consistent
environment. The historical spatial context of the agent provides a similarity
signal for self-supervised contrastive learning. We present a unique approach,
termed Environmental Spatial Similarity (ESS), that complements existing
contrastive learning methods. Using images from simulated, photorealistic
environments as an experimental setting, we demonstrate that ESS outperforms
traditional instance discrimination approaches. Moreover, sampling additional
data from the same environment substantially improves accuracy and provides new
augmentations. ESS allows remarkable proficiency in room classification and
spatial prediction tasks, especially in unfamiliar environments. This learning
paradigm has the potential to enable rapid visual learning in agents operating
in new environments with unique visual characteristics. Potentially
transformative applications span from robotics to space exploration. Our proof
of concept demonstrates improved efficiency over methods that rely on
extensive, disconnected datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stochastic Approximation with Delayed Updates: Finite-Time Rates under
  Markovian Sampling <span class="chip">AISTATS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11800v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11800v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arman Adibi, Nicolo Dal Fabbro, Luca Schenato, Sanjeev Kulkarni, H. Vincent Poor, George J. Pappas, Hamed Hassani, Aritra Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by applications in large-scale and multi-agent reinforcement
learning, we study the non-asymptotic performance of stochastic approximation
(SA) schemes with delayed updates under Markovian sampling. While the effect of
delays has been extensively studied for optimization, the manner in which they
interact with the underlying Markov process to shape the finite-time
performance of SA remains poorly understood. In this context, our first main
contribution is to show that under time-varying bounded delays, the delayed SA
update rule guarantees exponentially fast convergence of the \emph{last
iterate} to a ball around the SA operator's fixed point. Notably, our bound is
\emph{tight} in its dependence on both the maximum delay $\tau_{max}$, and the
mixing time $\tau_{mix}$. To achieve this tight bound, we develop a novel
inductive proof technique that, unlike various existing delayed-optimization
analyses, relies on establishing uniform boundedness of the iterates. As such,
our proof may be of independent interest. Next, to mitigate the impact of the
maximum delay on the convergence rate, we provide the first finite-time
analysis of a delay-adaptive SA scheme under Markovian sampling. In particular,
we show that the exponent of convergence of this scheme gets scaled down by
$\tau_{avg}$, as opposed to $\tau_{max}$ for the vanilla delayed SA rule; here,
$\tau_{avg}$ denotes the average delay across all iterations. Moreover, the
adaptive scheme requires no prior knowledge of the delay sequence for step-size
tuning. Our theoretical findings shed light on the finite-time effects of
delays for a broad class of algorithms, including TD learning, Q-learning, and
stochastic gradient descent under Markovian sampling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 27th International Conference on Artificial
  Intelligence and Statistics (AISTATS) 2024!</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Negative Evidential Deep Learning for Open-set Semi-supervised
  Learning <span class="chip">AAAI2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12091v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12091v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Yu, Danruo Deng, Furui Liu, Yueming Jin, Qi Dou, Guangyong Chen, Pheng-Ann Heng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised learning (SSL) methods assume that labeled data, unlabeled
data and test data are from the same distribution. Open-set semi-supervised
learning (Open-set SSL) considers a more practical scenario, where unlabeled
data and test data contain new categories (outliers) not observed in labeled
data (inliers). Most previous works focused on outlier detection via binary
classifiers, which suffer from insufficient scalability and inability to
distinguish different types of uncertainty. In this paper, we propose a novel
framework, Adaptive Negative Evidential Deep Learning (ANEDL) to tackle these
limitations. Concretely, we first introduce evidential deep learning (EDL) as
an outlier detector to quantify different types of uncertainty, and design
different uncertainty metrics for self-training and inference. Furthermore, we
propose a novel adaptive negative optimization strategy, making EDL more
tailored to the unlabeled dataset containing both inliers and outliers. As
demonstrated empirically, our proposed method outperforms existing
state-of-the-art methods across four datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental
  Health Sensing Studies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17219v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17219v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshat Choube, Vedant Das Swain, Varun Mishra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in mobile and wearable technologies have enabled the potential to
passively monitor a person's mental, behavioral, and affective health. These
approaches typically rely on longitudinal collection of self-reported outcomes,
e.g., depression, stress, and anxiety, to train machine learning (ML) models.
However, the need to continuously self-report adds a significant burden on the
participants, often resulting in attrition, missing labels, or insincere
responses. In this work, we introduce the Scale Scores Simulation using Mental
Models (SeSaMe) framework to alleviate participants' burden in digital mental
health studies. By leveraging pre-trained large language models (LLMs), SeSaMe
enables the simulation of participants' responses on psychological scales. In
SeSaMe, researchers can prompt LLMs with information on participants' internal
behavioral dispositions, enabling LLMs to construct mental models of
participants to simulate their responses on psychological scales. We
demonstrate an application of SeSaMe, where we use GPT-4 to simulate responses
on one scale using responses from another as behavioral information. We also
evaluate the alignment between human and SeSaMe-simulated responses to
psychological scales. Then, we present experiments to inspect the utility of
SeSaMe-simulated responses as ground truth in training ML models by replicating
established depression and anxiety screening tasks from a previous study. Our
results indicate SeSaMe to be a promising approach, but its alignment may vary
across scales and specific prediction objectives. We also observed that model
performance with simulated data was on par with using the real data for
training in most evaluation scenarios. We conclude by discussing the potential
implications of SeSaMe in addressing some challenges researchers face with
ground-truth collection in passive sensing studies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Byzantine-resilient Federated Learning With Adaptivity to Data
  Heterogeneity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13374v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13374v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyuan Zuo, Xingrun Yan, Rongfei Fan, Han Hu, Hangguan Shan, Tony Q. S. Quek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper deals with federated learning (FL) in the presence of malicious
Byzantine attacks and data heterogeneity. A novel Robust Average Gradient
Algorithm (RAGA) is proposed, which leverages the geometric median for
aggregation and can freely select the round number for local updating.
Different from most existing resilient approaches, which perform convergence
analysis based on strongly-convex loss function or homogeneously distributed
dataset, we conduct convergence analysis for not only strongly-convex but also
non-convex loss function over heterogeneous dataset. According to our
theoretical analysis, as long as the fraction of dataset from malicious users
is less than half, RAGA can achieve convergence at rate
$\mathcal{O}({1}/{T^{2/3- \delta}})$ where $T$ is the iteration number and
$\delta \in (0, 2/3)$ for non-convex loss function, and at linear rate for
strongly-convex loss function. Moreover, stationary point or global optimal
solution is proved to obtainable as data heterogeneity vanishes. Experimental
results corroborate the robustness of RAGA to Byzantine attacks and verifies
the advantage of RAGA over baselines on convergence performance under various
intensity of Byzantine attacks, for heterogeneous dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Demystifying Misconceptions in Social Bots Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.17251v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.17251v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefano Cresci, Kai-Cheng Yang, Angelo Spognardi, Roberto Di Pietro, Filippo Menczer, Marinella Petrocchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research on social bots aims at advancing knowledge and providing solutions
to one of the most debated forms of online manipulation. Yet, social bot
research is plagued by widespread biases, hyped results, and misconceptions
that set the stage for ambiguities, unrealistic expectations, and seemingly
irreconcilable findings. Overcoming such issues is instrumental towards
ensuring reliable solutions and reaffirming the validity of the scientific
method. In this contribution, we review some recent results in social bots
research, highlighting and revising factual errors as well as methodological
and conceptual biases. More importantly, we demystify common misconceptions,
addressing fundamental points on how social bots research is discussed. Our
analysis surfaces the need to discuss research about online disinformation and
manipulation in a rigorous, unbiased, and responsible way. This article
bolsters such effort by identifying and refuting common fallacious arguments
used by both proponents and opponents of social bots research, as well as
providing directions toward sound methodologies for future research in the
field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeepMachining: Online Prediction of Machining Errors of Lathe Machines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16451v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16451v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang-Li Lu, Hwai-Jung Hsu, Che-Wei Chou, H. T. Kung, Chen-Hsin Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We describe DeepMachining, a deep learning-based AI system for online
prediction of machining errors of lathe machine operations. We have built and
evaluated DeepMachining based on manufacturing data from factories.
Specifically, we first pretrain a deep learning model for a given lathe
machine's operations to learn the salient features of machining states. Then,
we fine-tune the pretrained model to adapt to specific machining tasks. We
demonstrate that DeepMachining achieves high prediction accuracy for multiple
tasks that involve different workpieces and cutting tools. To the best of our
knowledge, this work is one of the first factory experiments using pre-trained
deep-learning models to predict machining errors of lathe machines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Structure Guided Large Language Model for SQL Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13284v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13284v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinggang Zhang, Junnan Dong, Hao Chen, Wentao Li, Feiran Huang, Xiao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating accurate Structured Querying Language (SQL) is a long-standing
problem, especially in matching users' semantic queries with structured
databases and then generating structured SQL. Existing models typically input
queries and database schemas into the LLM and rely on the LLM to perform
semantic-structure matching and generate structured SQL. However, such
solutions overlook the structural information within user queries and
databases, which can be utilized to enhance the generation of structured SQL.
This oversight can lead to inaccurate or unexecutable SQL generation. To fully
exploit the structure, we propose a structure-to-SQL framework, which leverages
the inherent structure information to improve the SQL generation of LLMs.
Specifically, we introduce our Structure Guided SQL~(SGU-SQL) generation model.
SGU-SQL first links user queries and databases in a structure-enhanced manner.
It then decomposes complicated linked structures with grammar trees to guide
the LLM to generate the SQL step by step. Extensive experiments on two
benchmark datasets illustrate that SGU-SQL can outperform sixteen SQL
generation baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recurrent Action Transformer with Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.09459v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.09459v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexey Staroverov, Egor Cherepanov, Dmitry Yudin, Alexey K. Kovalev, Aleksandr I. Panov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the use of transformers in offline reinforcement learning has
become a rapidly developing area. This is due to their ability to treat the
agent's trajectory in the environment as a sequence, thereby reducing the
policy learning problem to sequence modeling. In environments where the agent's
decisions depend on past events, it is essential to capture both the event
itself and the decision point in the context of the model. However, the
quadratic complexity of the attention mechanism limits the potential for
context expansion. One solution to this problem is to enhance transformers with
memory mechanisms. In this paper, we propose the Recurrent Action Transformer
with Memory (RATE) - a model that incorporates recurrent memory. To evaluate
our model, we conducted extensive experiments on both memory-intensive
environments (VizDoom-Two-Color, T-Maze) and classic Atari games and MuJoCo
control environments. The results show that the use of memory can significantly
improve performance in memory-intensive environments while maintaining or
improving results in classic environments. We hope that our findings will
stimulate research on memory mechanisms for transformers applicable to offline
reinforcement learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Pre-Training of Time-Series Data for Unsupervised Fault
  Detection in Semiconductor Manufacturing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11427v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11427v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sewoong Lee, JinKyou Choi, Min Su Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces TRACE-GPT, which stands for Time-seRies
Anomaly-detection with Convolutional Embedding and Generative Pre-trained
Transformers. TRACE-GPT is designed to pre-train univariate time-series sensor
data and detect faults on unlabeled datasets in semiconductor manufacturing. In
semiconductor industry, classifying abnormal time-series sensor data from
normal data is important because it is directly related to wafer defect.
However, small, unlabeled, and even mixed training data without enough
anomalies make classification tasks difficult. In this research, we capture
features of time-series data with temporal convolutional embedding and
Generative Pre-trained Transformer (GPT) to classify abnormal sequences from
normal sequences using cross entropy loss. We prove that our model shows better
performance than previous unsupervised models with both an open dataset, the
University of California Riverside (UCR) time-series classification archive,
and the process log of our Chemical Vapor Deposition (CVD) equipment. Our model
has the highest F1 score at Equal Error Rate (EER) across all datasets and is
only 0.026 below the supervised state-of-the-art baseline on the open dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attacks, Defenses and Evaluations for LLM Conversation Safety: A <span class="highlight-title">Survey</span> <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09283v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09283v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichen Dong, Zhanhui Zhou, Chao Yang, Jing Shao, Yu Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are now commonplace in conversation
applications. However, their risks of misuse for generating harmful responses
have raised serious societal concerns and spurred recent research on LLM
conversation safety. Therefore, in this survey, we provide a comprehensive
overview of recent studies, covering three critical aspects of LLM conversation
safety: attacks, defenses, and evaluations. Our goal is to provide a structured
summary that enhances understanding of LLM conversation safety and encourages
further investigation into this important subject. For easy reference, we have
categorized all the studies mentioned in this survey according to our taxonomy,
available at: https://github.com/niconi19/LLM-conversation-safety.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shapley Values-Powered Framework for Fair Reward Split in Content
  Produced by GenAI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09700v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09700v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Glinsky, Alexey Sokolsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is evident that, currently, generative models are surpassed in quality by
human professionals. However, with the advancements in Artificial Intelligence,
this gap will narrow, leading to scenarios where individuals who have dedicated
years of their lives to mastering a skill become obsolete due to their high
costs, which are inherently linked to the time they require to complete a task
-- a task that AI could accomplish in minutes or seconds. To avoid future
social upheavals, we must, even now, contemplate how to fairly assess the
contributions of such individuals in training generative models and how to
compensate them for the reduction or complete loss of their incomes. In this
work, we propose a method to structure collaboration between model developers
and data providers. To achieve this, we employ Shapley Values to quantify the
contribution of artist(s) in an image generated by the Stable Diffusion-v1.5
model and to equitably allocate the reward among them.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 32 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ABScribe: Rapid <span class="highlight-title">Exploration</span> & Organization of Multiple Writing
  Variations in Human-AI Co-Writing Tasks using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00117v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00117v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohi Reza, Nathan Laundry, Ilya Musabirov, Peter Dushniku, Zhi Yuan "Michael" Yu, Kashish Mittal, Tovi Grossman, Michael Liut, Anastasia Kuzminykh, Joseph Jay Williams
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exploring alternative ideas by rewriting text is integral to the writing
process. State-of-the-art Large Language Models (LLMs) can simplify writing
variation generation. However, current interfaces pose challenges for
simultaneous consideration of multiple variations: creating new variations
without overwriting text can be difficult, and pasting them sequentially can
clutter documents, increasing workload and disrupting writers' flow. To tackle
this, we present ABScribe, an interface that supports rapid, yet visually
structured, exploration and organization of writing variations in human-AI
co-writing tasks. With ABScribe, users can swiftly modify variations using LLM
prompts, which are auto-converted into reusable buttons. Variations are stored
adjacently within text fields for rapid in-place comparisons using mouse-over
interactions on a popup toolbar. Our user study with 12 writers shows that
ABScribe significantly reduces task workload (d = 1.20, p < 0.001), enhances
user perceptions of the revision process (d = 2.41, p < 0.001) compared to a
popular baseline workflow, and provides insights into how writers explore
variations using LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CHI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CroSel: Cross <span class="highlight-title">Selection</span> of Confident Pseudo Labels for Partial-Label
  Learning <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10365v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10365v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyu Tian, Hongxin Wei, Yiqun Wang, Lei Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Partial-label learning (PLL) is an important weakly supervised learning
problem, which allows each training example to have a candidate label set
instead of a single ground-truth label. Identification-based methods have been
widely explored to tackle label ambiguity issues in PLL, which regard the true
label as a latent variable to be identified. However, identifying the true
labels accurately and completely remains challenging, causing noise in pseudo
labels during model training. In this paper, we propose a new method called
CroSel, which leverages historical predictions from the model to identify true
labels for most training examples. First, we introduce a cross selection
strategy, which enables two deep models to select true labels of partially
labeled data for each other. Besides, we propose a novel consistency
regularization term called co-mix to avoid sample waste and tiny noise caused
by false selection. In this way, CroSel can pick out the true labels of most
examples with high precision. Extensive experiments demonstrate the superiority
of CroSel, which consistently outperforms previous state-of-the-art methods on
benchmark datasets. Additionally, our method achieves over 90\% accuracy and
quantity for selecting true labels on CIFAR-type datasets under various
settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Challenging Common Paradigms in Multi-Task Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.04698v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.04698v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cathrin Elich, Lukas Kirchdorfer, Jan M. Köhler, Lukas Schott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While multi-task learning (MTL) has gained significant attention in recent
years, its underlying mechanisms remain poorly understood. Recent methods did
not yield consistent performance improvements over single task learning (STL)
baselines, underscoring the importance of gaining more profound insights about
challenges specific to MTL. In our study, we challenge paradigms in MTL in the
context of STL: First, the impact of the choice of optimizer has only been
mildly investigated in MTL. We show the pivotal role of common STL tools such
as the Adam optimizer in MTL empirically in various experiments. To further
investigate Adam's effectiveness, we theoretical derive a partial loss-scale
invariance under mild assumptions. Second, the notion of gradient conflicts has
often been phrased as a specific problem in MTL. We delve into the role of
gradient conflicts in MTL and compare it to STL. For angular gradient alignment
we find no evidence that this is a unique problem in MTL. We emphasize
differences in gradient magnitude as the main distinguishing factor. Lastly, we
compare the transferability of features learned through MTL and STL on common
image corruptions, and find light evidence that MTL can lead to superior
transferability. Overall, we find surprising similarities between STL and MTL
suggesting to consider methods from both fields in a broader context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>-</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Solving a Real-World Package Delivery Routing Problem Using Quantum
  Annealers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15114v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15114v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eneko Osaba, Esther Villar-Rodriguez, Antón Asla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research focused on the conjunction between quantum computing and routing
problems has been very prolific in recent years. Most of the works revolve
around classical problems such as the Traveling Salesman Problem or the Vehicle
Routing Problem. Even though working on these problems is valuable, it is also
undeniable that their academic-oriented nature falls short of real-world
requirements. The main objective of this research is to present a solving
method for realistic instances, avoiding problem relaxations or technical
shortcuts. Instead, a quantum-classical hybrid solver has been developed,
coined Q4RPD, that considers a set of real constraints such as a heterogeneous
fleet of vehicles, priority deliveries, and capacities characterized by two
values: weight and dimensions of the packages. Q4RPD resorts to the Leap
Constrained Quadratic Model Hybrid Solver of D-Wave. To demonstrate the
application of Q4RPD, an experimentation composed of six different instances
has been conducted, aiming to serve as illustrative examples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 11 figures and 4 tables. Paper submitted for review in
  Scientific Reports</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hourglass Tokenizer for Efficient Transformer-Based 3D Human Pose
  Estimation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12028v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12028v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Li, Mengyuan Liu, Hong Liu, Pichao Wang, Jialun Cai, Nicu Sebe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have been successfully applied in the field of video-based 3D
human pose estimation. However, the high computational costs of these video
pose transformers (VPTs) make them impractical on resource-constrained devices.
In this paper, we present a plug-and-play pruning-and-recovering framework,
called Hourglass Tokenizer (HoT), for efficient transformer-based 3D human pose
estimation from videos. Our HoT begins with pruning pose tokens of redundant
frames and ends with recovering full-length tokens, resulting in a few pose
tokens in the intermediate transformer blocks and thus improving the model
efficiency. To effectively achieve this, we propose a token pruning cluster
(TPC) that dynamically selects a few representative tokens with high semantic
diversity while eliminating the redundancy of video frames. In addition, we
develop a token recovering attention (TRA) to restore the detailed
spatio-temporal information based on the selected tokens, thereby expanding the
network output to the original full-length temporal resolution for fast
inference. Extensive experiments on two benchmark datasets (i.e., Human3.6M and
MPI-INF-3DHP) demonstrate that our method can achieve both high efficiency and
estimation accuracy compared to the original VPT models. For instance, applying
to MotionBERT and MixSTE on Human3.6M, our HoT can save nearly 50% FLOPs
without sacrificing accuracy and nearly 40% FLOPs with only 0.2% accuracy drop,
respectively. Code and models are available at
https://github.com/NationalGAILab/HoT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024, Open Sourced</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ $\textit{LinkPrompt}$: Natural and Universal Adversarial Attacks on
  Prompt-based Language Models <span class="chip">NAACL2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16432v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16432v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Xu, Wenjie Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt-based learning is a new language model training paradigm that adapts
the Pre-trained Language Models (PLMs) to downstream tasks, which revitalizes
the performance benchmarks across various natural language processing (NLP)
tasks. Instead of using a fixed prompt template to fine-tune the model, some
research demonstrates the effectiveness of searching for the prompt via
optimization. Such prompt optimization process of prompt-based learning on PLMs
also gives insight into generating adversarial prompts to mislead the model,
raising concerns about the adversarial vulnerability of this paradigm. Recent
studies have shown that universal adversarial triggers (UATs) can be generated
to alter not only the predictions of the target PLMs but also the prediction of
corresponding Prompt-based Fine-tuning Models (PFMs) under the prompt-based
learning paradigm. However, UATs found in previous works are often unreadable
tokens or characters and can be easily distinguished from natural texts with
adaptive defenses. In this work, we consider the naturalness of the UATs and
develop $\textit{LinkPrompt}$, an adversarial attack algorithm to generate UATs
by a gradient-based beam search algorithm that not only effectively attacks the
target PLMs and PFMs but also maintains the naturalness among the trigger
tokens. Extensive results demonstrate the effectiveness of
$\textit{LinkPrompt}$, as well as the transferability of UATs generated by
$\textit{LinkPrompt}$ to open-sourced Large Language Model (LLM) Llama2 and
API-accessed LLM GPT-3.5-turbo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the main conference of NAACL2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLatrieval: LLM-Verified Retrieval for Verifiable Generation <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07838v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07838v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaonan Li, Changtai Zhu, Linyang Li, Zhangyue Yin, Tianxiang Sun, Xipeng Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Verifiable generation aims to let the large language model (LLM) generate
text with supporting documents, which enables the user to flexibly verify the
answer and makes the LLM's output more reliable. Retrieval plays a crucial role
in verifiable generation. Specifically, the retrieved documents not only
supplement knowledge to help the LLM generate correct answers, but also serve
as supporting evidence for the user to verify the LLM's output. However, the
widely used retrievers become the bottleneck of the entire pipeline and limit
the overall performance. Their capabilities are usually inferior to LLMs since
they often have much fewer parameters than the large language model and have
not been demonstrated to scale well to the size of LLMs. If the retriever does
not correctly find the supporting documents, the LLM can not generate the
correct and verifiable answer, which overshadows the LLM's remarkable
abilities. To address these limitations, we propose \LLatrieval (Large Language
Model Verified Retrieval), where the LLM updates the retrieval result until it
verifies that the retrieved documents can sufficiently support answering the
question. Thus, the LLM can iteratively provide feedback to retrieval and
facilitate the retrieval result to fully support verifiable generation.
Experiments show that LLatrieval significantly outperforms extensive baselines
and achieves state-of-the-art results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NAACL 2024 (Main Conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Object Coherence in Layout-to-Image Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10522v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10522v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yibin Wang, Weizhong Zhang, Jianwei Zheng, Cheng Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Layout-to-image synthesis is an emerging technique in conditional image
generation. It aims to generate complex scenes, where users require fine
control over the layout of the objects in a scene. However, it remains
challenging to control the object coherence, including semantic coherence
(e.g., the cat looks at the flowers or not) and physical coherence (e.g., the
hand and the racket should not be misaligned). In this paper, we propose a
novel diffusion model with effective global semantic fusion (GSF) and
self-similarity feature enhancement modules to guide the object coherence for
this task. For semantic coherence, we argue that the image caption contains
rich information for defining the semantic relationship within the objects in
the images. Instead of simply employing cross-attention between captions and
generated images, which addresses the highly relevant layout restriction and
semantic coherence separately and thus leads to unsatisfying results shown in
our experiments, we develop GSF to fuse the supervision from the layout
restriction and semantic coherence requirement and exploit it to guide the
image synthesis process. Moreover, to improve the physical coherence, we
develop a Self-similarity Coherence Attention (SCA) module to explicitly
integrate local contextual physical coherence into each pixel's generation
process. Specifically, we adopt a self-similarity map to encode the coherence
restrictions and employ it to extract coherent features from text embedding.
Through visualization of our self-similarity map, we explore the essence of
SCA, revealing that its effectiveness is not only in capturing reliable
physical coherence patterns but also in enhancing complex texture generation.
Extensive experiments demonstrate the superiority of our proposed method in
both image generation quality and controllability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01739v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01739v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu Zhou, Yang You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To help the open-source community have a better understanding of
Mixture-of-Experts (MoE) based large language models (LLMs), we train and
release OpenMoE, a series of fully open-sourced and reproducible decoder-only
MoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T
tokens. Our investigation confirms that MoE-based LLMs can offer a more
favorable cost-effectiveness trade-off than dense LLMs, highlighting the
potential effectiveness for future LLM development.
  One more important contribution of this study is an in-depth analysis of the
routing mechanisms within our OpenMoE models, leading to three significant
findings: Context-Independent Specialization, Early Routing Learning, and
Drop-towards-the-End. We discovered that routing decisions in MoE models are
predominantly based on token IDs, with minimal context relevance. The
token-to-expert assignments are determined early in the pre-training phase and
remain largely unchanged. This imperfect routing can result in performance
degradation, particularly in sequential tasks like multi-turn conversations,
where tokens appearing later in a sequence are more likely to be dropped.
Finally, we rethink our design based on the above-mentioned observations and
analysis. To facilitate future MoE LLM development, we propose potential
strategies for mitigating the issues we found and further improving
off-the-shelf MoE LLM designs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VIGraph: Generative Self-supervised Learning for Class-Imbalanced Node
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01191v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01191v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulan Hu, Sheng Ouyang, Zhirui Yang, Yong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class imbalance in graph data presents significant challenges for node
classification. While existing methods, such as SMOTE-based approaches,
partially mitigate this issue, they still exhibit limitations in constructing
imbalanced graphs. Generative self-supervised learning (SSL) methods,
exemplified by graph autoencoders (GAEs), offer a promising solution by
directly generating minority nodes from the data itself, yet their potential
remains underexplored. In this paper, we delve into the shortcomings of
SMOTE-based approaches in the construction of imbalanced graphs. Furthermore,
we introduce VIGraph, a simple yet effective generative SSL approach that
relies on the Variational GAE as the fundamental model. VIGraph strictly
adheres to the concept of imbalance when constructing imbalanced graphs and
innovatively leverages the variational inference (VI) ability of Variational
GAE to generate nodes for minority classes. VIGraph introduces comprehensive
training strategies, including cross-view contrastive learning at the decoding
phase to capture semantic knowledge, adjacency matrix reconstruction to
preserve graph structure, and alignment strategy to ensure stable training.
VIGraph can generate high-quality nodes directly usable for classification,
eliminating the need to integrate the generated nodes back to the graph as well
as additional retraining found in SMOTE-based methods. We conduct extensive
experiments, results from which demonstrate the superiority and generality of
our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Quadruped Locomotion Using Differentiable Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14864v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14864v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunlong Song, Sangbae Kim, Davide Scaramuzza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While most recent advancements in legged robot control have been driven by
model-free reinforcement learning, we explore the potential of differentiable
simulation. Differentiable simulation promises faster convergence and more
stable training by computing low-variant first-order gradients using the robot
model, but so far, its use for legged robot control has remained limited to
simulation. The main challenge with differentiable simulation lies in the
complex optimization landscape of robotic tasks due to discontinuities in
contact-rich environments, e.g., quadruped locomotion. This work proposes a
new, differentiable simulation framework to overcome these challenges. The key
idea involves decoupling the complex whole-body simulation, which may exhibit
discontinuities due to contact, into two separate continuous domains.
Subsequently, we align the robot state resulting from the simplified model with
a more precise, non-differentiable simulator to maintain sufficient simulation
accuracy. Our framework enables learning quadruped walking in minutes using a
single simulated robot without any parallelization. When augmented with GPU
parallelization, our approach allows the quadruped robot to master diverse
locomotion skills, including trot, pace, bound, and gallop, on challenging
terrains in minutes. Additionally, our policy achieves robust locomotion
performance in the real world zero-shot. To the best of our knowledge, this
work represents the first demonstration of using differentiable simulation for
controlling a real quadruped robot. This work provides several important
insights into using differentiable simulations for legged locomotion in the
real world.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Retrieval-Augmented Generation for Large Language Models: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10997v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10997v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) showcase impressive capabilities but encounter
challenges like hallucination, outdated knowledge, and non-transparent,
untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has
emerged as a promising solution by incorporating knowledge from external
databases. This enhances the accuracy and credibility of the generation,
particularly for knowledge-intensive tasks, and allows for continuous knowledge
updates and integration of domain-specific information. RAG synergistically
merges LLMs' intrinsic knowledge with the vast, dynamic repositories of
external databases. This comprehensive review paper offers a detailed
examination of the progression of RAG paradigms, encompassing the Naive RAG,
the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the
tripartite foundation of RAG frameworks, which includes the retrieval, the
generation and the augmentation techniques. The paper highlights the
state-of-the-art technologies embedded in each of these critical components,
providing a profound understanding of the advancements in RAG systems.
Furthermore, this paper introduces up-to-date evaluation framework and
benchmark. At the end, this article delineates the challenges currently faced
and points out prospective avenues for research and development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ongoing Work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ World Models via <span class="highlight-title">Policy</span>-Guided Trajectory Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.08533v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.08533v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marc Rigter, Jun Yamada, Ingmar Posner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  World models are a powerful tool for developing intelligent agents. By
predicting the outcome of a sequence of actions, world models enable policies
to be optimised via on-policy reinforcement learning (RL) using synthetic data,
i.e. in "in imagination". Existing world models are autoregressive in that they
interleave predicting the next state with sampling the next action from the
policy. Prediction error inevitably compounds as the trajectory length grows.
In this work, we propose a novel world modelling approach that is not
autoregressive and generates entire on-policy trajectories in a single pass
through a diffusion model. Our approach, Policy-Guided Trajectory Diffusion
(PolyGRAD), leverages a denoising model in addition to the gradient of the
action distribution of the policy to diffuse a trajectory of initially random
states and actions into an on-policy synthetic trajectory. We analyse the
connections between PolyGRAD, score-based generative models, and
classifier-guided diffusion models. Our results demonstrate that PolyGRAD
outperforms state-of-the-art baselines in terms of trajectory prediction error
for short trajectories, with the exception of autoregressive diffusion. For
short trajectories, PolyGRAD obtains similar errors to autoregressive
diffusion, but with lower computational requirements. For long trajectories,
PolyGRAD obtains comparable performance to baselines. Our experiments
demonstrate that PolyGRAD enables performant policies to be trained via
on-policy RL in imagination for MuJoCo continuous control domains. Thus,
PolyGRAD introduces a new paradigm for accurate on-policy world modelling
without autoregressive sampling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in TMLR, March 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Functional Graph Convolutional Networks: A unified multi-task and
  multi-modal learning framework to facilitate health and social-care insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10158v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10158v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobia Boschi, Francesca Bonin, Rodrigo Ordonez-Hurtado, Cécile Rousseau, Alessandra Pascale, John Dinsmore
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel Functional Graph Convolutional Network (funGCN)
framework that combines Functional Data Analysis and Graph Convolutional
Networks to address the complexities of multi-task and multi-modal learning in
digital health and longitudinal studies. With the growing importance of health
solutions to improve health care and social support, ensure healthy lives, and
promote well-being at all ages, funGCN offers a unified approach to handle
multivariate longitudinal data for multiple entities and ensures
interpretability even with small sample sizes. Key innovations include
task-specific embedding components that manage different data types, the
ability to perform classification, regression, and forecasting, and the
creation of a knowledge graph for insightful data interpretation. The efficacy
of funGCN is validated through simulation experiments and a real-data
application.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Concept-Based Causal Transition and Symbolic Reasoning for
  Visual Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03325v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03325v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilue Qian, Peiyu Yu, Ying Nian Wu, Yao Su, Wei Wang, Lifeng Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual planning simulates how humans make decisions to achieve desired goals
in the form of searching for visual causal transitions between an initial
visual state and a final visual goal state. It has become increasingly
important in egocentric vision with its advantages in guiding agents to perform
daily tasks in complex environments. In this paper, we propose an interpretable
and generalizable visual planning framework consisting of i) a novel
Substitution-based Concept Learner (SCL) that abstracts visual inputs into
disentangled concept representations, ii) symbol abstraction and reasoning that
performs task planning via the self-learned symbols, and iii) a Visual Causal
Transition model (ViCT) that grounds visual causal transitions to semantically
similar real-world actions. Given an initial state, we perform goal-conditioned
visual planning with a symbolic reasoning method fueled by the learned
representations and causal transitions to reach the goal state. To verify the
effectiveness of the proposed model, we collect a large-scale visual planning
dataset based on AI2-THOR, dubbed as CCTP. Extensive experiments on this
challenging dataset demonstrate the superior performance of our method in
visual task planning. Empirically, we show that our framework can generalize to
unseen task trajectories, unseen object categories, and real-world data.
Further details of this work are provided at
https://fqyqc.github.io/ConTranPlan/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identifying the Correlation Between Language Distance and Cross-Lingual
  Transfer in a Multilingual Representation Space <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.02151v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.02151v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fred Philippy, Siwen Guo, Shohreh Haddadan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior research has investigated the impact of various linguistic features on
cross-lingual transfer performance. In this study, we investigate the manner in
which this effect can be mapped onto the representation space. While past
studies have focused on the impact on cross-lingual alignment in multilingual
language models during fine-tuning, this study examines the absolute evolution
of the respective language representation spaces produced by MLLMs. We place a
specific emphasis on the role of linguistic characteristics and investigate
their inter-correlation with the impact on representation spaces and
cross-lingual transfer performance. Additionally, this paper provides
preliminary evidence of how these findings can be leveraged to enhance transfer
to linguistically distant languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGTYP Workshop 2023 (co-located with EACL 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Effects of Mixed <span class="highlight-title">Sample</span> Data Augmentation are Class Dependent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.09136v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.09136v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haeil Lee, Hansang Lee, Junmo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixed Sample Data Augmentation (MSDA) techniques, such as Mixup, CutMix, and
PuzzleMix, have been widely acknowledged for enhancing performance in a variety
of tasks. A previous study reported the class dependency of traditional data
augmentation (DA), where certain classes benefit disproportionately compared to
others. This paper reveals a class dependent effect of MSDA, where some classes
experience improved performance while others experience degraded performance.
This research addresses the issue of class dependency in MSDA and proposes an
algorithm to mitigate it. The approach involves training on a mixture of MSDA
and non-MSDA data, which not only mitigates the negative impact on the affected
classes, but also improves overall accuracy. Furthermore, we provide in-depth
analysis and discussion of why MSDA introduced class dependencies and which
classes are most likely to have them.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 18 figures, Overall Revision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spectral Meets Spatial: Harmonising 3D Shape Matching and Interpolation <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18920v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18920v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongliang Cao, Marvin Eisenberger, Nafie El Amrani, Daniel Cremers, Florian Bernard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although 3D shape matching and interpolation are highly interrelated, they
are often studied separately and applied sequentially to relate different 3D
shapes, thus resulting in sub-optimal performance. In this work we present a
unified framework to predict both point-wise correspondences and shape
interpolation between 3D shapes. To this end, we combine the deep functional
map framework with classical surface deformation models to map shapes in both
spectral and spatial domains. On the one hand, by incorporating spatial maps,
our method obtains more accurate and smooth point-wise correspondences compared
to previous functional map methods for shape matching. On the other hand, by
introducing spectral maps, our method gets rid of commonly used but
computationally expensive geodesic distance constraints that are only valid for
near-isometric shape deformations. Furthermore, we propose a novel test-time
adaptation scheme to capture both pose-dominant and shape-dominant
deformations. Using different challenging datasets, we demonstrate that our
method outperforms previous state-of-the-art methods for both shape matching
and interpolation, even compared to supervised approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Regulatable AI Systems: Technical Gaps and <span class="highlight-title">Policy</span> Opportunities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.12609v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.12609v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xudong Shen, Hannah Brown, Jiashu Tao, Martin Strobel, Yao Tong, Akshay Narayan, Harold Soh, Finale Doshi-Velez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is increasing attention being given to how to regulate AI systems. As
governing bodies grapple with what values to encapsulate into regulation, we
consider the technical half of the question: To what extent can AI experts vet
an AI system for adherence to regulatory requirements? We investigate this
question through the lens of two public sector procurement checklists,
identifying what we can do now, what should be possible with technical
innovation, and what requirements need a more interdisciplinary approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>scheduled for publication in the Communications of the ACM, titled
  "Directions of Technical Innovation for Regulatable AI Systems"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMP++: Motion Manifold Primitives with Parametric Curve Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17072v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17072v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yonghyeon Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motion Manifold Primitives (MMP), a manifold-based approach for encoding
basic motion skills, can produce diverse trajectories, enabling the system to
adapt to unseen constraints. Nonetheless, we argue that current MMP models lack
crucial functionalities of movement primitives, such as temporal and via-points
modulation, found in traditional approaches. This shortfall primarily stems
from MMP's reliance on discrete-time trajectories. To overcome these
limitations, we introduce Motion Manifold Primitives++ (MMP++), a new model
that integrates the strengths of both MMP and traditional methods by
incorporating parametric curve representations into the MMP framework.
Furthermore, we identify a significant challenge with MMP++: performance
degradation due to geometric distortions in the latent space, meaning that
similar motions are not closely positioned. To address this, Isometric Motion
Manifold Primitives++ (IMMP++) is proposed to ensure the latent space
accurately preserves the manifold's geometry. Our experimental results across
various applications, including 2-DoF planar motions, 7-DoF robot arm motions,
and SE(3) trajectory planning, show that MMP++ and IMMP++ outperform existing
methods in trajectory generation tasks, achieving substantial improvements in
some cases. Moreover, they enable the modulation of latent coordinates and
via-points, thereby allowing efficient online adaptation to dynamic
environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages. This work has been submitted to the IEEE for possible
  publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Regret-Based Defense in Adversarial <span class="highlight-title">Reinforcement</span> Learning <span class="chip">AAMAS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.06912v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.06912v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roman Belaire, Pradeep Varakantham, Thanh Nguyen, David Lo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Reinforcement Learning (DRL) policies have been shown to be vulnerable
to small adversarial noise in observations. Such adversarial noise can have
disastrous consequences in safety-critical environments. For instance, a
self-driving car receiving adversarially perturbed sensory observations about
nearby signs (e.g., a stop sign physically altered to be perceived as a speed
limit sign) or objects (e.g., cars altered to be recognized as trees) can be
fatal. Existing approaches for making RL algorithms robust to an
observation-perturbing adversary have focused on reactive approaches that
iteratively improve against adversarial examples generated at each iteration.
While such approaches have been shown to provide improvements over regular RL
methods, they are reactive and can fare significantly worse if certain
categories of adversarial examples are not generated during training. To that
end, we pursue a more proactive approach that relies on directly optimizing a
well-studied robustness measure, regret instead of expected value. We provide a
principled approach that minimizes maximum regret over a "neighborhood" of
observations to the received "observation". Our regret criterion can be used to
modify existing value- and policy-based Deep RL methods. We demonstrate that
our approaches provide a significant improvement in performance across a wide
variety of benchmarks against leading approaches for robust Deep RL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAMAS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MA4DIV: Multi-<span class="highlight-title">Agent</span> <span class="highlight-title">Reinforcement</span> Learning for Search Result
  Diversification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17421v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17421v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqun Chen, Jiaxin Mao, Yi Zhang, Dehong Ma, Long Xia, Jun Fan, Daiting Shi, Zhicong Cheng, Simiu Gu, Dawei Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The objective of search result diversification (SRD) is to ensure that
selected documents cover as many different subtopics as possible. Existing
methods primarily utilize a paradigm of "greedy selection", i.e., selecting one
document with the highest diversity score at a time. These approaches tend to
be inefficient and are easily trapped in a suboptimal state. In addition, some
other methods aim to approximately optimize the diversity metric, such as
$\alpha$-NDCG, but the results still remain suboptimal. To address these
challenges, we introduce Multi-Agent reinforcement learning (MARL) for search
result DIVersity, which called MA4DIV. In this approach, each document is an
agent and the search result diversification is modeled as a cooperative task
among multiple agents. This approach allows for directly optimizing the
diversity metrics, such as $\alpha$-NDCG, while achieving high training
efficiency. We conducted preliminary experiments on public TREC datasets to
demonstrate the effectiveness and potential of MA4DIV. Considering the limited
number of queries in public TREC datasets, we construct a large-scale dataset
from industry sources and show that MA4DIV achieves substantial improvements in
both effectiveness and efficiency than existing baselines on a industrial scale
dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMs Are Few-Shot In-Context Low-Resource Language Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16512v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16512v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Cahyawijaya, Holy Lovenia, Pascale Fung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) empowers large language models (LLMs) to perform
diverse tasks in underrepresented languages using only short in-context
information, offering a crucial avenue for narrowing the gap between
high-resource and low-resource languages. Nonetheless, there is only a handful
of works explored ICL for low-resource languages with most of them focusing on
relatively high-resource languages, such as French and Spanish. In this work,
we extensively study ICL and its cross-lingual variation (X-ICL) on 25
low-resource and 7 relatively higher-resource languages. Our study not only
assesses the effectiveness of ICL with LLMs in low-resource languages but also
identifies the shortcomings of in-context label alignment, and introduces a
more effective alternative: query alignment. Moreover, we provide valuable
insights into various facets of ICL for low-resource languages. Our study
concludes the significance of few-shot in-context information on enhancing the
low-resource understanding quality of LLMs through semantically relevant
information by closing the language gap in the target language and aligning the
semantics between the targeted low-resource and the high-resource language that
the model is proficient in. Our work highlights the importance of advancing ICL
research, particularly for low-resource languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Programming Education with ChatGPT: A Case Study on Student
  Perceptions and Interactions in a Python Course 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15472v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15472v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boxaun Ma, Li Chen, Shin'ichi Konomi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of ChatGPT as a supportive tool in education, notably in
programming courses, addresses the unique challenges of programming education
by providing assistance with debugging, code generation, and explanations.
Despite existing research validating ChatGPT's effectiveness, its application
in university-level programming education and a detailed understanding of
student interactions and perspectives remain limited. This paper explores
ChatGPT's impact on learning in a Python programming course tailored for
first-year students over eight weeks. By analyzing responses from surveys,
open-ended questions, and student-ChatGPT dialog data, we aim to provide a
comprehensive view of ChatGPT's utility and identify both its advantages and
limitations as perceived by students. Our study uncovers a generally positive
reception toward ChatGPT and offers insights into its role in enhancing the
programming education experience. These findings contribute to the broader
discourse on AI's potential in education, suggesting paths for future research
and application.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SSM Meets Video Diffusion Models: Efficient Video Generation with
  Structured State Spaces <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07711v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07711v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuta Oshima, Shohei Taniguchi, Masahiro Suzuki, Yutaka Matsuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the remarkable achievements in image generation through diffusion
models, the research community has shown increasing interest in extending these
models to video generation. Recent diffusion models for video generation have
predominantly utilized attention layers to extract temporal features. However,
attention layers are limited by their memory consumption, which increases
quadratically with the length of the sequence. This limitation presents
significant challenges when attempting to generate longer video sequences using
diffusion models. To overcome this challenge, we propose leveraging state-space
models (SSMs). SSMs have recently gained attention as viable alternatives due
to their linear memory consumption relative to sequence length. In the
experiments, we first evaluate our SSM-based model with UCF101, a standard
benchmark of video generation. In addition, to investigate the potential of
SSMs for longer video generation, we perform an experiment using the MineRL
Navigate dataset, varying the number of frames to 64, 200, and 400. In these
settings, our SSM-based model can considerably save memory consumption for
longer sequences, while maintaining competitive FVD scores to the
attention-based models. Our codes are available at
https://github.com/shim0114/SSM-Meets-Video-Diffusion-Models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as workshop paper at ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Weakly Supervised AUC Optimization: A Unified Partial AUC Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14258v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14258v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Xie, Yu Liu, Hao-Yuan He, Ming Li, Zhi-Hua Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since acquiring perfect supervision is usually difficult, real-world machine
learning tasks often confront inaccurate, incomplete, or inexact supervision,
collectively referred to as weak supervision. In this work, we present WSAUC, a
unified framework for weakly supervised AUC optimization problems, which covers
noisy label learning, positive-unlabeled learning, multi-instance learning, and
semi-supervised learning scenarios. Within the WSAUC framework, we first frame
the AUC optimization problems in various weakly supervised scenarios as a
common formulation of minimizing the AUC risk on contaminated sets, and
demonstrate that the empirical risk minimization problems are consistent with
the true AUC. Then, we introduce a new type of partial AUC, specifically, the
reversed partial AUC (rpAUC), which serves as a robust training objective for
AUC maximization in the presence of contaminated labels. WSAUC offers a
universal solution for AUC optimization in various weakly supervised scenarios
by maximizing the empirical rpAUC. Theoretical and experimental results under
multiple settings support the effectiveness of WSAUC on a range of weakly
supervised AUC optimization tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE TPAMI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ProSwitch: Knowledge-Guided Language Model Fine-Tuning to Generate
  Professional and Non-Professional Styled Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09131v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09131v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Zong, Yuyan Chen, Weiming Lu, Jian Shao, Yueting Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated efficacy in various linguistic
applications, including text summarization and controlled text generation.
However, studies into their capacity of switching between styles via
fine-tuning remain underexplored. This study concentrates on textual
professionalism and introduces a novel methodology, named ProSwitch, which
equips a language model with the ability to produce both professional and
non-professional responses through knowledge-guided instruction tuning.
ProSwitch unfolds across three phases: data preparation for gathering domain
knowledge and training corpus; instruction tuning for optimizing language
models with multiple levels of instruction formats; and comprehensive
evaluation for assessing the professionalism discrimination and reference-based
quality of generated text. Comparative analysis of ProSwitch against both
general and specialized language models reveals that our approach outperforms
baselines in switching between professional and non-professional text
generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Re2LLM: Reflective <span class="highlight-title">Reinforcement</span> Large Language Model for Session-based
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16427v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16427v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyan Wang, Yingpeng Du, Zhu Sun, Haoyan Chua, Kaidong Feng, Wenya Wang, Jie Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are emerging as promising approaches to enhance
session-based recommendation (SBR), where both prompt-based and
fine-tuning-based methods have been widely investigated to align LLMs with SBR.
However, the former methods struggle with optimal prompts to elicit the correct
reasoning of LLMs due to the lack of task-specific feedback, leading to
unsatisfactory recommendations. Although the latter methods attempt to
fine-tune LLMs with domain-specific knowledge, they face limitations such as
high computational costs and reliance on open-source backbones. To address such
issues, we propose a Reflective Reinforcement Large Language Model (Re2LLM) for
SBR, guiding LLMs to focus on specialized knowledge essential for more accurate
recommendations effectively and efficiently. In particular, we first design the
Reflective Exploration Module to effectively extract knowledge that is readily
understandable and digestible by LLMs. To be specific, we direct LLMs to
examine recommendation errors through self-reflection and construct a knowledge
base (KB) comprising hints capable of rectifying these errors. To efficiently
elicit the correct reasoning of LLMs, we further devise the Reinforcement
Utilization Module to train a lightweight retrieval agent. It learns to select
hints from the constructed KB based on the task-specific feedback, where the
hints can serve as guidance to help correct LLMs reasoning for better
recommendations. Extensive experiments on multiple real-world datasets
demonstrate that our method consistently outperforms state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dial-MAE: ConTextual Masked Auto-Encoder for Retrieval-based Dialogue
  Systems <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04357v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04357v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenpeng Su, Xing Wu, Wei Zhou, Guangyuan Ma, Songlin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dialogue response selection aims to select an appropriate response from
several candidates based on a given user and system utterance history. Most
existing works primarily focus on post-training and fine-tuning tailored for
cross-encoders. However, there are no post-training methods tailored for dense
encoders in dialogue response selection. We argue that when the current
language model, based on dense dialogue systems (such as BERT), is employed as
a dense encoder, it separately encodes dialogue context and response, leading
to a struggle to achieve the alignment of both representations. Thus, we
propose Dial-MAE (Dialogue Contextual Masking Auto-Encoder), a straightforward
yet effective post-training technique tailored for dense encoders in dialogue
response selection. Dial-MAE uses an asymmetric encoder-decoder architecture to
compress the dialogue semantics into dense vectors, which achieves better
alignment between the features of the dialogue context and response. Our
experiments have demonstrated that Dial-MAE is highly effective, achieving
state-of-the-art performance on two commonly evaluated benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SoftTiger: A Clinical Foundation Model for Healthcare Workflows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00868v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00868v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ye Chen, Igor Couto, Wei Cai, Cong Fu, Bruno Dorneles
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SoftTiger, a clinical large language model (CLaM) designed as a
foundation model for healthcare workflows. The narrative and unstructured
nature of clinical notes is a major obstacle for healthcare intelligentization.
We address a critical problem of structuring clinical notes into clinical data,
according to international interoperability standards. We collect and annotate
data for three subtasks, namely, international patient summary, clinical
impression and medical encounter. We then supervised fine-tuned a
state-of-the-art LLM using public and credentialed clinical data. The training
is orchestrated in a way that the target model can first support basic clinical
tasks such as abbreviation expansion and temporal information extraction, and
then learn to perform more complex downstream clinical tasks. Moreover, we
address several modeling challenges in the healthcare context, e.g., extra long
context window. Our blind pairwise evaluation shows that SoftTiger outperforms
other popular open-source models and GPT-3.5, comparable to Gemini-pro, with a
mild gap from GPT-4. We believe that LLMs may become a step-stone towards
healthcare digitalization and democratization. Therefore, we publicly release
SoftTiger models at scales of 13 billion and 70 billion parameters, as well as
datasets and code for our innovative scalable evaluation, hopefully, making a
significant contribution to the healthcare industry.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Probing Multimodal Large Language Models for Global and Local Semantic
  Representations <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17304v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17304v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingxu Tao, Quzhe Huang, Kun Xu, Liwei Chen, Yansong Feng, Dongyan Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancement of Multimodal Large Language Models (MLLMs) has greatly
accelerated the development of applications in understanding integrated texts
and images. Recent works leverage image-caption datasets to train MLLMs,
achieving state-of-the-art performance on image-to-text tasks. However, there
are few studies exploring which layers of MLLMs make the most effort to the
global image information, which plays vital roles in multimodal comprehension
and generation. In this study, we find that the intermediate layers of models
can encode more global semantic information, whose representation vectors
perform better on visual-language entailment tasks, rather than the topmost
layers. We further probe models regarding local semantic representations
through object recognition tasks. We find that the topmost layers may
excessively focus on local information, leading to a diminished ability to
encode global information. Our code and data are released via
https://github.com/kobayashikanna01/probing_MLLM_rep.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by LREC-COLING 2024 as a short paper (Camera Ready)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In-Distribution and Out-of-Distribution Self-supervised ECG
  Representation Learning for Arrhythmia Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.06427v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.06427v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sahar Soltanieh, Javad Hashemi, Ali Etemad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a systematic investigation into the effectiveness of
Self-Supervised Learning (SSL) methods for Electrocardiogram (ECG) arrhythmia
detection. We begin by conducting a novel analysis of the data distributions on
three popular ECG-based arrhythmia datasets: PTB-XL, Chapman, and Ribeiro. To
the best of our knowledge, our study is the first to quantitatively explore and
characterize these distributions in the area. We then perform a comprehensive
set of experiments using different augmentations and parameters to evaluate the
effectiveness of various SSL methods, namely SimCRL, BYOL, and SwAV, for ECG
representation learning, where we observe the best performance achieved by
SwAV. Furthermore, our analysis shows that SSL methods achieve highly
competitive results to those achieved by supervised state-of-the-art methods.
To further assess the performance of these methods on both In-Distribution (ID)
and Out-of-Distribution (OOD) ECG data, we conduct cross-dataset training and
testing experiments. Our comprehensive experiments show almost identical
results when comparing ID and OOD schemes, indicating that SSL techniques can
learn highly effective representations that generalize well across different
OOD datasets. This finding can have major implications for ECG-based arrhythmia
detection. Lastly, to further analyze our results, we perform detailed
per-disease studies on the performance of the SSL methods on the three
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been published in the IEEE Journal of Biomedical and
  Health Informatics (JBHI). Copyright IEEE. Please cite as: S. Soltanieh, J.
  Hashemi and A. Etemad, "In-Distribution and Out-of-Distribution
  Self-Supervised ECG Representation Learning for Arrhythmia Detection," in
  IEEE Journal of Biomedical and Health Informatics, vol. 28, no. 2, pp.
  789-800, Feb. 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Imitating Cost-Constrained Behaviors in <span class="highlight-title">Reinforcement</span> Learning <span class="chip">ICAPS-24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17456v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17456v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Shao, Pradeep Varakantham, Shih-Fen Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Complex planning and scheduling problems have long been solved using various
optimization or heuristic approaches. In recent years, imitation learning that
aims to learn from expert demonstrations has been proposed as a viable
alternative to solving these problems. Generally speaking, imitation learning
is designed to learn either the reward (or preference) model or directly the
behavioral policy by observing the behavior of an expert. Existing work in
imitation learning and inverse reinforcement learning has focused on imitation
primarily in unconstrained settings (e.g., no limit on fuel consumed by the
vehicle). However, in many real-world domains, the behavior of an expert is
governed not only by reward (or preference) but also by constraints. For
instance, decisions on self-driving delivery vehicles are dependent not only on
the route preferences/rewards (depending on past demand data) but also on the
fuel in the vehicle and the time available. In such problems, imitation
learning is challenging as decisions are not only dictated by the reward model
but are also dependent on a cost-constrained model. In this paper, we provide
multiple methods that match expert distributions in the presence of trajectory
cost constraints through (a) Lagrangian-based method; (b) Meta-gradients to
find a good trade-off between expected return and minimizing constraint
violation; and (c) Cost-violation-based alternating gradient. We empirically
show that leading imitation learning approaches imitate cost-constrained
behaviors poorly and our meta-gradient-based approach achieves the best
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 34th International Conference on Automated Planning
  and Scheduling (ICAPS-24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Innovation Paradox: Concept Space Expansion with Diminishing
  Originality and the Promise of Creative AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13300v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13300v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Serhad Sarica, Jianxi Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Innovation, typically spurred by reusing, recombining, and synthesizing
existing concepts, is expected to result in an exponential growth of the
concept space over time. However, our statistical analysis of TechNet, which is
a comprehensive technology semantic network encompassing over four million
concepts derived from patent texts, reveals a linear rather than exponential
expansion of the overall technological concept space. Moreover, there is a
notable decline in the originality of newly created concepts. These trends can
be attributed to the constraints of human cognitive abilities to innovate
beyond an ever-growing space of prior art, among other factors. Integrating
creative artificial intelligence (CAI) into the innovation process holds the
potential to overcome these limitations and alter the observed trends in the
future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Forthcoming on the Design Science</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMs in Political Science: Heralding a New Era of Visual Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00154v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00154v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interest is increasing among political scientists in leveraging the extensive
information available in images. However, the challenge of interpreting these
images lies in the need for specialized knowledge in computer vision and access
to specialized hardware. As a result, image analysis has been limited to a
relatively small group within the political science community. This landscape
could potentially change thanks to the rise of large language models (LLMs).
This paper aims to raise awareness of the feasibility of using Gemini for image
content analysis. A retrospective analysis was conducted on a corpus of 688
images. Content reports were elicited from Gemini for each image and then
manually evaluated by the authors. We find that Gemini is highly accurate in
performing object detection, which is arguably the most common and fundamental
task in image analysis for political scientists. Equally important, we show
that it is easy to implement as the entire command consists of a single prompt
in natural language; it is fast to run and should meet the time budget of most
researchers; and it is free to use and does not require any specialized
hardware. In addition, we illustrate how political scientists can leverage
Gemini for other image understanding tasks, including face identification,
sentiment analysis, and caption generation. Our findings suggest that Gemini
and other similar LLMs have the potential to drastically stimulate and
accelerate image research in political science and social sciences more
broadly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language
  Models <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16915v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16915v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atsushi Keyaki, Ribeka Keyaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning in information retrieval systems using pre-trained language
models (PLM-based IR) requires learning query representations and
query-document relations, in addition to downstream task-specific learning.
This study introduces coarse-tuning as an intermediate learning stage that
bridges pre-training and fine-tuning. By learning query representations and
query-document relations in coarse-tuning, we aim to reduce the load of
fine-tuning and improve the learning effect of downstream IR tasks. We propose
Query-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the
appropriateness of query-document pairs. Evaluation experiments show that the
proposed method significantly improves MRR and/or nDCG@5 in four ad-hoc
document retrieval datasets. Furthermore, the results of the query prediction
task suggested that coarse-tuning facilitated learning of query representation
and query-document relations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Look Before You Leap: Problem Elaboration Prompting Improves
  Mathematical Reasoning in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15764v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15764v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Liao, Jidong Tian, Shaohua Hu, Hao He, Yaohui Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) still grapple with complex tasks like
mathematical reasoning. Despite significant efforts invested in improving
prefix prompts or reasoning process, the crucial role of problem context might
have been neglected. Accurate recognition of inputs is fundamental for solving
mathematical tasks, as ill-formed problems could potentially mislead LLM's
reasoning. In this study, we propose a new approach named Problem Elaboration
Prompting (PEP) to enhance the mathematical capacities of LLMs. Specifically,
PEP decomposes and elucidates the problem context before reasoning, therefore
enhancing the context modeling and parsing efficiency. Experiments across
datasets and models demonstrate promising performances: (1) PEP demonstrates an
overall enhancement in various mathematical tasks. For instance, with the
GPT-3.5 model, PEP exhibits improvements of 9.93% and 8.80% on GSM8k through
greedy decoding and self-consistency, respectively. (2) PEP can be easily
implemented and integrated with other prompting methods. (3) PEP shows
particular strength in handling distraction problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Follower Agnostic Methods for Stackelberg <span class="highlight-title">Game</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.01421v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.01421v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chinmay Maheshwari, James Cheng, S. Shankar Sasty, Lillian Ratliff, Eric Mazumdar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present an efficient algorithm to solve online Stackelberg
games, featuring multiple followers, in a follower-agnostic manner. Unlike
previous works, our approach works even when leader has no knowledge about the
followers' utility functions or strategy space. Our algorithm introduces a
unique gradient estimator, leveraging specially designed strategies to probe
followers. In a departure from traditional assumptions of optimal play, we
model followers' responses using a convergent adaptation rule, allowing for
realistic and dynamic interactions. The leader constructs the gradient
estimator solely based on observations of followers' actions. We provide both
non-asymptotic convergence rates to stationary points of the leader's objective
and demonstrate asymptotic convergence to a \emph{local Stackelberg
equilibrium}. To validate the effectiveness of our algorithm, we use this
algorithm to solve the problem of incentive design on a large-scale
transportation network, showcasing its robustness even when the leader lacks
access to followers' demand.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Act without Actions <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10812v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10812v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Schmidt, Minqi Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-training large models on vast amounts of web data has proven to be an
effective approach for obtaining powerful, general models in domains such as
language and vision. However, this paradigm has not yet taken hold in
reinforcement learning. This is because videos, the most abundant form of
embodied behavioral data on the web, lack the action labels required by
existing methods for imitating behavior from demonstrations. We introduce
Latent Action Policies (LAPO), a method for recovering latent action
information, and thereby latent-action policies, world models, and inverse
dynamics models, purely from videos. LAPO is the first method able to recover
the structure of the true action space just from observed dynamics, even in
challenging procedurally-generated environments. LAPO enables training
latent-action policies that can be rapidly fine-tuned into expert-level
policies, either offline using a small action-labeled dataset, or online with
rewards. LAPO takes a first step towards pre-training powerful, generalist
policies and world models on the vast amounts of videos readily available on
the web.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2024 (spotlight). The code can be found at
  http://github.com/schmidtdominik/LAPO</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Static Evaluation: A Dynamic Approach to Assessing AI Assistants'
  API Invocation Capabilities <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11128v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11128v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Honglin Mu, Yang Xu, Yunlong Feng, Xiaofeng Han, Yitong Li, Yutai Hou, Wanxiang Che
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rise of Large Language Models (LLMs), AI assistants' ability to
utilize tools, especially through API calls, has advanced notably. This
progress has necessitated more accurate evaluation methods. Many existing
studies adopt static evaluation, where they assess AI assistants' API call
based on pre-defined dialogue histories. However, such evaluation method can be
misleading, as an AI assistant might fail in generating API calls from
preceding human interaction in real cases. Instead of the resource-intensive
method of direct human-machine interactions, we propose Automated Dynamic
Evaluation (AutoDE) to assess an assistant's API call capability without human
involvement. In our framework, we endeavor to closely mirror genuine human
conversation patterns in human-machine interactions, using a LLM-based user
agent, equipped with a user script to ensure human alignment. Experimental
results highlight that AutoDE uncovers errors overlooked by static evaluations,
aligning more closely with human assessment. Testing four AI assistants using
our crafted benchmark, our method further mirrored human evaluation compared to
conventional static evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Guided Distant Supervision for Multilingual Relation Extraction Data:
  Adapting to a New Language <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17143v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17143v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alistair Plum, Tharindu Ranasinghe, Christoph Purschke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relation extraction is essential for extracting and understanding
biographical information in the context of digital humanities and related
subjects. There is a growing interest in the community to build datasets
capable of training machine learning models to extract relationships. However,
annotating such datasets can be expensive and time-consuming, in addition to
being limited to English. This paper applies guided distant supervision to
create a large biographical relationship extraction dataset for German. Our
dataset, composed of more than 80,000 instances for nine relationship types, is
the largest biographical German relationship extraction dataset. We also create
a manually annotated dataset with 2000 instances to evaluate the models and
release it together with the dataset compiled using guided distant supervision.
We train several state-of-the-art machine learning models on the automatically
created dataset and release them as well. Furthermore, we experiment with
multilingual and cross-lingual experiments that could benefit many low-resource
languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024 (The 2024 Joint International Conference
  on Computational Linguistics, Language Resources and Evaluation)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GlotScript: A Resource and Tool for Low Resource Writing System
  Identification <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13320v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13320v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Hossein Kargaran, François Yvon, Hinrich Schütze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present GlotScript, an open resource and tool for low resource writing
system identification. GlotScript-R is a resource that provides the attested
writing systems for more than 7,000 languages. It is compiled by aggregating
information from existing writing system resources. GlotScript-T is a writing
system identification tool that covers all 161 Unicode 15.0 scripts. For an
input text, it returns its script distribution where scripts are identified by
ISO 15924 codes. We also present two use cases for GlotScript. First, we
demonstrate that GlotScript can help cleaning multilingual corpora such as mC4
and OSCAR. Second, we analyze the tokenization of a number of language models
such as GPT-4 using GlotScript and provide insights on the coverage of low
resource scripts and languages by each language model. We hope that GlotScript
will become a useful resource for work on low resource languages in the NLP
community. GlotScript-R and GlotScript-T are available at
https://github.com/cisnlp/GlotScript.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NLPre: a revised approach towards language-centric benchmarking of
  Natural Language Preprocessing systems <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04507v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04507v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martyna Wiącek, Piotr Rybak, Łukasz Pszenny, Alina Wróblewska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advancements of transformer-based architectures, we observe the rise
of natural language preprocessing (NLPre) tools capable of solving preliminary
NLP tasks (e.g. tokenisation, part-of-speech tagging, dependency parsing, or
morphological analysis) without any external linguistic guidance. It is arduous
to compare novel solutions to well-entrenched preprocessing toolkits, relying
on rule-based morphological analysers or dictionaries. Aware of the
shortcomings of existing NLPre evaluation approaches, we investigate a novel
method of reliable and fair evaluation and performance reporting. Inspired by
the GLUE benchmark, the proposed language-centric benchmarking system enables
comprehensive ongoing evaluation of multiple NLPre tools, while credibly
tracking their performance. The prototype application is configured for Polish
and integrated with the thoroughly assembled NLPre-PL benchmark. Based on this
benchmark, we conduct an extensive evaluation of a variety of Polish NLPre
systems. To facilitate the construction of benchmarking environments for other
languages, e.g. NLPre-GA for Irish or NLPre-ZH for Chinese, we ensure full
customization of the publicly released source code of the benchmarking system.
The links to all the resources (deployed platforms, source code, trained
models, datasets etc.) can be found on the project website:
https://sites.google.com/view/nlpre-benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Trustworthy Reranking: A Simple yet Effective Abstention
  Mechanism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12997v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12997v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hippolyte Gisserot-Boukhlef, Manuel Faysse, Emmanuel Malherbe, Céline Hudelot, Pierre Colombo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Information Retrieval (NIR) has significantly improved upon
heuristic-based IR systems. Yet, failures remain frequent, the models used
often being unable to retrieve documents relevant to the user's query. We
address this challenge by proposing a lightweight abstention mechanism tailored
for real-world constraints, with particular emphasis placed on the reranking
phase. We introduce a protocol for evaluating abstention strategies in a
black-box scenario, demonstrating their efficacy, and propose a simple yet
effective data-driven mechanism. We provide open-source code for experiment
replication and abstention implementation, fostering wider adoption and
application in diverse contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CARE: Co-Attention Network for Joint Entity and Relation Extraction <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12531v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12531v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjun Kong, Yamei Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Joint entity and relation extraction is the fundamental task of information
extraction, consisting of two subtasks: named entity recognition and relation
extraction. However, most existing joint extraction methods suffer from issues
of feature confusion or inadequate interaction between the two subtasks.
Addressing these challenges, in this work, we propose a Co-Attention network
for joint entity and Relation Extraction (CARE). Our approach includes adopting
a parallel encoding strategy to learn separate representations for each
subtask, aiming to avoid feature overlap or confusion. At the core of our
approach is the co-attention module that captures two-way interaction between
the two subtasks, allowing the model to leverage entity information for
relation prediction and vice versa, thus promoting mutual enhancement. Through
extensive experiments on three benchmark datasets for joint entity and relation
extraction (NYT, WebNLG, and SciERC), we demonstrate that our proposed model
outperforms existing baseline models. Our code will be available at
https://github.com/kwj0x7f/CARE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Few-Shot Detection of Machine-Generated Text using Style Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.06712v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.06712v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafael Rivera Soto, Kailin Koch, Aleem Khan, Barry Chen, Marcus Bishop, Nicholas Andrews
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of instruction-tuned language models that convincingly mimic human
writing poses a significant risk of abuse. However, such abuse may be
counteracted with the ability to detect whether a piece of text was composed by
a language model rather than a human author. Some previous approaches to this
problem have relied on supervised methods by training on corpora of confirmed
human- and machine- written documents. Unfortunately, model under-specification
poses an unavoidable challenge for neural network-based detectors, making them
brittle in the face of data shifts, such as the release of newer language
models producing still more fluent text than the models used to train the
detectors. Other approaches require access to the models that may have
generated a document in question, which is often impractical. In light of these
challenges, we pursue a fundamentally different approach not relying on samples
from language models of concern at training time. Instead, we propose to
leverage representations of writing style estimated from human-authored text.
Indeed, we find that features effective at distinguishing among human authors
are also effective at distinguishing human from machine authors, including
state-of-the-art large language models like Llama-2, ChatGPT, and GPT-4.
Furthermore, given a handful of examples composed by each of several specific
language models of interest, our approach affords the ability to predict which
model generated a given document. The code and data to reproduce our
experiments are available at
https://github.com/LLNL/LUAR/tree/main/fewshot_iclr2024.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can
  Fool Large Language Models Easily <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08268v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08268v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Ding, Jun Kuang, Dan Ma, Xuezhi Cao, Yunsen Xian, Jiajun Chen, Shujian Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed to
provide useful and safe responses. However, adversarial prompts known as
'jailbreaks' can circumvent safeguards, leading LLMs to generate potentially
harmful content. Exploring jailbreak prompts can help to better reveal the
weaknesses of LLMs and further steer us to secure them. Unfortunately, existing
jailbreak methods either suffer from intricate manual design or require
optimization on other white-box models, which compromises either generalization
or efficiency. In this paper, we generalize jailbreak prompt attacks into two
aspects: (1) Prompt Rewriting and (2) Scenario Nesting. Based on this, we
propose ReNeLLM, an automatic framework that leverages LLMs themselves to
generate effective jailbreak prompts. Extensive experiments demonstrate that
ReNeLLM significantly improves the attack success rate while greatly reducing
the time cost compared to existing baselines. Our study also reveals the
inadequacy of current defense methods in safeguarding LLMs. Finally, we analyze
the failure of LLMs defense from the perspective of prompt execution priority,
and propose corresponding defense strategies. We hope that our research can
catalyze both the academic community and LLMs developers towards the provision
of safer and more regulated LLMs. The code is available at
https://github.com/NJUNLP/ReNeLLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Acccepted by NAACL 2024, 18 pages, 7 figures, 13 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visually Guided Generative Text-Layout Pre-training for Document
  Intelligence <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16516v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16516v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiming Mao, Haoli Bai, Lu Hou, Jiansheng Wei, Xin Jiang, Qun Liu, Kam-Fai Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior study shows that pre-training techniques can boost the performance of
visual document understanding (VDU), which typically requires models to gain
abilities to perceive and reason both document texts and layouts (e.g.,
locations of texts and table-cells). To this end, we propose visually guided
generative text-layout pre-training, named ViTLP. Given a document image, the
model optimizes hierarchical language and layout modeling objectives to
generate the interleaved text and layout sequence. In addition, to address the
limitation of processing long documents by Transformers, we introduce a
straightforward yet effective multi-segment generative pre-training scheme,
facilitating ViTLP to process word-intensive documents of any length. ViTLP can
function as a native OCR model to localize and recognize texts of document
images. Besides, ViTLP can be effectively applied to various downstream VDU
tasks. Extensive experiments show that ViTLP achieves competitive performance
over existing baselines on benchmark VDU tasks, including information
extraction, document classification, and document question answering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2024 main conference. The first version of this
  paper was submitted to OpenReview
  (https://openreview.net/forum?id=ARtBIBAmNR) in June 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InfoCTM: A Mutual Information Maximization Perspective of Cross-Lingual
  Topic Modeling <span class="chip">AAAI2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.03544v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.03544v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaobao Wu, Xinshuai Dong, Thong Nguyen, Chaoqun Liu, Liangming Pan, Anh Tuan Luu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-lingual topic models have been prevalent for cross-lingual text
analysis by revealing aligned latent topics. However, most existing methods
suffer from producing repetitive topics that hinder further analysis and
performance decline caused by low-coverage dictionaries. In this paper, we
propose the Cross-lingual Topic Modeling with Mutual Information (InfoCTM).
Instead of the direct alignment in previous work, we propose a topic alignment
with mutual information method. This works as a regularization to properly
align topics and prevent degenerate topic representations of words, which
mitigates the repetitive topic issue. To address the low-coverage dictionary
issue, we further propose a cross-lingual vocabulary linking method that finds
more linked cross-lingual words for topic alignment beyond the translations of
a given dictionary. Extensive experiments on English, Chinese, and Japanese
datasets demonstrate that our method outperforms state-of-the-art baselines,
producing more coherent, diverse, and well-aligned topics and showing better
transferability for cross-lingual classification tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI2023 conference. Code is available at
  https://github.com/BobXWu/InfoCTM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Text to Source: Results in Detecting Large Language Model-Generated
  Content <span class="chip">COLING</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13322v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13322v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wissam Antoun, Benoît Sagot, Djamé Seddah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread use of Large Language Models (LLMs), celebrated for their
ability to generate human-like text, has raised concerns about misinformation
and ethical implications. Addressing these concerns necessitates the
development of robust methods to detect and attribute text generated by LLMs.
This paper investigates "Cross-Model Detection," by evaluating whether a
classifier trained to distinguish between source LLM-generated and
human-written text can also detect text from a target LLM without further
training. The study comprehensively explores various LLM sizes and families,
and assesses the impact of conversational fine-tuning techniques, quantization,
and watermarking on classifier generalization. The research also explores Model
Attribution, encompassing source model identification, model family, and model
size classification, in addition to quantization and watermarking detection.
Our results reveal several key findings: a clear inverse relationship between
classifier effectiveness and model size, with larger LLMs being more
challenging to detect, especially when the classifier is trained on data from
smaller models. Training on data from similarly sized LLMs can improve
detection performance from larger models but may lead to decreased performance
when dealing with smaller models. Additionally, model attribution experiments
show promising results in identifying source models and model families,
highlighting detectable signatures in LLM-generated text, with particularly
remarkable outcomes in watermarking detection, while no detectable signatures
of quantization were observed. Overall, our study contributes valuable insights
into the interplay of model size, family, and training data in LLM detection
and attribution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to COLING-LREC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Intrinsic Subgraph Generation for Interpretable Graph based Visual
  Question Answering <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17647v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17647v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pascal Tilli, Ngoc Thang Vu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The large success of deep learning based methods in Visual Question Answering
(VQA) has concurrently increased the demand for explainable methods. Most
methods in Explainable Artificial Intelligence (XAI) focus on generating
post-hoc explanations rather than taking an intrinsic approach, the latter
characterizing an interpretable model. In this work, we introduce an
interpretable approach for graph-based VQA and demonstrate competitive
performance on the GQA dataset. This approach bridges the gap between
interpretability and performance. Our model is designed to intrinsically
produce a subgraph during the question-answering process as its explanation,
providing insight into the decision making. To evaluate the quality of these
generated subgraphs, we compare them against established post-hoc
explainability methods for graph neural networks, and perform a human
evaluation. Moreover, we present quantitative metrics that correlate with the
evaluations of human assessors, acting as automatic metrics for the generated
explanatory subgraphs. Our implementation is available at
https://github.com/DigitalPhonetics/Intrinsic-Subgraph-Generation-for-VQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ÌròyìnSpeech: A multi-purpose Yorùbá Speech Corpus <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.16071v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.16071v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tolulope Ogunremi, Kola Tubosun, Anuoluwapo Aremu, Iroro Orife, David Ifeoluwa Adelani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce \`{I}r\`{o}y\`{i}nSpeech, a new corpus influenced by the desire
to increase the amount of high quality, contemporary Yor\`{u}b\'{a} speech
data, which can be used for both Text-to-Speech (TTS) and Automatic Speech
Recognition (ASR) tasks. We curated about 23000 text sentences from news and
creative writing domains with the open license CC-BY-4.0. To encourage a
participatory approach to data creation, we provide 5000 curated sentences to
the Mozilla Common Voice platform to crowd-source the recording and validation
of Yor\`{u}b\'{a} speech data. In total, we created about 42 hours of speech
data recorded by 80 volunteers in-house, and 6 hours of validated recordings on
Mozilla Common Voice platform. Our TTS evaluation suggests that a
high-fidelity, general domain, single-speaker Yor\`{u}b\'{a} voice is possible
with as little as 5 hours of speech. Similarly, for ASR we obtained a baseline
word error rate (WER) of 23.8.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Centered Masking for Language-Image Pre-Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15837v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15837v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingliang Liang, Martha Larson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Gaussian masking for Language-Image Pre-Training (GLIP) a novel,
straightforward, and effective technique for masking image patches during
pre-training of a vision-language model. GLIP builds on Fast Language-Image
Pre-Training (FLIP), which randomly masks image patches while training a CLIP
model. GLIP replaces random masking with centered masking, that uses a Gaussian
distribution and is inspired by the importance of image patches at the center
of the image. GLIP retains the same computational savings as FLIP, while
improving performance across a range of downstream datasets and tasks, as
demonstrated by our experimental results. We show the benefits of GLIP to be
easy to obtain, requiring no delicate tuning of the Gaussian, and also
applicable to data sets containing images without an obvious center focus.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ X-LLaVA: Optimizing Bilingual Large Vision-Language Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11399v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11399v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongjae Shin, Hyunseok Lim, Inho Won, Changsu Choi, Minjun Kim, Seungwoo Song, Hangyeol Yoo, Sangmin Kim, Kyungtae Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The impressive development of large language models (LLMs) is expanding into
the realm of large multimodal models (LMMs), which incorporate multiple types
of data beyond text. However, the nature of multimodal models leads to
significant expenses in the creation of training data. Furthermore,
constructing multilingual data for LMMs presents its own set of challenges due
to language diversity and complexity. Therefore, in this study, we propose two
cost-effective methods to solve this problem: (1) vocabulary expansion and
pretraining of multilingual LLM for specific languages, and (2) automatic and
elaborate construction of multimodal datasets using GPT4-V. Based on015 these
methods, we constructed a 91K English-Korean-Chinese multilingual, multimodal
training dataset. Additionally, we developed a bilingual multimodal model that
exhibits excellent performance in both Korean and English, surpassing existing
approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adapting Knowledge for Few-shot Table-to-Text Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.12468v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.12468v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhixin Guo, Minyxuan Yan, Jiexing Qi, Jianping Zhou, Ziwei He, Guanjie Zheng, Xinbing Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained language models (PLMs) have made remarkable progress in
table-to-text generation tasks. However, the lack of domain-specific knowledge
makes it challenging to bridge the topological gap between tabular data and
text, especially in real-world applications with limited resources. To mitigate
the limitation of insufficient labeled data, we propose a novel framework:
Adapt-Knowledge-to-Generate (AKG). The core insight of AKG is to adapt
unlabeled domain-specific knowledge into the model, which brings at least three
benefits: (1) it injects representation of normal table-related descriptions to
bridge the topological gap between tabular data and texts; (2) it enables us to
use large amounts of unlabeled domain-specific knowledge fully, which can
alleviate the PLMs' inherent shortcomings of lacking domain knowledge; (3) it
allows us to design various tasks to employ the domain-specific knowledge.
Extensive experiments and analyses are conducted on three open-domain, few-shot
natural language generation (NLG) data sets: Humans, Songs, and Books. Compared
to previous state-of-the-art approaches, our model achieves superior
performance in terms of both fluency and accuracy as judged by human and
automatic evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2302.04415</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EASYTOOL: Enhancing LLM-based <span class="highlight-title">Agent</span>s with Concise Tool Instruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.06201v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.06201v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Yongliang Shen, Ren Kan, Dongsheng Li, Deqing Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To address intricate real-world tasks, there has been a rising interest in
tool utilization in applications of large language models (LLMs). To develop
LLM-based agents, it usually requires LLMs to understand many tool functions
from different tool documentation. But these documentations could be diverse,
redundant or incomplete, which immensely affects the capability of LLMs in
using tools. To solve this, we introduce EASYTOOL, a framework transforming
diverse and lengthy tool documentation into a unified and concise tool
instruction for easier tool usage. EasyTool purifies essential information from
extensive tool documentation of different sources, and elaborates a unified
interface (i.e., tool instruction) to offer standardized tool descriptions and
functionalities for LLM-based agents. Extensive experiments on multiple
different tasks demonstrate that EasyTool can significantly reduce token
consumption and improve the performance of tool utilization in real-world
scenarios. Our code will be available at
\url{https://github.com/microsoft/JARVIS/} in the future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mix-Initiative Response Generation with Dynamic Prefix Tuning <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17636v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17636v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Nie, Heyan Huang, Xian-Ling Mao, Lizi Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixed initiative serves as one of the key factors in controlling conversation
directions. For a speaker, responding passively or leading proactively would
result in rather different responses. However, most dialogue systems focus on
training a holistic response generation model without any distinction among
different initiatives. It leads to the cross-contamination problem, where the
model confuses different initiatives and generates inappropriate responses.
Moreover, obtaining plenty of human annotations for initiative labels can be
expensive. To address this issue, we propose a general mix-Initiative Dynamic
Prefix Tuning framework (IDPT) to decouple different initiatives from the
generation model, which learns initiative-aware prefixes in both supervised and
unsupervised settings. Specifically, IDPT decouples initiative factors into
different prefix parameters and uses the attention mechanism to adjust the
selection of initiatives in guiding generation dynamically. The prefix
parameters can be tuned towards accurate initiative prediction as well as
mix-initiative response generation. Extensive experiments on two public
dialogue datasets show that the proposed IDPT outperforms previous baselines on
both automatic metrics and human evaluations. It also manages to generate
appropriate responses with manipulated initiatives.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the main conference of NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PEMA: An Offsite-Tunable Plug-in External Memory Adaptation for Language
  Models <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08590v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08590v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        HyunJin Kim, Young Jin Kim, JinYeong Bak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained language models (PLMs) show impressive performance in various
downstream NLP tasks. However, pre-training large language models demands
substantial memory and training compute. Furthermore, due to the substantial
resources required, many PLM weights are confidential. Consequently, users are
compelled to share their data with model owners for fine-tuning specific tasks.
To overcome the limitations, we introduce Plug-in External Memory Adaptation
(PEMA), a Parameter-Efficient Fine-Tuning (PEFT) method, enabling PLM
fine-tuning without requiring access to all the weights. PEMA integrates with
context representations from test data during inference to perform downstream
tasks. It uses external memory to store PLM-generated context representations
mapped with target tokens. Our method utilizes weight matrices of LoRA-like
bottlenecked adapter in the PLM's final layer to enhance efficiency. Our
approach also includes Gradual Unrolling, a novel interpolation strategy to
improve generation quality. We validate PEMA's effectiveness through
experiments on syntactic and real datasets for machine translation and style
transfer. Our findings show that PEMA outperforms other PEFT approaches in
memory and latency efficiency for training, and also excels in maintaining
sentence meaning and generating appropriate language and styles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CBQ: Cross-Block Quantization for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.07950v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.07950v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Ding, Xiaoyu Liu, Zhijun Tu, Yun Zhang, Wei Li, Jie Hu, Hanting Chen, Yehui Tang, Zhiwei Xiong, Baoqun Yin, Yunhe Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Post-training quantization (PTQ) has played a key role in compressing large
language models (LLMs) with ultra-low costs. However, existing PTQ methods only
focus on handling the outliers within one layer or one block, which ignores the
dependency of blocks and leads to severe performance degradation in low-bit
settings. In this paper, we propose CBQ, a cross-block reconstruction-based PTQ
method for LLMs. CBQ employs a cross-block dependency using a homologous
reconstruction scheme, establishing long-range dependencies across multiple
blocks to minimize error accumulation. Furthermore, CBQ incorporates a
coarse-to-fine preprocessing (CFP) strategy for suppressing weight and
activation outliers, coupled with an adaptive LoRA-Rounding technique for
precise weight quantization. These innovations enable CBQ to not only handle
extreme outliers effectively but also improve overall quantization accuracy.
Extensive experiments show that CBQ achieves superior low-bit quantization
(W4A4, W4A8, W2A16) and outperforms existing state-of-the-art methods across
various LLMs and datasets. Notably, CBQ quantizes the 4-bit LLAMA1-65B model
within only 4.3 hours on a single GPU, achieving a commendable tradeoff between
performance and quantization efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting
  Jailbreaks <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14965v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14965v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhinav Rao, Sachin Vashistha, Atharva Naik, Somak Aditya, Monojit Choudhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent explorations with commercial Large Language Models (LLMs) have shown
that non-expert users can jailbreak LLMs by simply manipulating their prompts;
resulting in degenerate output behavior, privacy and security breaches,
offensive outputs, and violations of content regulator policies. Limited
studies have been conducted to formalize and analyze these attacks and their
mitigations. We bridge this gap by proposing a formalism and a taxonomy of
known (and possible) jailbreaks. We survey existing jailbreak methods and their
effectiveness on open-source and commercial LLMs (such as GPT-based models,
OPT, BLOOM, and FLAN-T5-XXL). We further discuss the challenges of jailbreak
detection in terms of their effectiveness against known attacks. For further
analysis, we release a dataset of model outputs across 3700 jailbreak prompts
over 4 tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024 - The 2024 Joint International
  Conference on Computational Linguistics, Language Resources and Evaluation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BridgeTower: Building Bridges Between Encoders in Vision-Language
  Representation Learning <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.08657v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.08657v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Xu, Chenfei Wu, Shachar Rosenman, Vasudev Lal, Wanxiang Che, Nan Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language (VL) models with the Two-Tower architecture have dominated
visual-language representation learning in recent years. Current VL models
either use lightweight uni-modal encoders and learn to extract, align and fuse
both modalities simultaneously in a deep cross-modal encoder, or feed the
last-layer uni-modal representations from the deep pre-trained uni-modal
encoders into the top cross-modal encoder. Both approaches potentially restrict
vision-language representation learning and limit model performance. In this
paper, we propose BridgeTower, which introduces multiple bridge layers that
build a connection between the top layers of uni-modal encoders and each layer
of the cross-modal encoder. This enables effective bottom-up cross-modal
alignment and fusion between visual and textual representations of different
semantic levels of pre-trained uni-modal encoders in the cross-modal encoder.
Pre-trained with only 4M images, BridgeTower achieves state-of-the-art
performance on various downstream vision-language tasks. In particular, on the
VQAv2 test-std set, BridgeTower achieves an accuracy of 78.73%, outperforming
the previous state-of-the-art model METER by 1.09% with the same pre-training
data and almost negligible additional parameters and computational costs.
Notably, when further scaling the model, BridgeTower achieves an accuracy of
81.15%, surpassing models that are pre-trained on orders-of-magnitude larger
datasets. Code and checkpoints are available at
https://github.com/microsoft/BridgeTower.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2023, Oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language Models are Free Boosters for Biomedical Imaging Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17343v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17343v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhixin Lai, Jing Wu, Suiyao Chen, Yucheng Zhou, Naira Hovakimyan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we uncover the unexpected efficacy of residual-based large
language models (LLMs) as part of encoders for biomedical imaging tasks, a
domain traditionally devoid of language or textual data. The approach diverges
from established methodologies by utilizing a frozen transformer block,
extracted from pre-trained LLMs, as an innovative encoder layer for the direct
processing of visual tokens. This strategy represents a significant departure
from the standard multi-modal vision-language frameworks, which typically hinge
on language-driven prompts and inputs. We found that these LLMs could boost
performance across a spectrum of biomedical imaging applications, including
both 2D and 3D visual classification tasks, serving as plug-and-play boosters.
More interestingly, as a byproduct, we found that the proposed framework
achieved superior performance, setting new state-of-the-art results on
extensive, standardized datasets in MedMNIST-2D and 3D. Through this work, we
aim to open new avenues for employing LLMs in biomedical imaging and enriching
the understanding of their potential in this specialized domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Partial Mobilization: Tracking Multilingual Information Flows Amongst
  Russian Media Outlets and Telegram 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10856v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10856v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hans W. A. Hanley, Zakir Durumeric
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In response to disinformation and propaganda from Russian online media
following the invasion of Ukraine, Russian media outlets such as Russia Today
and Sputnik News were banned throughout Europe. To maintain viewership, many of
these Russian outlets began to heavily promote their content on messaging
services like Telegram. In this work, we study how 16 Russian media outlets
interacted with and utilized 732 Telegram channels throughout 2022. Leveraging
the foundational model MPNet, DP-means clustering, and Hawkes processes, we
trace how narratives spread between news sites and Telegram channels. We show
that news outlets not only propagate existing narratives through Telegram but
that they source material from the messaging platform. For example, across the
websites in our study, between 2.3% (ura.news) and 26.7% (ukraina.ru) of
articles discussed content that originated/resulted from activity on Telegram.
Finally, tracking the spread of individual topics, we measure the rate at which
news outlets and Telegram channels disseminate content within the Russian media
ecosystem, finding that websites like ura.news and Telegram channels such as
@genshab are the most effective at disseminating their content.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICWSM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NLP-based detection of systematic anomalies among the narratives of
  consumer complaints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.11138v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.11138v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peiheng Gao, Ning Sun, Xuefeng Wang, Chen Yang, Ričardas Zitikis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop an NLP-based procedure for detecting systematic nonmeritorious
consumer complaints, simply called systematic anomalies, among complaint
narratives. While classification algorithms are used to detect pronounced
anomalies, in the case of smaller and frequent systematic anomalies, the
algorithms may falter due to a variety of reasons, including technical ones as
well as natural limitations of human analysts. Therefore, as the next step
after classification, we convert the complaint narratives into quantitative
data, which are then analyzed using an algorithm for detecting systematic
anomalies. We illustrate the entire procedure using complaint narratives from
the Consumer Complaint Database of the Consumer Financial Protection Bureau.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Science and Game Theory
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collective schedules: axioms and algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Durand, Fanny Pascual
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The collective schedules problem consists in computing a schedule of tasks
shared between individuals. Tasks may have different duration, and individuals
have preferences over the order of the shared tasks. This problem has numerous
applications since tasks may model public infrastructure projects, events
taking place in a shared room, or work done by co-workers. Our aim is, given
the preferred schedules of individuals (voters), to return a consensus
schedule. We propose an axiomatic study of the collective schedule problem, by
using classic axioms in computational social choice and new axioms that take
into account the duration of the tasks. We show that some axioms are
incompatible, and we study the axioms fulfilled by three rules: one which has
been studied in the seminal paper on collective schedules (Pascual et al.
2018), one which generalizes the Kemeny rule, and one which generalizes
Spearman's footrule. From an algorithmic point of view, we show that these
rules solve NP-hard problems, but that it is possible to solve optimally these
problems for small but realistic size instances, and we give an efficient
heuristic for large instances. We conclude this paper with experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Metric Distortion of Randomized Social Choice Functions: C1 Maximal
  Lottery Rules and Simulations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18340v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18340v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Frank, Patrick Lederer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The metric distortion of a randomized social choice function (RSCF)
quantifies its worst-case approximation ratio of the optimal social cost when
the voters' costs for alternatives are given by distances in a metric space.
This notion has recently attracted significant attention as numerous RSCFs that
aim to minimize the metric distortion have been suggested. However, such
tailored voting rules usually have little appeal other than their low metric
distortion. In this paper, we will thus study the metric distortion of
well-established RSCFs. In more detail, we first show that C1 maximal lottery
rules, a well-known class of RSCFs, have a metric distortion of $4$ and
furthermore prove that this is optimal within the class of majoritarian RSCFs
(which only depend on the majority relation). As our second contribution, we
perform extensive computer experiments on the metric distortion of established
RSCFs to obtain insights into their average-case performance. These computer
experiments are based on a new linear program for computing the metric
distortion of a lottery on a given profile and reveal that some classical RSCFs
perform almost as well as the currently best known RSCF with respect to the
metric distortion on randomly sampled profiles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mistake, Manipulation and Margin Guarantees in Online Strategic
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18176v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18176v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingqing Shen, Nam Ho-Nguyen, Khanh-Hung Giang-Tran, Fatma Kılınç-Karzan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider an online strategic classification problem where each arriving
agent can manipulate their true feature vector to obtain a positive predicted
label, while incurring a cost that depends on the amount of manipulation. The
learner seeks to predict the agent's true label given access to only the
manipulated features. After the learner releases their prediction, the agent's
true label is revealed. Previous algorithms such as the strategic perceptron
guarantee finitely many mistakes under a margin assumption on agents' true
feature vectors. However, these are not guaranteed to encourage agents to be
truthful. Promoting truthfulness is intimately linked to obtaining adequate
margin on the predictions, thus we provide two new algorithms aimed at
recovering the maximum margin classifier in the presence of strategic agent
behavior. We prove convergence, finite mistake and finite manipulation
guarantees for a variety of agent cost structures. We also provide generalized
versions of the strategic perceptron with mistake guarantees for different
costs. Our numerical study on real and synthetic data demonstrates that the new
algorithms outperform previous ones in terms of margin, number of manipulation
and number of mistakes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Local (coarse) correlated equilibria in non-concave <span class="highlight-title">game</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18174v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18174v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mete Şeref Ahunbay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate local notions of correlated equilibria, distributions of
actions for smooth games such that players do not incur any regret against
modifications of their strategies along a set of continuous vector fields. Our
analysis shows that such equilibria are intrinsically linked to the projected
gradient dynamics of the game. We identify the equivalent of coarse equilibria
in this setting when no regret is incurred against any gradient field of a
differentiable function. As a result, such equilibria are approximable when all
players employ online (projected) gradient ascent with equal step-sizes as
learning algorithms, and when their compact and convex action sets either (1)
possess a smooth boundary, or (2) are polyhedra over which linear optimisation
is ``trivial''. As a consequence, primal-dual proofs of performance guarantees
for local coarse equilibria take the form of a generalised Lyapunov function
for the gradient dynamics of the game. Adapting the regret matching framework
to our setting, we also show that general local correlated equilibria are
approximable when the set of vector fields is finite, given access to a
fixed-point oracle for linear or conical combinations. For the class of
affine-linear vector fields, which subsumes correlated equilibria of normal
form games as a special case, such a fixed-point turns out to be the solution
of a convex quadratic minimisation problem. Our results are independent of
concavity assumptions on players' utilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Cyber Response Time on Temporal Active Directory Networks
  Using Decoys <span class="chip">GECCO 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18162v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18162v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huy Q. Ngo, Mingyu Guo, Hung Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Microsoft Active Directory (AD) is the default security management system for
Window domain network. We study the problem of placing decoys in AD network to
detect potential attacks. We model the problem as a Stackelberg game between an
attacker and a defender on AD attack graphs where the defender employs a set of
decoys to detect the attacker on their way to Domain Admin (DA). Contrary to
previous works, we consider time-varying (temporal) attack graphs. We proposed
a novel metric called response time, to measure the effectiveness of our decoy
placement in temporal attack graphs. Response time is defined as the duration
from the moment attackers trigger the first decoy to when they compromise the
DA. Our goal is to maximize the defender's response time to the worst-case
attack paths. We establish the NP-hard nature of the defender's optimization
problem, leading us to develop Evolutionary Diversity Optimization (EDO)
algorithms. EDO algorithms identify diverse sets of high-quality solutions for
the optimization problem. Despite the polynomial nature of the fitness
function, it proves experimentally slow for larger graphs. To enhance
scalability, we proposed an algorithm that exploits the static nature of AD
infrastructure in the temporal setting. Then, we introduce tailored repair
operations, ensuring the convergence to better results while maintaining
scalability for larger graphs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be appear in ACM GECCO 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Equilibrium Analysis of the Arad-Rubinstein <span class="highlight-title">Game</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17139v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17139v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Ewerhart, Stanisław Kaźmierowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Colonel Blotto games with discrete strategy spaces effectively illustrate the
intricate nature of multidimensional strategic reasoning. This paper studies
the equilibrium set of such games where, in line with prior experimental work,
the tie-breaking rule is allowed to be flexible. We begin by pointing out that
equilibrium constructions known from the literature extend to our class of
games. However, we also note that irrespective of the tie-breaking rule, the
equilibrium set is excessively large. Specifically, any pure strategy that
allocates at most twice the fair share to each battlefield is used with
positive probability in some equilibrium. Furthermore, refinements based on the
elimination of weakly dominated strategies prove ineffective. To derive
specific predictions amid this multiplicity, we compute strategies resulting
from long-run adaptive learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ No-Regret Learning in Bilateral Trade via Global Budget Balance <span class="chip">STOC 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12370v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12370v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martino Bernasconi, Matteo Castiglioni, Andrea Celli, Federico Fusco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bilateral trade models the problem of intermediating between two rational
agents -- a seller and a buyer -- both characterized by a private valuation for
an item they want to trade. We study the online learning version of the
problem, in which at each time step a new seller and buyer arrive and the
learner has to set prices for them without any knowledge about their
(adversarially generated) valuations.
  In this setting, known impossibility results rule out the existence of
no-regret algorithms when budget balanced has to be enforced at each time step.
In this paper, we introduce the notion of \emph{global budget balance}, which
only requires the learner to fulfill budget balance over the entire time
horizon. Under this natural relaxation, we provide the first no-regret
algorithms for adversarial bilateral trade under various feedback models.
First, we show that in the full-feedback model, the learner can guarantee
$\tilde O(\sqrt{T})$ regret against the best fixed prices in hindsight, and
that this bound is optimal up to poly-logarithmic terms. Second, we provide a
learning algorithm guaranteeing a $\tilde O(T^{3/4})$ regret upper bound with
one-bit feedback, which we complement with a $\Omega(T^{5/7})$ lower bound that
holds even in the two-bit feedback model. Finally, we introduce and analyze an
alternative benchmark that is provably stronger than the best fixed prices in
hindsight and is inspired by the literature on bandits with knapsacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at STOC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Coevolution of cognition and cooperation in structured populations under
  <span class="highlight-title">reinforcement</span> learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.11376v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.11376v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rossana Mastrandrea, Leonardo Boncinelli, Ennio Bilancini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the evolution of behavior under reinforcement learning in a
Prisoner's Dilemma where agents interact in a regular network and can learn
about whether they play one-shot or repeatedly by incurring a cost of
deliberation. With respect to other behavioral rules used in the literature,
(i) we confirm the existence of a threshold value of the probability of
repeated interaction, switching the emergent behavior from intuitive defector
to dual-process cooperator; (ii) we find a different role of the node degree,
with smaller degrees reducing the evolutionary success of dual-process
cooperators; (iii) we observe a higher frequency of deliberation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Follower Agnostic Methods for Stackelberg <span class="highlight-title">Game</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.01421v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.01421v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chinmay Maheshwari, James Cheng, S. Shankar Sasty, Lillian Ratliff, Eric Mazumdar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present an efficient algorithm to solve online Stackelberg
games, featuring multiple followers, in a follower-agnostic manner. Unlike
previous works, our approach works even when leader has no knowledge about the
followers' utility functions or strategy space. Our algorithm introduces a
unique gradient estimator, leveraging specially designed strategies to probe
followers. In a departure from traditional assumptions of optimal play, we
model followers' responses using a convergent adaptation rule, allowing for
realistic and dynamic interactions. The leader constructs the gradient
estimator solely based on observations of followers' actions. We provide both
non-asymptotic convergence rates to stationary points of the leader's objective
and demonstrate asymptotic convergence to a \emph{local Stackelberg
equilibrium}. To validate the effectiveness of our algorithm, we use this
algorithm to solve the problem of incentive design on a large-scale
transportation network, showcasing its robustness even when the leader lacks
access to followers' demand.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth
  Estimation <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18807v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18807v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suraj Patni, Aradhye Agarwal, Chetan Arora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the absence of parallax cues, a learning-based single image depth
estimation (SIDE) model relies heavily on shading and contextual cues in the
image. While this simplicity is attractive, it is necessary to train such
models on large and varied datasets, which are difficult to capture. It has
been shown that using embeddings from pre-trained foundational models, such as
CLIP, improves zero shot transfer in several applications. Taking inspiration
from this, in our paper we explore the use of global image priors generated
from a pre-trained ViT model to provide more detailed contextual information.
We argue that the embedding vector from a ViT model, pre-trained on a large
dataset, captures greater relevant information for SIDE than the usual route of
generating pseudo image captions, followed by CLIP based text embeddings. Based
on this idea, we propose a new SIDE model using a diffusion backbone which is
conditioned on ViT embeddings. Our proposed design establishes a new
state-of-the-art (SOTA) for SIDE on NYUv2 dataset, achieving Abs Rel error of
0.059(14% improvement) compared to 0.069 by the current SOTA (VPD). And on
KITTI dataset, achieving Sq Rel error of 0.139 (2% improvement) compared to
0.142 by the current SOTA (GEDepth). For zero-shot transfer with a model
trained on NYUv2, we report mean relative improvement of (20%, 23%, 81%, 25%)
over NeWCRFs on (Sun-RGBD, iBims1, DIODE, HyperSim) datasets, compared to (16%,
18%, 45%, 9%) by ZoeDepth. The code is available at
https://github.com/Aradhye2002/EcoDepth.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long-form factuality in large language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, Cosmo Du, Quoc V. Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often generate content that contains factual
errors when responding to fact-seeking prompts on open-ended topics. To
benchmark a model's long-form factuality in open domains, we first use GPT-4 to
generate LongFact, a prompt set comprising thousands of questions spanning 38
topics. We then propose that LLM agents can be used as automated evaluators for
long-form factuality through a method which we call Search-Augmented Factuality
Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into
a set of individual facts and to evaluate the accuracy of each fact using a
multi-step reasoning process comprising sending search queries to Google Search
and determining whether a fact is supported by the search results. Furthermore,
we propose extending F1 score as an aggregated metric for long-form factuality.
To do so, we balance the percentage of supported facts in a response
(precision) with the percentage of provided facts relative to a hyperparameter
representing a user's preferred response length (recall).
  Empirically, we demonstrate that LLM agents can achieve superhuman rating
performance - on a set of ~16k individual facts, SAFE agrees with crowdsourced
human annotators 72% of the time, and on a random subset of 100 disagreement
cases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times
cheaper than human annotators. We also benchmark thirteen language models on
LongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding
that larger language models generally achieve better long-form factuality.
LongFact, SAFE, and all experimental code are available at
https://github.com/google-deepmind/long-form-factuality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ImageNet-D: Benchmarking Neural Network Robustness on Diffusion
  Synthetic Object <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18775v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18775v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenshuang Zhang, Fei Pan, Junmo Kim, In So Kweon, Chengzhi Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We establish rigorous benchmarks for visual perception robustness. Synthetic
images such as ImageNet-C, ImageNet-9, and Stylized ImageNet provide specific
type of evaluation over synthetic corruptions, backgrounds, and textures, yet
those robustness benchmarks are restricted in specified variations and have low
synthetic quality. In this work, we introduce generative model as a data source
for synthesizing hard images that benchmark deep models' robustness. Leveraging
diffusion models, we are able to generate images with more diversified
backgrounds, textures, and materials than any prior work, where we term this
benchmark as ImageNet-D. Experimental results show that ImageNet-D results in a
significant accuracy drop to a range of vision models, from the standard ResNet
visual classifier to the latest foundation models like CLIP and MiniGPT-4,
significantly reducing their accuracy by up to 60\%. Our work suggests that
diffusion models can be an effective source to test vision models. The code and
dataset are available at https://github.com/chenshuang-zhang/imagenet_d.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Superior Parallel Big Data Clustering through Competitive Stochastic
  <span class="highlight-title">Sample</span> Size Optimization in Big-means 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rustam Mussabayev, Ravil Mussabayev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel K-means clustering algorithm, an advancement on
the conventional Big-means methodology. The proposed method efficiently
integrates parallel processing, stochastic sampling, and competitive
optimization to create a scalable variant designed for big data applications.
It addresses scalability and computation time challenges typically faced with
traditional techniques. The algorithm adjusts sample sizes dynamically for each
worker during execution, optimizing performance. Data from these sample sizes
are continually analyzed, facilitating the identification of the most efficient
configuration. By incorporating a competitive element among workers using
different sample sizes, efficiency within the Big-means algorithm is further
stimulated. In essence, the algorithm balances computational time and
clustering quality by employing a stochastic, competitive sampling strategy in
a parallel computing setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CaT: Constraints as Terminations for Legged Locomotion <span class="highlight-title">Reinforcement</span>
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18765v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18765v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elliot Chane-Sane, Pierre-Alexandre Leziart, Thomas Flayols, Olivier Stasse, Philippe Souères, Nicolas Mansard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Reinforcement Learning (RL) has demonstrated impressive results in
solving complex robotic tasks such as quadruped locomotion. Yet, current
solvers fail to produce efficient policies respecting hard constraints. In this
work, we advocate for integrating constraints into robot learning and present
Constraints as Terminations (CaT), a novel constrained RL algorithm. Departing
from classical constrained RL formulations, we reformulate constraints through
stochastic terminations during policy learning: any violation of a constraint
triggers a probability of terminating potential future rewards the RL agent
could attain. We propose an algorithmic approach to this formulation, by
minimally modifying widely used off-the-shelf RL algorithms in robot learning
(such as Proximal Policy Optimization). Our approach leads to excellent
constraint adherence without introducing undue complexity and computational
overhead, thus mitigating barriers to broader adoption. Through empirical
evaluation on the real quadruped robot Solo crossing challenging obstacles, we
demonstrate that CaT provides a compelling solution for incorporating
constraints into RL frameworks. Videos and code are available at
https://constraints-as-terminations.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project webpage: https://constraints-as-terminations.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detection of subclinical atherosclerosis by image-based deep learning on
  chest x-ray 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guglielmo Gallone, Francesco Iodice, Alberto Presta, Davide Tore, Ovidio de Filippo, Michele Visciano, Carlo Alberto Barbano, Alessandro Serafini, Paola Gorrini, Alessandro Bruno, Walter Grosso Marra, James Hughes, Mario Iannaccone, Paolo Fonio, Attilio Fiandrotti, Alessandro Depaoli, Marco Grangetto, Gaetano Maria de Ferrari, Fabrizio D'Ascenzo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aims. To develop a deep-learning based system for recognition of subclinical
atherosclerosis on a plain frontal chest x-ray. Methods and Results. A
deep-learning algorithm to predict coronary artery calcium (CAC) score (the
AI-CAC model) was developed on 460 chest x-ray (80% training cohort, 20%
internal validation cohort) of primary prevention patients (58.4% male, median
age 63 [51-74] years) with available paired chest x-ray and chest computed
tomography (CT) indicated for any clinical reason and performed within 3
months. The CAC score calculated on chest CT was used as ground truth. The
model was validated on an temporally-independent cohort of 90 patients from the
same institution (external validation). The diagnostic accuracy of the AI-CAC
model assessed by the area under the curve (AUC) was the primary outcome.
Overall, median AI-CAC score was 35 (0-388) and 28.9% patients had no AI-CAC.
AUC of the AI-CAC model to identify a CAC>0 was 0.90 in the internal validation
cohort and 0.77 in the external validation cohort. Sensitivity was consistently
above 92% in both cohorts. In the overall cohort (n=540), among patients with
AI-CAC=0, a single ASCVD event occurred, after 4.3 years. Patients with
AI-CAC>0 had significantly higher Kaplan Meier estimates for ASCVD events
(13.5% vs. 3.4%, log-rank=0.013). Conclusion. The AI-CAC model seems to
accurately detect subclinical atherosclerosis on chest x-ray with elevated
sensitivity, and to predict ASCVD events with elevated negative predictive
value. Adoption of the AI-CAC model to refine CV risk stratification or as an
opportunistic screening tool requires prospective evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to European Heart Journal - Cardiovascular Imaging Added
  also the additional material 44 pages (30 main paper, 14 additional
  material), 14 figures (5 main manuscript, 9 additional material)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding the Learning Dynamics of Alignment with Human Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shawn Im, Yixuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligning large language models (LLMs) with human intentions has become a
critical task for safely deploying models in real-world systems. While existing
alignment approaches have seen empirical success, theoretically understanding
how these methods affect model behavior remains an open question. Our work
provides an initial attempt to theoretically analyze the learning dynamics of
human preference alignment. We formally show how the distribution of preference
datasets influences the rate of model updates and provide rigorous guarantees
on the training accuracy. Our theory also reveals an intricate phenomenon where
the optimization is prone to prioritizing certain behaviors with higher
preference distinguishability. We empirically validate our findings on
contemporary LLMs and alignment tasks, reinforcing our theoretical insights and
shedding light on considerations for future alignment approaches. Disclaimer:
This paper contains potentially offensive text; reader discretion is advised.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Usage-Specific Survival Modeling Based on Operational Data and Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olov Holmer, Mattias Krysander, Erik Frisk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate predictions of when a component will fail are crucial when planning
maintenance, and by modeling the distribution of these failure times, survival
models have shown to be particularly useful in this context. The presented
methodology is based on conventional neural network-based survival models that
are trained using data that is continuously gathered and stored at specific
times, called snapshots. An important property of this type of training data is
that it can contain more than one snapshot from a specific individual which
results in that standard maximum likelihood training can not be directly
applied since the data is not independent. However, the papers show that if the
data is in a specific format where all snapshot times are the same for all
individuals, called homogeneously sampled, maximum likelihood training can be
applied and produce desirable results. In many cases, the data is not
homogeneously sampled and in this case, it is proposed to resample the data to
make it homogeneously sampled. How densely the dataset is sampled turns out to
be an important parameter; it should be chosen large enough to produce good
results, but this also increases the size of the dataset which makes training
slow. To reduce the number of samples needed during training, the paper also
proposes a technique to, instead of resampling the dataset once before the
training starts, randomly resample the dataset at the start of each epoch
during the training. The proposed methodology is evaluated on both a simulated
dataset and an experimental dataset of starter battery failures. The results
show that if the data is homogeneously sampled the methodology works as
intended and produces accurate survival models. The results also show that
randomly resampling the dataset on each epoch is an effective way to reduce the
size of the training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nonlinear model reduction for operator learning <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18735v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18735v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamidreza Eivazi, Stefan Wittek, Andreas Rausch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Operator learning provides methods to approximate mappings between
infinite-dimensional function spaces. Deep operator networks (DeepONets) are a
notable architecture in this field. Recently, an extension of DeepONet based on
model reduction and neural networks, proper orthogonal decomposition
(POD)-DeepONet, has been able to outperform other architectures in terms of
accuracy for several benchmark tests. We extend this idea towards nonlinear
model order reduction by proposing an efficient framework that combines neural
networks with kernel principal component analysis (KPCA) for operator learning.
Our results demonstrate the superior performance of KPCA-DeepONet over
POD-DeepONet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a Tiny Paper at ICLR 2024 (Notable)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Manufacturing Quality Prediction Models through the
  Integration of Explainability Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18731v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18731v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dennis Gross, Helge Spieker, Arnaud Gotlieb, Ricardo Knoblauch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research presents a method that utilizes explainability techniques to
amplify the performance of machine learning (ML) models in forecasting the
quality of milling processes, as demonstrated in this paper through a
manufacturing use case. The methodology entails the initial training of ML
models, followed by a fine-tuning phase where irrelevant features identified
through explainability methods are eliminated. This procedural refinement
results in performance enhancements, paving the way for potential reductions in
manufacturing costs and a better understanding of the trained ML models. This
study highlights the usefulness of explainability techniques in both explaining
and optimizing predictive models in the manufacturing realm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-Supervised Learning for Deep Causal Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasin Ibrahim, Hermione Warr, Konstantinos Kamnitsas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing models that can answer questions of the form "How would $x$ change
if $y$ had been $z$?" is fundamental for advancing medical image analysis.
Training causal generative models that address such counterfactual questions,
though, currently requires that all relevant variables have been observed and
that corresponding labels are available in training data. However, clinical
data may not have complete records for all patients and state of the art causal
generative models are unable to take full advantage of this. We thus develop,
for the first time, a semi-supervised deep causal generative model that
exploits the causal relationships between variables to maximise the use of all
available data. We explore this in the setting where each sample is either
fully labelled or fully unlabelled, as well as the more clinically realistic
case of having different labels missing for each sample. We leverage techniques
from causal inference to infer missing values and subsequently generate
realistic counterfactuals, even for samples with incomplete labels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning for Traffic Flow Prediction using Cellular Automata-based
  Model and CNN-LSTM architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18710v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18710v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaohui Yang, Kshitij Jerath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works have attempted to use deep learning to predict future states of
traffic flow, but have met with mixed results. These approaches face two key
challenges. First, training deep learning neural networks requires large
amounts of training data which are not yet easily available for traffic flow
systems. Second, even when data is available, the neural networks require
access to historical data that covers most possible traffic flow dynamics to
successfully predict future traffic states. Specifically, these deep learning
approaches do not fully leverage domain-knowledge about traffic flow dynamics,
despite a significant existing knowledge-base. In this work, we propose to
solve both issues using a Convolutional Neural Network (CNNs) with Long Short
Term Memory (LSTM) deep learning architecture to successfully predict traffic
flow, while leveraging a cellular automata-based statistical mechanics model of
traffic flow to generate training and test data. Another major contribution of
this paper is the insight that training data for a large traffic system can
actually be sampled from the simulations of a much smaller traffic system. This
is achieved through observing that the normalized energy distribution of the
statistical mechanics model is scale invariant, which significantly eases the
burden of data generation for large scale traffic systems. The resulting
simulations indicate good agreement between the predicted and the true traffic
flow dynamics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conditional Wasserstein Distances with Applications in Bayesian OT Flow
  Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18705v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18705v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jannis Chemseddine, Paul Hagemann, Christian Wald, Gabriele Steidl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In inverse problems, many conditional generative models approximate the
posterior measure by minimizing a distance between the joint measure and its
learned approximation. While this approach also controls the distance between
the posterior measures in the case of the Kullback--Leibler divergence, this is
in general not hold true for the Wasserstein distance. In this paper, we
introduce a conditional Wasserstein distance via a set of restricted couplings
that equals the expected Wasserstein distance of the posteriors. Interestingly,
the dual formulation of the conditional Wasserstein-1 flow resembles losses in
the conditional Wasserstein GAN literature in a quite natural way. We derive
theoretical properties of the conditional Wasserstein distance, characterize
the corresponding geodesics and velocity fields as well as the flow ODEs.
Subsequently, we propose to approximate the velocity fields by relaxing the
conditional Wasserstein distance. Based on this, we propose an extension of OT
Flow Matching for solving Bayesian inverse problems and demonstrate its
numerical advantages on an inverse problem and class-conditional image
generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper supersedes arXiv:2310.13433</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fpga-Based Neural Thrust Controller for UAVs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18703v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18703v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sharif Azem, David Scheunert, Mengguang Li, Jonas Gehrunger, Kai Cui, Christian Hochberger, Heinz Koepp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of unmanned aerial vehicles (UAVs) has improved a variety of
fields by providing a versatile, cost-effective and accessible platform for
implementing state-of-the-art algorithms. To accomplish a broader range of
tasks, there is a growing need for enhanced on-board computing to cope with
increasing complexity and dynamic environmental conditions. Recent advances
have seen the application of Deep Neural Networks (DNNs), particularly in
combination with Reinforcement Learning (RL), to improve the adaptability and
performance of UAVs, especially in unknown environments. However, the
computational requirements of DNNs pose a challenge to the limited computing
resources available on many UAVs. This work explores the use of Field
Programmable Gate Arrays (FPGAs) as a viable solution to this challenge,
offering flexibility, high performance, energy and time efficiency. We propose
a novel hardware board equipped with an Artix-7 FPGA for a popular open-source
micro-UAV platform. We successfully validate its functionality by implementing
an RL-based low-level controller using real-world experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contrastive Learning with Orthonormal Anchors (CLOA) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huanran Li, Daniel Pimentel-Alarcón
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study focuses on addressing the instability issues prevalent in
contrastive learning, specifically examining the InfoNCE loss function and its
derivatives. We reveal a critical observation that these loss functions exhibit
a restrictive behavior, leading to a convergence phenomenon where embeddings
tend to merge into a singular point. This "over-fusion" effect detrimentally
affects classification accuracy in subsequent supervised-learning tasks.
Through theoretical analysis, we demonstrate that embeddings, when equalized or
confined to a rank-1 linear subspace, represent a local minimum for InfoNCE. In
response to this challenge, our research introduces an innovative strategy that
leverages the same or fewer labeled data than typically used in the fine-tuning
phase. The loss we proposed, Orthonormal Anchor Regression Loss, is designed to
disentangle embedding clusters, significantly enhancing the distinctiveness of
each embedding while simultaneously ensuring their aggregation into dense,
well-defined clusters. Our method demonstrates remarkable improvements with
just a fraction of the conventional label requirements, as evidenced by our
results on CIFAR10 and CIFAR100 datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InceptionTime vs. Wavelet -- A comparison for time series classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18687v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18687v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Klenkert, Daniel Schaeffer, Julian Stauch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks were used to classify infrasound data. Two different
approaches were compared. One based on the direct classification of time series
data, using a custom implementation of the InceptionTime network. For the other
approach, we generated 2D images of the wavelet transformation of the signals,
which were subsequently classified using a ResNet implementation. Choosing
appropriate hyperparameter settings, both achieve a classification accuracy of
above 90 %, with the direct approach reaching 95.2 %.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Representatividad Muestral en la Incertidumbre Simétrica Multivariada
  para la Selección de Atributos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18685v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18685v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gustavo Sosa-Cabrera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we analyze the behavior of the multivariate symmetric
uncertainty (MSU) measure through the use of statistical simulation techniques
under various mixes of informative and non-informative randomly generated
features. Experiments show how the number of attributes, their cardinalities,
and the sample size affect the MSU. In this thesis, through observation of
results, it is proposed an heuristic condition that preserves good quality in
the MSU under different combinations of these three factors, providing a new
useful criterion to help drive the process of dimension reduction.
  --
  En el presente trabajo hemos analizado el comportamiento de una versi\'on
multivariada de la incertidumbre sim\'etrica a trav\'es de t\'ecnicas de
simulaci\'on estad\'isticas sobre varias combinaciones de atributos
informativos y no-informativos generados de forma aleatoria. Los experimentos
muestran como el n\'umero de atributos, sus cardinalidades y el tama\~no
muestral afectan al MSU como medida. En esta tesis, mediante la observaci\'on
de resultados hemos propuesto una condici\'on que preserva una buena calidad en
el MSU bajo diferentes combinaciones de los tres factores mencionados, lo cual
provee un nuevo y valioso criterio para llevar a cabo el proceso de reducci\'on
de dimensionalidad.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>52 pages, in Spanish. Advisors: Miguel Garc\'ia-Torres, Santiago
  G\'omez-Guerrero, Christian E. Schaerer Serra</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TransFusion: Contrastive Learning with Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huanran Li, Daniel Pimentel-Alarcón
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel framework, TransFusion, designed to make the
process of contrastive learning more analytical and explainable. TransFusion
consists of attention blocks whose softmax being replaced by ReLU, and its
final block's weighted-sum operation is truncated to leave the adjacency matrix
as the output. The model is trained by minimizing the Jensen-Shannon Divergence
between its output and the target affinity matrix, which indicates whether each
pair of samples belongs to the same or different classes. The main contribution
of TransFusion lies in defining a theoretical limit for answering two
fundamental questions in the field: the maximum level of data augmentation and
the minimum batch size required for effective contrastive learning.
Furthermore, experimental results indicate that TransFusion successfully
extracts features that isolate clusters from complex real-world data, leading
to improved classification accuracy in downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 4 figures,</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NL-ITI: Optimizing Probing and Intervention for Improvement of ITI
  Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18680v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18680v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakub Hoscilowicz, Adam Wiacek, Jan Chojnacki, Adam Cieslak, Leszek Michon, Vitalii Urbanevych, Artur Janicki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLM) are prone to returning false information. It
constitutes one of major challenges in the AI field. In our work, we explore
paradigm introduced by Inference-Time-Intervention (ITI). In first stage, it
identifies attention heads, which contain the highest amount of desired type of
knowledge (e.g., truthful). Afterwards, during inference, LLM activations are
shifted for chosen subset of attention heads. We further improved the ITI
framework by introducing a nonlinear probing and multi-token intervention -
Non-Linear ITI (NL-ITI). NL-ITI is tested on diverse multiple-choice
benchmarks, including TruthfulQA, on which we report around 14% MC1 metric
improvement with respect to the baseline ITI results. NL-ITI achieves also
encouraging results on other testsets - on Business Ethics subdomain of MMLU,
around 18% MC1 improvement over baseline LLaMA2-7B. Additionally, NL-ITI
performs better while being less invasive in the behavior of LLM at the same
time (as measured by Kullback-Leibler divergence).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at https://github.com/Samsung/NL-ITI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fact Checking Beyond Training Set <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18671v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18671v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Payam Karisani, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating the veracity of everyday claims is time consuming and in some
cases requires domain expertise. We empirically demonstrate that the commonly
used fact checking pipeline, known as the retriever-reader, suffers from
performance deterioration when it is trained on the labeled data from one
domain and used in another domain. Afterwards, we delve into each component of
the pipeline and propose novel algorithms to address this problem. We propose
an adversarial algorithm to make the retriever component robust against
distribution shift. Our core idea is to initially train a bi-encoder on the
labeled source data, and then, to adversarially train two separate document and
claim encoders using unlabeled target data. We then focus on the reader
component and propose to train it such that it is insensitive towards the order
of claims and evidence documents. Our empirical evaluations support the
hypothesis that such a reader shows a higher robustness against distribution
shift. To our knowledge, there is no publicly available multi-topic fact
checking dataset. Thus, we propose a simple automatic method to re-purpose two
well-known fact checking datasets. We then construct eight fact checking
scenarios from these datasets, and compare our model to a set of strong
baseline models, including recent domain adaptation models that use GPT4 for
generating synthetic data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aiming for Relevance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18668v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18668v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bar Eini Porat, Danny Eytan, Uri Shalit
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vital signs are crucial in intensive care units (ICUs). They are used to
track the patient's state and to identify clinically significant changes.
Predicting vital sign trajectories is valuable for early detection of adverse
events. However, conventional machine learning metrics like RMSE often fail to
capture the true clinical relevance of such predictions. We introduce novel
vital sign prediction performance metrics that align with clinical contexts,
focusing on deviations from clinical norms, overall trends, and trend
deviations. These metrics are derived from empirical utility curves obtained in
a previous study through interviews with ICU clinicians. We validate the
metrics' usefulness using simulated and real clinical datasets (MIMIC and
eICU). Furthermore, we employ these metrics as loss functions for neural
networks, resulting in models that excel in predicting clinically significant
events. This research paves the way for clinically relevant machine learning
model evaluation and optimization, promising to improve ICU patient care. 10
pages, 9 figures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 9 figures, AMIA Informatics 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Network-Based Piecewise Survival Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olov Holmer, Erik Frisk, Mattias Krysander
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, a family of neural network-based survival models is presented.
The models are specified based on piecewise definitions of the hazard function
and the density function on a partitioning of the time; both constant and
linear piecewise definitions are presented, resulting in a family of four
models. The models can be seen as an extension of the commonly used
discrete-time and piecewise exponential models and thereby add flexibility to
this set of standard models. Using a simulated dataset the models are shown to
perform well compared to the highly expressive, state-of-the-art energy-based
model, while only requiring a fraction of the computation time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transformers-based architectures for stroke segmentation: A <span class="highlight-title">review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yalda Zafari-Ghadim, Essam A. Rashed, Mohamed Mabrok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stroke remains a significant global health concern, necessitating precise and
efficient diagnostic tools for timely intervention and improved patient
outcomes. The emergence of deep learning methodologies has transformed the
landscape of medical image analysis. Recently, Transformers, initially designed
for natural language processing, have exhibited remarkable capabilities in
various computer vision applications, including medical image analysis. This
comprehensive review aims to provide an in-depth exploration of the
cutting-edge Transformer-based architectures applied in the context of stroke
segmentation. It commences with an exploration of stroke pathology, imaging
modalities, and the challenges associated with accurate diagnosis and
segmentation. Subsequently, the review delves into the fundamental ideas of
Transformers, offering detailed insights into their architectural intricacies
and the underlying mechanisms that empower them to effectively capture complex
spatial information within medical images. The existing literature is
systematically categorized and analyzed, discussing various approaches that
leverage Transformers for stroke segmentation. A critical assessment is
provided, highlighting the strengths and limitations of these methods,
including considerations of performance and computational efficiency.
Additionally, this review explores potential avenues for future research and
development
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fusion approaches for emotion recognition from speech using acoustic and
  text-based features <span class="chip">ICASSP 2020</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonardo Pepino, Pablo Riera, Luciana Ferrer, Agustin Gravano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study different approaches for classifying emotions from
speech using acoustic and text-based features. We propose to obtain
contextualized word embeddings with BERT to represent the information contained
in speech transcriptions and show that this results in better performance than
using Glove embeddings. We also propose and compare different strategies to
combine the audio and text modalities, evaluating them on IEMOCAP and
MSP-PODCAST datasets. We find that fusing acoustic and text-based systems is
beneficial on both datasets, though only subtle differences are observed across
the evaluated fusion approaches. Finally, for IEMOCAP, we show the large effect
that the criteria used to define the cross-validation folds have on results. In
particular, the standard way of creating folds for this dataset results in a
highly optimistic estimation of performance for the text-based system,
suggesting that some previous works may overestimate the advantage of
incorporating transcriptions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages. Accepted in ICASSP 2020</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ First Experiences with the Identification of People at Risk for Diabetes
  in Argentina using Machine Learning Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18631v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18631v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enzo Rucci, Gonzalo Tittarelli, Franco Ronchetti, Jorge F. Elgart, Laura Lanzarini, Juan José Gagliardino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting Type 2 Diabetes (T2D) and Prediabetes (PD) is a real challenge for
medicine due to the absence of pathogenic symptoms and the lack of known
associated risk factors. Even though some proposals for machine learning models
enable the identification of people at risk, the nature of the condition makes
it so that a model suitable for one population may not necessarily be suitable
for another. In this article, the development and assessment of predictive
models to identify people at risk for T2D and PD specifically in Argentina are
discussed. First, the database was thoroughly preprocessed and three specific
datasets were generated considering a compromise between the number of records
and the amount of available variables. After applying 5 different
classification models, the results obtained show that a very good performance
was observed for two datasets with some of these models. In particular, RF, DT,
and ANN demonstrated great classification power, with good values for the
metrics under consideration. Given the lack of this type of tool in Argentina,
this work represents the first step towards the development of more
sophisticated models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in Computer Science - CACIC 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable Lipschitz Estimation for CNNs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusuf Sulehman, Tingting Mu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating the Lipschitz constant of deep neural networks is of growing
interest as it is useful for informing on generalisability and adversarial
robustness. Convolutional neural networks (CNNs) in particular, underpin much
of the recent success in computer vision related applications. However,
although existing methods for estimating the Lipschitz constant can be tight,
they have limited scalability when applied to CNNs. To tackle this, we propose
a novel method to accelerate Lipschitz constant estimation for CNNs. The core
idea is to divide a large convolutional block via a joint layer and width-wise
partition, into a collection of smaller blocks. We prove an upper-bound on the
Lipschitz constant of the larger block in terms of the Lipschitz constants of
the smaller blocks. Through varying the partition factor, the resulting method
can be adjusted to prioritise either accuracy or scalability and permits
parallelisation. We demonstrate an enhanced scalability and comparable accuracy
to existing baselines through a range of experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Heterogeneous Peridynamic Neural Operators: Discover Biotissue
  Constitutive Law and Microstructure From Digital Image Correlation
  Measurements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18597v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18597v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siavash Jafarzadeh, Stewart Silling, Lu Zhang, Colton Ross, Chung-Hao Lee, S. M. Rakibur Rahman, Shuodao Wang, Yue Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human tissues are highly organized structures with specific collagen fiber
arrangements varying from point to point. The effects of such heterogeneity
play an important role for tissue function, and hence it is of critical to
discover and understand the distribution of such fiber orientations from
experimental measurements, such as the digital image correlation data. To this
end, we introduce the heterogeneous peridynamic neural operator (HeteroPNO)
approach, for data-driven constitutive modeling of heterogeneous anisotropic
materials. The goal is to learn both a nonlocal constitutive law together with
the material microstructure, in the form of a heterogeneous fiber orientation
field, from loading field-displacement field measurements. To this end, we
propose a two-phase learning approach. Firstly, we learn a homogeneous
constitutive law in the form of a neural network-based kernel function and a
nonlocal bond force, to capture complex homogeneous material responses from
data. Then, in the second phase we reinitialize the learnt bond force and the
kernel function, and training them together with a fiber orientation field for
each material point. Owing to the state-based peridynamic skeleton, our
HeteroPNO-learned material models are objective and have the balance of linear
and angular momentum guaranteed. Moreover, the effects from heterogeneity and
nonlinear constitutive relationship are captured by the kernel function and the
bond force respectively, enabling physical interpretability. As a result, our
HeteroPNO architecture can learn a constitutive model for a biological tissue
with anisotropic heterogeneous response undergoing large deformation regime.
Moreover, the framework is capable to provide displacement and stress field
predictions for new and unseen loading instances.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Impact of Uniform Inputs on Activation Sparsity and Energy-Latency
  Attacks in Computer Vision <span class="chip">SP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18587v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18587v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Müller, Erwin Quiring
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Resource efficiency plays an important role for machine learning nowadays.
The energy and decision latency are two critical aspects to ensure a
sustainable and practical application. Unfortunately, the energy consumption
and decision latency are not robust against adversaries. Researchers have
recently demonstrated that attackers can compute and submit so-called sponge
examples at inference time to increase the energy consumption and decision
latency of neural networks. In computer vision, the proposed strategy crafts
inputs with less activation sparsity which could otherwise be used to
accelerate the computation. In this paper, we analyze the mechanism how these
energy-latency attacks reduce activation sparsity. In particular, we find that
input uniformity is a key enabler. A uniform image, that is, an image with
mostly flat, uniformly colored surfaces, triggers more activations due to a
specific interplay of convolution, batch normalization, and ReLU activation.
Based on these insights, we propose two new simple, yet effective strategies
for crafting sponge examples: sampling images from a probability distribution
and identifying dense, yet inconspicuous inputs in natural datasets. We
empirically examine our findings in a comprehensive evaluation with multiple
image classification models and show that our attack achieves the same sparsity
effect as prior sponge-example methods, but at a fraction of computation
effort. We also show that our sponge examples transfer between different neural
networks. Finally, we discuss applications of our findings for the good by
improving efficiency by increasing sparsity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the DLSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One flow to correct them all: improving simulations in high-energy
  physics with a single normalising flow and a switch 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Caio Cesar Daumann, Mauro Donega, Johannes Erdmann, Massimiliano Galli, Jan Lukas Späh, Davide Valsecchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simulated events are key ingredients in almost all high-energy physics
analyses. However, imperfections in the simulation can lead to sizeable
differences between the observed data and simulated events. The effects of such
mismodelling on relevant observables must be corrected either effectively via
scale factors, with weights or by modifying the distributions of the
observables and their correlations. We introduce a correction method that
transforms one multidimensional distribution (simulation) into another one
(data) using a simple architecture based on a single normalising flow with a
boolean condition. We demonstrate the effectiveness of the method on a
physics-inspired toy dataset with non-trivial mismodelling of several
observables and their correlations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Optimizing Hyperparameters for Quantum Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18579v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18579v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sabrina Herbst, Vincenzo De Maio, Ivona Brandic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing capabilities of Machine Learning (ML) models go hand in hand
with an immense amount of data and computational power required for training.
Therefore, training is usually outsourced into HPC facilities, where we have
started to experience limits in scaling conventional HPC hardware, as theorized
by Moore's law. Despite heavy parallelization and optimization efforts, current
state-of-the-art ML models require weeks for training, which is associated with
an enormous $CO_2$ footprint. Quantum Computing, and specifically Quantum
Machine Learning (QML), can offer significant theoretical speed-ups and
enhanced expressive power. However, training QML models requires tuning various
hyperparameters, which is a nontrivial task and suboptimal choices can highly
affect the trainability and performance of the models. In this study, we
identify the most impactful hyperparameters and collect data about the
performance of QML models. We compare different configurations and provide
researchers with performance data and concrete suggestions for hyperparameter
selection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SteinGen: Generating Fidelitous and Diverse Graph <span class="highlight-title">Sample</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gesine Reinert, Wenkai Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating graphs that preserve characteristic structures while promoting
sample diversity can be challenging, especially when the number of graph
observations is small. Here, we tackle the problem of graph generation from
only one observed graph. The classical approach of graph generation from
parametric models relies on the estimation of parameters, which can be
inconsistent or expensive to compute due to intractable normalisation
constants. Generative modelling based on machine learning techniques to
generate high-quality graph samples avoids parameter estimation but usually
requires abundant training samples. Our proposed generating procedure,
SteinGen, which is phrased in the setting of graphs as realisations of
exponential random graph models, combines ideas from Stein's method and MCMC by
employing Markovian dynamics which are based on a Stein operator for the target
model. SteinGen uses the Glauber dynamics associated with an estimated Stein
operator to generate a sample, and re-estimates the Stein operator from the
sample after every sampling step. We show that on a class of exponential random
graph models this novel "estimation and re-estimation" generation strategy
yields high distributional similarity (high fidelity) to the original data,
combined with high sample diversity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-Informed Graph Neural Networks for Water Distribution Systems <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18570v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18570v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Inaam Ashraf, Janine Strotherm, Luca Hermes, Barbara Hammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Water distribution systems (WDS) are an integral part of critical
infrastructure which is pivotal to urban development. As 70% of the world's
population will likely live in urban environments in 2050, efficient simulation
and planning tools for WDS play a crucial role in reaching UN's sustainable
developmental goal (SDG) 6 - "Clean water and sanitation for all". In this
realm, we propose a novel and efficient machine learning emulator, more
precisely, a physics-informed deep learning (DL) model, for hydraulic state
estimation in WDS. Using a recursive approach, our model only needs a few graph
convolutional neural network (GCN) layers and employs an innovative algorithm
based on message passing. Unlike conventional machine learning tasks, the model
uses hydraulic principles to infer two additional hydraulic state features in
the process of reconstructing the available ground truth feature in an
unsupervised manner. To the best of our knowledge, this is the first DL
approach to emulate the popular hydraulic simulator EPANET, utilizing no
additional information. Like most DL models and unlike the hydraulic simulator,
our model demonstrates vastly faster emulation times that do not increase
drastically with the size of the WDS. Moreover, we achieve high accuracy on the
ground truth and very similar results compared to the hydraulic simulator as
demonstrated through experiments on five real-world WDS datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of the paper with the same title published at
  Proceedings of the AAAI Conference on Artificial Intelligence 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PDNNet: PDN-Aware GNN-CNN Heterogeneous Network for Dynamic IR Drop
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Zhao, Zhuomin Chai, Xun Jiang, Yibo Lin, Runsheng Wang, Ru Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  IR drop on the power delivery network (PDN) is closely related to PDN's
configuration and cell current consumption. As the integrated circuit (IC)
design is growing larger, dynamic IR drop simulation becomes computationally
unaffordable and machine learning based IR drop prediction has been explored as
a promising solution. Although CNN-based methods have been adapted to IR drop
prediction task in several works, the shortcomings of overlooking PDN
configuration is non-negligible. In this paper, we consider not only how to
properly represent cell-PDN relation, but also how to model IR drop following
its physical nature in the feature aggregation procedure. Thus, we propose a
novel graph structure, PDNGraph, to unify the representations of the PDN
structure and the fine-grained cell-PDN relation. We further propose a
dual-branch heterogeneous network, PDNNet, incorporating two parallel GNN-CNN
branches to favorably capture the above features during the learning process.
Several key designs are presented to make the dynamic IR drop prediction highly
effective and interpretable. We are the first work to apply graph structure to
deep-learning based dynamic IR drop prediction method. Experiments show that
PDNNet outperforms the state-of-the-art CNN-based methods by up to 39.3%
reduction in prediction error and achieves 545x speedup compared to the
commercial tool, which demonstrates the superiority of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Noise-Robust Keyword Spotting through Self-supervised Pretraining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacob Mørk, Holger Severin Bovbjerg, Gergely Kiss, Zheng-Hua Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Voice assistants are now widely available, and to activate them a keyword
spotting (KWS) algorithm is used. Modern KWS systems are mainly trained using
supervised learning methods and require a large amount of labelled data to
achieve a good performance. Leveraging unlabelled data through self-supervised
learning (SSL) has been shown to increase the accuracy in clean conditions.
This paper explores how SSL pretraining such as Data2Vec can be used to enhance
the robustness of KWS models in noisy conditions, which is under-explored.
  Models of three different sizes are pretrained using different pretraining
approaches and then fine-tuned for KWS. These models are then tested and
compared to models trained using two baseline supervised learning methods, one
being standard training using clean data and the other one being multi-style
training (MTR). The results show that pretraining and fine-tuning on clean data
is superior to supervised learning on clean data across all testing conditions,
and superior to supervised MTR for testing conditions of SNR above 5 dB. This
indicates that pretraining alone can increase the model's robustness. Finally,
it is found that using noisy data for pretraining models, especially with the
Data2Vec-denoising approach, significantly enhances the robustness of KWS
models in noisy conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attention-aware semantic relevance predicting Chinese sentence reading 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18542v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18542v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, several influential computational models and metrics have
been proposed to predict how humans comprehend and process sentence. One
particularly promising approach is contextual semantic similarity. Inspired by
the attention algorithm in Transformer and human memory mechanisms, this study
proposes an ``attention-aware'' approach for computing contextual semantic
relevance. This new approach takes into account the different contributions of
contextual parts and the expectation effect, allowing it to incorporate
contextual information fully. The attention-aware approach also facilitates the
simulation of existing reading models and evaluate them. The resulting
``attention-aware'' metrics of semantic relevance can more accurately predict
fixation durations in Chinese reading tasks recorded in an eye-tracking corpus
than those calculated by existing approaches. The study's findings further
provide strong support for the presence of semantic preview benefits in Chinese
naturalistic reading. Furthermore, the attention-aware metrics of semantic
relevance, being memory-based, possess high interpretability from both
linguistic and cognitive standpoints, making them a valuable computational tool
for modeling eye-movements in reading and further gaining insight into the
process of language comprehension. Our approach underscores the potential of
these metrics to advance our comprehension of how humans understand and process
language, ultimately leading to a better understanding of language
comprehension and processing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ skscope: Fast Sparsity-Constrained Optimization in Python 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18540v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18540v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zezhi Wang, Jin Zhu, Peng Chen, Huiyang Peng, Xiaoke Zhang, Anran Wang, Yu Zheng, Junxian Zhu, Xueqin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Applying iterative solvers on sparsity-constrained optimization (SCO)
requires tedious mathematical deduction and careful programming/debugging that
hinders these solvers' broad impact. In the paper, the library skscope is
introduced to overcome such an obstacle. With skscope, users can solve the SCO
by just programming the objective function. The convenience of skscope is
demonstrated through two examples in the paper, where sparse linear regression
and trend filtering are addressed with just four lines of code. More
importantly, skscope's efficient implementation allows state-of-the-art solvers
to quickly attain the sparse solution regardless of the high dimensionality of
parameter space. Numerical experiments reveal the available solvers in skscope
can achieve up to 80x speedup on the competing relaxation solutions obtained
via the benchmarked convex solver. skscope is published on the Python Package
Index (PyPI) and Conda, and its source code is available at:
https://github.com/abess-team/skscope.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safe and Robust <span class="highlight-title">Reinforcement</span>-Learning: Principles and Practice 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taku Yamagata, Raul Santos-Rodriguez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) has shown remarkable success in solving
relatively complex tasks, yet the deployment of RL systems in real-world
scenarios poses significant challenges related to safety and robustness. This
paper aims to identify and further understand those challenges thorough the
exploration of the main dimensions of the safe and robust RL landscape,
encompassing algorithmic, ethical, and practical considerations. We conduct a
comprehensive review of methodologies and open problems that summarizes the
efforts in recent years to address the inherent risks associated with RL
applications.
  After discussing and proposing definitions for both safe and robust RL, the
paper categorizes existing research works into different algorithmic approaches
that enhance the safety and robustness of RL agents. We examine techniques such
as uncertainty estimation, optimisation methodologies, exploration-exploitation
trade-offs, and adversarial training. Environmental factors, including
sim-to-real transfer and domain adaptation, are also scrutinized to understand
how RL systems can adapt to diverse and dynamic surroundings. Moreover, human
involvement is an integral ingredient of the analysis, acknowledging the broad
set of roles that humans can take in this context.
  Importantly, to aid practitioners in navigating the complexities of safe and
robust RL implementation, this paper introduces a practical checklist derived
from the synthesized literature. The checklist encompasses critical aspects of
algorithm design, training environment considerations, and ethical guidelines.
It will serve as a resource for developers and policymakers alike to ensure the
responsible deployment of RL systems in many application domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Theoretical Bound-Guided Hierarchical VAE for Neural Image Codecs <span class="chip">ICME2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18535v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18535v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichi Zhang, Zhihao Duan, Yuning Huang, Fengqing Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies reveal a significant theoretical link between variational
autoencoders (VAEs) and rate-distortion theory, notably in utilizing VAEs to
estimate the theoretical upper bound of the information rate-distortion
function of images. Such estimated theoretical bounds substantially exceed the
performance of existing neural image codecs (NICs). To narrow this gap, we
propose a theoretical bound-guided hierarchical VAE (BG-VAE) for NIC. The
proposed BG-VAE leverages the theoretical bound to guide the NIC model towards
enhanced performance. We implement the BG-VAE using Hierarchical VAEs and
demonstrate its effectiveness through extensive experiments. Along with
advanced neural network blocks, we provide a versatile, variable-rate NIC that
outperforms existing methods when considering both rate-distortion performance
and computational complexity. The code is available at BG-VAE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2024 IEEE International Conference on Multimedia and Expo (ICME2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Plays a Pivotal Role in the Object-Attribute Compositional
  Generalization of CLIP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reza Abbasi, Mohammad Samiei, Mohammad Hossein Rohban, Mahdieh Soleymani Baghshah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models, such as CLIP, have shown promising
Out-of-Distribution (OoD) generalization under various types of distribution
shifts. Recent studies attempted to investigate the leading cause of this
capability. In this work, we follow the same path, but focus on a specific type
of OoD data - images with novel compositions of attribute-object pairs - and
study whether such models can successfully classify those images into
composition classes. We carefully designed an authentic image test dataset
called ImageNet-AO, consisting of attributes for objects that are unlikely
encountered in the CLIP training sets. We found that CLIPs trained with large
datasets such as OpenAI CLIP, LAION-400M, and LAION-2B show orders-of-magnitude
improvement in effective compositional OoD generalization compared to both
supervised models and CLIPs trained with smaller datasets, such as CC-12M and
YFCC-15M. Our results provide evidence that the scale and diversity of training
data and language supervision play a key role in unlocking the compositional
generalization abilities of vision-language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Oral accepted at OODCV 2023(http://www.ood-cv.org)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Line Search Methods for Large Scale Neural Network Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18519v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18519v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Kenneweg, Tristan Kenneweg, Barbara Hammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent studies, line search methods have shown significant improvements in
the performance of traditional stochastic gradient descent techniques,
eliminating the need for a specific learning rate schedule. In this paper, we
identify existing issues in state-of-the-art line search methods, propose
enhancements, and rigorously evaluate their effectiveness. We test these
methods on larger datasets and more complex data domains than before.
Specifically, we improve the Armijo line search by integrating the momentum
term from ADAM in its search direction, enabling efficient large-scale
training, a task that was previously prone to failure using Armijo line search
methods. Our optimization approach outperforms both the previous Armijo
implementation and tuned learning rate schedules for Adam. Our evaluation
focuses on Transformers and CNNs in the domains of NLP and image data. Our work
is publicly available as a Python package, which provides a hyperparameter free
Pytorch optimizer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Algorithms for Regularized Nonnegative Scale-invariant
  Low-rank Approximation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18517v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18517v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeremy E. Cohen, Valentin Leplat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Regularized nonnegative low-rank approximations such as sparse Nonnegative
Matrix Factorization or sparse Nonnegative Tucker Decomposition are an
important branch of dimensionality reduction models with enhanced
interpretability. However, from a practical perspective, the choice of
regularizers and regularization coefficients, as well as the design of
efficient algorithms, is challenging because of the multifactor nature of these
models and the lack of theory to back these choices. This paper aims at
improving upon these issues. By studying a more general model called the
Homogeneous Regularized Scale-Invariant, we prove that the scale-invariance
inherent to low-rank approximation models causes an implicit regularization
with both unexpected beneficial and detrimental effects. This observation
allows to better understand the effect of regularization functions in low-rank
approximation models, to guide the choice of the regularization
hyperparameters, and to design balancing strategies to enhance the convergence
speed of dedicated optimization algorithms. Some of these results were already
known but restricted to specific instances of regularized low-rank
approximations. We also derive a generic Majorization Minimization algorithm
that handles many regularized nonnegative low-rank approximations, with
convergence guarantees. We showcase our contributions on sparse Nonnegative
Matrix Factorization, ridge-regularized Canonical Polyadic decomposition and
sparse Nonnegative Tucker Decomposition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CT-3DFlow : Leveraging 3D Normalizing Flows for Unsupervised Detection
  of Pathological Pulmonary CT scans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18514v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18514v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aissam Djahnine, Alexandre Popoff, Emilien Jupin-Delevaux, Vincent Cottin, Olivier Nempont, Loic Boussel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised pathology detection can be implemented by training a model on
healthy data only and measuring the deviation from the training set upon
inference, for example with CNN-based feature extraction and one-class
classifiers, or reconstruction-score-based methods such as AEs, GANs and
Diffusion models. Normalizing Flows (NF) have the ability to directly learn the
probability distribution of training examples through an invertible
architecture. We leverage this property in a novel 3D NF-based model named
CT-3DFlow, specifically tailored for patient-level pulmonary pathology
detection in chest CT data. Our model is trained unsupervised on healthy 3D
pulmonary CT patches, and detects deviations from its log-likelihood
distribution as anomalies. We aggregate patches-level likelihood values from a
patient's CT scan to provide a patient-level 'normal'/'abnormal' prediction.
Out-of-distribution detection performance is evaluated using expert annotations
on a separate chest CT test dataset, outperforming other state-of-the-art
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributed Maximum Consensus over Noisy Links 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18509v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18509v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ehsan Lari, Reza Arablouei, Naveen K. D. Venkategowda, Stefan Werner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a distributed algorithm, termed noise-robust distributed maximum
consensus (RD-MC), for estimating the maximum value within a multi-agent
network in the presence of noisy communication links. Our approach entails
redefining the maximum consensus problem as a distributed optimization problem,
allowing a solution using the alternating direction method of multipliers.
Unlike existing algorithms that rely on multiple sets of noise-corrupted
estimates, RD-MC employs a single set, enhancing both robustness and
efficiency. To further mitigate the effects of link noise and improve
robustness, we apply moving averaging to the local estimates. Through extensive
simulations, we demonstrate that RD-MC is significantly more robust to
communication link noise compared to existing maximum-consensus algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 7 figures, submitted to EUSIPCO 2024 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Faster Convergence for Transformer Fine-tuning with Line Search Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Kenneweg, Leonardo Galli, Tristan Kenneweg, Barbara Hammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works have shown that line search methods greatly increase performance
of traditional stochastic gradient descent methods on a variety of datasets and
architectures [1], [2]. In this work we succeed in extending line search
methods to the novel and highly popular Transformer architecture and dataset
domains in natural language processing. More specifically, we combine the
Armijo line search with the Adam optimizer and extend it by subdividing the
networks architecture into sensible units and perform the line search
separately on these local units. Our optimization method outperforms the
traditional Adam optimizer and achieves significant performance improvements
for small data sets or small training budgets, while performing equal or better
for other tested cases. Our work is publicly available as a python package,
which provides a hyperparameter-free pytorch optimizer that is compatible with
arbitrary network architectures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Direct mineral content prediction from drill core images via transfer
  learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18495v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18495v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Romana Boiger, Sergey V. Churakov, Ignacio Ballester Llagaria, Georg Kosakowski, Raphael Wüst, Nikolaos I. Prasianakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep subsurface exploration is important for mining, oil and gas industries,
as well as in the assessment of geological units for the disposal of chemical
or nuclear waste, or the viability of geothermal energy systems. Typically,
detailed examinations of subsurface formations or units are performed on
cuttings or core materials extracted during drilling campaigns, as well as on
geophysical borehole data, which provide detailed information about the
petrophysical properties of the rocks. Depending on the volume of rock samples
and the analytical program, the laboratory analysis and diagnostics can be very
time-consuming. This study investigates the potential of utilizing machine
learning, specifically convolutional neural networks (CNN), to assess the
lithology and mineral content solely from analysis of drill core images, aiming
to support and expedite the subsurface geological exploration. The paper
outlines a comprehensive methodology, encompassing data preprocessing, machine
learning methods, and transfer learning techniques. The outcome reveals a
remarkable 96.7% accuracy in the classification of drill core segments into
distinct formation classes. Furthermore, a CNN model was trained for the
evaluation of mineral content using a learning data set from multidimensional
log analysis data (silicate, total clay, carbonate). When benchmarked against
laboratory XRD measurements on samples from the cores, both the advanced
multidimensional log analysis model and the neural network approach developed
here provide equally good performance. This work demonstrates that deep
learning and particularly transfer learning can support extracting
petrophysical properties, including mineral content and formation
classification, from drill core images, thus offering a road map for enhancing
model performance and data set quality in image-based analysis of drill cores.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning in PINNs: Phase transition, total diffusion, and generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18494v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18494v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sokratis J. Anagnostopoulos, Juan Diego Toscano, Nikolaos Stergiopulos, George Em Karniadakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the learning dynamics of fully-connected neural networks
through the lens of gradient signal-to-noise ratio (SNR), examining the
behavior of first-order optimizers like Adam in non-convex objectives. By
interpreting the drift/diffusion phases in the information bottleneck theory,
focusing on gradient homogeneity, we identify a third phase termed ``total
diffusion", characterized by equilibrium in the learning rates and homogeneous
gradients. This phase is marked by an abrupt SNR increase, uniform residuals
across the sample space and the most rapid training convergence. We propose a
residual-based re-weighting scheme to accelerate this diffusion in quadratic
loss functions, enhancing generalization. We also explore the information
compression phenomenon, pinpointing a significant saturation-induced
compression of activations at the total diffusion phase, with deeper layers
experiencing negligible information loss. Supported by experimental data on
physics-informed neural networks (PINNs), which underscore the importance of
gradient homogeneity due to their PDE-based sample inter-dependence, our
findings suggest that recognizing phase transitions could refine ML
optimization strategies for improved generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Impact of Employing Weather Forecast Data as Input to the Estimation of
  Evapotranspiration by Deep Neural Network Models <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18489v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18489v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro J. Vaz, Gabriela Schütz, Carlos Guerrero, Pedro J. S. Cardoso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reference Evapotranspiration (ET0) is a key parameter for designing smart
irrigation scheduling, since it is related by a coefficient to the water needs
of a crop. The United Nations Food and Agriculture Organization, proposed a
standard method for ET0 computation (FAO56PM), based on the parameterization of
the Penman-Monteith equation, that is widely adopted in the literature. To
compute ET0 using the FAO56-PM method, four main weather parameters are needed:
temperature, humidity, wind, and solar radiation (SR). One way to make daily
ET0 estimations for future days is to use freely available weather forecast
services (WFSs), where many meteorological parameters are estimated up to the
next 15 days. A problem with this method is that currently, SR is not provided
as a free forecast parameter on most of those online services or, normally,
such forecasts present a financial cost penalty. For this reason, several ET0
estimation models using machine and deep learning were developed and presented
in the literature, that use as input features a reduced set of carefully
selected weather parameters, that are compatible with common freely available
WFSs. However, most studies on this topic have only evaluated model performance
using data from weather stations (WSs), without considering the effect of using
weather forecast data. In this study, the performance of authors' previous
models is evaluated when using weather forecast data from two online WFSs, in
the following scenarios: (i) direct ET0 estimation by an ANN model, and (ii)
estimate SR by ANN model, and then use that estimation for ET0 computation,
using the FAO56-PM method. Employing data collected from two WFSs and a WS
located in Vale do Lobo, Portugal, the latter approach achieved the best
result, with a coefficient of determination (R2) ranging between 0.893 and
0.667, when considering forecasts up to 15 days.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A partial version of the work submitted to ESRE/INTERNATIONAL
  CONFERENCE ON ENVIRONMENTAL SCIENCES AND RENEWABLE ENERGY</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthesizing EEG Signals from Event-Related Potential Paradigms with
  Conditional Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guido Klein, Pierre Guetschel, Gianluigi Silvestri, Michael Tangermann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data scarcity in the brain-computer interface field can be alleviated through
the use of generative models, specifically diffusion models. While diffusion
models have previously been successfully applied to electroencephalogram (EEG)
data, existing models lack flexibility w.r.t.~sampling or require alternative
representations of the EEG data. To overcome these limitations, we introduce a
novel approach to conditional diffusion models that utilizes classifier-free
guidance to directly generate subject-, session-, and class-specific EEG data.
In addition to commonly used metrics, domain-specific metrics are employed to
evaluate the specificity of the generated samples. The results indicate that
the proposed model can generate EEG data that resembles real data for each
subject, session, and class.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to 9th Graz BCI conference, 6 pages, 3 figures, first
  figure is split into two subfigures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SingularTrajectory: Universal Trajectory Predictor Using Diffusion Model <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18452v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18452v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Inhwan Bae, Young-Jae Park, Hae-Gon Jeon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are five types of trajectory prediction tasks: deterministic,
stochastic, domain adaptation, momentary observation, and few-shot. These
associated tasks are defined by various factors, such as the length of input
paths, data split and pre-processing methods. Interestingly, even though they
commonly take sequential coordinates of observations as input and infer future
paths in the same coordinates as output, designing specialized architectures
for each task is still necessary. For the other task, generality issues can
lead to sub-optimal performances. In this paper, we propose SingularTrajectory,
a diffusion-based universal trajectory prediction framework to reduce the
performance gap across the five tasks. The core of SingularTrajectory is to
unify a variety of human dynamics representations on the associated tasks. To
do this, we first build a Singular space to project all types of motion
patterns from each task into one embedding space. We next propose an adaptive
anchor working in the Singular space. Unlike traditional fixed anchor methods
that sometimes yield unacceptable paths, our adaptive anchor enables correct
anchors, which are put into a wrong location, based on a traversability map.
Finally, we adopt a diffusion-based predictor to further enhance the prototype
paths using a cascaded denoising process. Our unified framework ensures the
generality across various benchmark settings such as input modality, and
trajectory lengths. Extensive experiments on five public benchmarks demonstrate
that SingularTrajectory substantially outperforms existing models, highlighting
its effectiveness in estimating general dynamics of human movements. Code is
publicly available at https://github.com/inhwanbae/SingularTrajectory .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoRAST: Towards Foundation Model-Powered Correlated Data Analysis in
  Resource-Constrained CPS and IoT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Hu, Jinhang Zuo, Alanis Zhao, Bob Iannucci, Carlee Joe-Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models (FMs) emerge as a promising solution to harness distributed
and diverse environmental data by leveraging prior knowledge to understand the
complicated temporal and spatial correlations within heterogeneous datasets.
Unlike distributed learning frameworks such as federated learning, which often
struggle with multimodal data, FMs can transform diverse inputs into
embeddings. This process facilitates the integration of information from
various modalities and the application of prior learning to new domains.
However, deploying FMs in resource-constrained edge systems poses significant
challenges. To this end, we introduce CoRAST, a novel learning framework that
utilizes FMs for enhanced analysis of distributed, correlated heterogeneous
data. Utilizing a server-based FM, CoRAST can exploit existing environment
information to extract temporal, spatial, and cross-modal correlations among
sensor data. This enables CoRAST to offer context-aware insights for localized
client tasks through FM-powered global representation learning. Our evaluation
on real-world weather dataset demonstrates CoRAST's ability to exploit
correlated heterogeneous data through environmental representation learning to
reduce the forecast errors by up to 50.3% compared to the baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted and to be published in 2024 IEEE International Workshop on
  Foundation Models for Cyber-Physical Systems & Internet of Things (FMSys)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Language Beat Numerical Regression? Language-Based Multimodal
  Trajectory Prediction <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Inhwan Bae, Junoh Lee, Hae-Gon Jeon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models have demonstrated impressive ability in context understanding
and generative performance. Inspired by the recent success of language
foundation models, in this paper, we propose LMTraj (Language-based Multimodal
Trajectory predictor), which recasts the trajectory prediction task into a sort
of question-answering problem. Departing from traditional numerical regression
models, which treat the trajectory coordinate sequence as continuous signals,
we consider them as discrete signals like text prompts. Specially, we first
transform an input space for the trajectory coordinate into the natural
language space. Here, the entire time-series trajectories of pedestrians are
converted into a text prompt, and scene images are described as text
information through image captioning. The transformed numerical and image data
are then wrapped into the question-answering template for use in a language
model. Next, to guide the language model in understanding and reasoning
high-level knowledge, such as scene context and social relationships between
pedestrians, we introduce an auxiliary multi-task question and answering. We
then train a numerical tokenizer with the prompt data. We encourage the
tokenizer to separate the integer and decimal parts well, and leverage it to
capture correlations between the consecutive numbers in the language model.
Lastly, we train the language model using the numerical tokenizer and all of
the question-answer prompts. Here, we propose a beam-search-based most-likely
prediction and a temperature-based multimodal prediction to implement both
deterministic and stochastic inferences. Applying our LMTraj, we show that the
language-based model can be a powerful pedestrian trajectory predictor, and
outperforms existing numerical-based predictor methods. Code is publicly
available at https://github.com/inhwanbae/LMTrajectory .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FRESCO: Federated <span class="highlight-title">Reinforcement</span> Energy System for Cooperative
  Optimization <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18444v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18444v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Mauricio Cuadrado, Roberto Alejandro Gutierrez, Martin Takáč
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise in renewable energy is creating new dynamics in the energy grid that
promise to create a cleaner and more participative energy grid, where
technology plays a crucial part in making the required flexibility to achieve
the vision of the next-generation grid. This work presents FRESCO, a framework
that aims to ease the implementation of energy markets using a hierarchical
control architecture of reinforcement learning agents trained using federated
learning. The core concept we are proving is that having greedy agents subject
to changing conditions from a higher level agent creates a cooperative setup
that will allow for fulfilling all the individual objectives. This paper
presents a general overview of the framework, the current progress, and some
insights we obtained from the recent results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tiny Paper at ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalized <span class="highlight-title">Policy</span> Learning for Smart Grids: FL TRPO Approach <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18439v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18439v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunxiang Li, Nicolas Mauricio Cuadrado, Samuel Horváth, Martin Takáč
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The smart grid domain requires bolstering the capabilities of existing energy
management systems; Federated Learning (FL) aligns with this goal as it
demonstrates a remarkable ability to train models on heterogeneous datasets
while maintaining data privacy, making it suitable for smart grid applications,
which often involve disparate data distributions and interdependencies among
features that hinder the suitability of linear models. This paper introduces a
framework that combines FL with a Trust Region Policy Optimization (FL TRPO)
aiming to reduce energy-associated emissions and costs. Our approach reveals
latent interconnections and employs personalized encoding methods to capture
unique insights, understanding the relationships between features and optimal
strategies, allowing our model to generalize to previously unseen data.
Experimental results validate the robustness of our approach, affirming its
proficiency in effectively learning policy models for smart grid challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024 Workshop: Tackling Climate Change with Machine Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Global Vegetation Modeling with Pre-Trained Weather Transformers <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18438v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18438v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pascal Janetzky, Florian Gallusser, Simon Hentschel, Andreas Hotho, Anna Krause
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate vegetation models can produce further insights into the complex
interaction between vegetation activity and ecosystem processes. Previous
research has established that long-term trends and short-term variability of
temperature and precipitation affect vegetation activity. Motivated by the
recent success of Transformer-based Deep Learning models for medium-range
weather forecasting, we adapt the publicly available pre-trained FourCastNet to
model vegetation activity while accounting for the short-term dynamics of
climate variability. We investigate how the learned global representation of
the atmosphere's state can be transferred to model the normalized difference
vegetation index (NDVI). Our model globally estimates vegetation activity at a
resolution of \SI{0.25}{\degree} while relying only on meteorological data. We
demonstrate that leveraging pre-trained weather models improves the NDVI
estimates compared to learning an NDVI model from scratch. Additionally, we
compare our results to other recent data-driven NDVI modeling approaches from
machine learning and ecology literature. We further provide experimental
evidence on how much data and training time is necessary to turn FourCastNet
into an effective vegetation model. Code and models will be made available upon
publication.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tackling Climate Change with Machine Learning Workshop @ ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collaborative Active Learning in Conditional Trust Environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18436v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18436v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zan-Kai Chong, Hiroyuki Ohsaki, Bryan Ng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate collaborative active learning, a paradigm in
which multiple collaborators explore a new domain by leveraging their combined
machine learning capabilities without disclosing their existing data and
models. Instead, the collaborators share prediction results from the new domain
and newly acquired labels. This collaboration offers several advantages: (a) it
addresses privacy and security concerns by eliminating the need for direct
model and data disclosure; (b) it enables the use of different data sources and
insights without direct data exchange; and (c) it promotes cost-effectiveness
and resource efficiency through shared labeling costs. To realize these
benefits, we introduce a collaborative active learning framework designed to
fulfill the aforementioned objectives. We validate the effectiveness of the
proposed framework through simulations. The results demonstrate that
collaboration leads to higher AUC scores compared to independent efforts,
highlighting the framework's ability to overcome the limitations of individual
models. These findings support the use of collaborative approaches in active
learning, emphasizing their potential to enhance outcomes through collective
expertise and shared resources. Our work provides a foundation for further
research on collaborative active learning and its practical applications in
various domains where data privacy, cost efficiency, and model performance are
critical considerations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 9 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ U-Sketch: An Efficient Approach for Sketch to Image Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilias Mitsouras, Eleftherios Tsonis, Paraskevi Tzouveli, Athanasios Voulodimos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have demonstrated remarkable performance in text-to-image
synthesis, producing realistic and high resolution images that faithfully
adhere to the corresponding text-prompts. Despite their great success, they
still fall behind in sketch-to-image synthesis tasks, where in addition to
text-prompts, the spatial layout of the generated images has to closely follow
the outlines of certain reference sketches. Employing an MLP latent edge
predictor to guide the spatial layout of the synthesized image by predicting
edge maps at each denoising step has been recently proposed. Despite yielding
promising results, the pixel-wise operation of the MLP does not take into
account the spatial layout as a whole, and demands numerous denoising
iterations to produce satisfactory images, leading to time inefficiency. To
this end, we introduce U-Sketch, a framework featuring a U-Net type latent edge
predictor, which is capable of efficiently capturing both local and global
features, as well as spatial correlations between pixels. Moreover, we propose
the addition of a sketch simplification network that offers the user the choice
of preprocessing and simplifying input sketches for enhanced outputs. The
experimental results, corroborated by user feedback, demonstrate that our
proposed U-Net latent edge predictor leads to more realistic results, that are
better aligned with the spatial outlines of the reference sketches, while
drastically reducing the number of required denoising steps and, consequently,
the overall execution time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SemRoDe: Macro Adversarial Training to Learn Representations That are
  Robust to Word-Level Attacks <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18423v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18423v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian Formento, Wenjie Feng, Chuan Sheng Foo, Luu Anh Tuan, See-Kiong Ng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models (LMs) are indispensable tools for natural language processing
tasks, but their vulnerability to adversarial attacks remains a concern. While
current research has explored adversarial training techniques, their
improvements to defend against word-level attacks have been limited. In this
work, we propose a novel approach called Semantic Robust Defence (SemRoDe), a
Macro Adversarial Training strategy to enhance the robustness of LMs. Drawing
inspiration from recent studies in the image domain, we investigate and later
confirm that in a discrete data setting such as language, adversarial samples
generated via word substitutions do indeed belong to an adversarial domain
exhibiting a high Wasserstein distance from the base domain. Our method learns
a robust representation that bridges these two domains. We hypothesize that if
samples were not projected into an adversarial domain, but instead to a domain
with minimal shift, it would improve attack robustness. We align the domains by
incorporating a new distance-based objective. With this, our model is able to
learn more generalized representations by aligning the model's high-level
output features and therefore better handling unseen adversarial samples. This
method can be generalized across word embeddings, even when they share minimal
overlap at both vocabulary and word-substitution levels. To evaluate the
effectiveness of our approach, we conduct experiments on BERT and RoBERTa
models on three datasets. The results demonstrate promising state-of-the-art
robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in NAACL 2024 (Main Track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Topos of Transformer Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18415v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18415v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mattia Jacopo Villani, Peter McBurney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The transformer neural network has significantly out-shined all other neural
network architectures as the engine behind large language models. We provide a
theoretical analysis of the expressivity of the transformer architecture
through the lens of topos theory. From this viewpoint, we show that many common
neural network architectures, such as the convolutional, recurrent and graph
convolutional networks, can be embedded in a pretopos of piecewise-linear
functions, but that the transformer necessarily lives in its topos completion.
In particular, this suggests that the two network families instantiate
different fragments of logic: the former are first order, whereas transformers
are higher-order reasoners. Furthermore, we draw parallels with architecture
search and gradient descent, integrating our analysis in the framework of
cybernetic agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering
  Using a VLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18406v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18406v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonkyun Kim, Changin Choi, Wonseok Lee, Wonjong Rhee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stimulated by the sophisticated reasoning capabilities of recent Large
Language Models (LLMs), a variety of strategies for bridging video modality
have been devised. A prominent strategy involves Video Language Models
(VideoLMs), which train a learnable interface with video data to connect
advanced vision encoders with LLMs. Recently, an alternative strategy has
surfaced, employing readily available foundation models, such as VideoLMs and
LLMs, across multiple stages for modality bridging. In this study, we introduce
a simple yet novel strategy where only a single Vision Language Model (VLM) is
utilized. Our starting point is the plain insight that a video comprises a
series of images, or frames, interwoven with temporal information. The essence
of video comprehension lies in adeptly managing the temporal aspects along with
the spatial details of each frame. Initially, we transform a video into a
single composite image by arranging multiple frames in a grid layout. The
resulting single image is termed as an image grid. This format, while
maintaining the appearance of a solitary image, effectively retains temporal
information within the grid structure. Therefore, the image grid approach
enables direct application of a single high-performance VLM without
necessitating any video-data training. Our extensive experimental analysis
across ten zero-shot video question answering benchmarks, including five
open-ended and five multiple-choice benchmarks, reveals that the proposed Image
Grid Vision Language Model (IG-VLM) surpasses the existing methods in nine out
of ten benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code is available at https://github.com/imagegridworth/IG-VLM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Spectrogram Analysis in a Multiple Classifier Fusion Framework for
  Power Grid Classification Using Electric Network Frequency <span class="chip">ICPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18402v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18402v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgios Tzolopoulos, Christos Korgialas, Constantine Kotropoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Electric Network Frequency (ENF) serves as a unique signature inherent to
power distribution systems. Here, a novel approach for power grid
classification is developed, leveraging ENF. Spectrograms are generated from
audio and power recordings across different grids, revealing distinctive ENF
patterns that aid in grid classification through a fusion of classifiers. Four
traditional machine learning classifiers plus a Convolutional Neural Network
(CNN), optimized using Neural Architecture Search, are developed for One-vs-All
classification. This process generates numerous predictions per sample, which
are then compiled and used to train a shallow multi-label neural network
specifically designed to model the fusion process, ultimately leading to the
conclusive class prediction for each sample. Experimental findings reveal that
both validation and testing accuracy outperform those of current
state-of-the-art classifiers, underlining the effectiveness and robustness of
the proposed methodology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13th International Conference on Pattern Recognition Applications and
  Methods (ICPRAM)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Colour and Brush Stroke Pattern Recognition in Abstract Art using
  Modified Deep Convolutional Generative Adversarial Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18397v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18397v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Srinitish Srinivasan, Varenya Pathak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Abstract Art is an immensely popular, discussed form of art that often has
the ability to depict the emotions of an artist. Many researchers have made
attempts to study abstract art in the form of edge detection, brush stroke and
emotion recognition algorithms using machine and deep learning. This papers
describes the study of a wide distribution of abstract paintings using
Generative Adversarial Neural Networks(GAN). GANs have the ability to learn and
reproduce a distribution enabling researchers and scientists to effectively
explore and study the generated image space. However, the challenge lies in
developing an efficient GAN architecture that overcomes common training
pitfalls. This paper addresses this challenge by introducing a modified-DCGAN
(mDCGAN) specifically designed for high-quality artwork generation. The
approach involves a thorough exploration of the modifications made, delving
into the intricate workings of DCGANs, optimisation techniques, and
regularisation methods aimed at improving stability and realism in art
generation enabling effective study of generated patterns. The proposed mDCGAN
incorporates meticulous adjustments in layer configurations and architectural
choices, offering tailored solutions to the unique demands of art generation
while effectively combating issues like mode collapse and gradient vanishing.
Further this paper explores the generated latent space by performing random
walks to understand vector relationships between brush strokes and colours in
the abstract art space and a statistical analysis of unstable outputs after a
certain period of GAN training and compare its significant difference. These
findings validate the effectiveness of the proposed approach, emphasising its
potential to revolutionise the field of digital art generation and digital art
ecosystem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 5 tables, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tensor-based Graph Learning with Consistency and Specificity for
  Multi-view Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18393v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18393v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Shi, Lei Cao, Yunshan Ye, Yu Zhao, Badong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph learning is widely recognized as a crucial technique in multi-view
clustering. Existing graph learning methods typically involve constructing an
adaptive neighbor graph based on probabilistic neighbors and then learning a
consensus graph to for clustering, however, they are confronted with two
limitations. Firstly, they often rely on Euclidean distance to measure
similarity when constructing the adaptive neighbor graph, which proves
inadequate in capturing the intrinsic structure among data points in many
real-world scenarios. Secondly, most of these methods focus solely on consensus
graph, ignoring view-specific graph information. In response to the
aforementioned drawbacks, we in this paper propose a novel tensor-based graph
learning framework that simultaneously considers consistency and specificity
for multi-view clustering. Specifically, we calculate the similarity distance
on the Stiefel manifold to preserve the intrinsic structure among data points.
By making an assumption that the learned neighbor graph of each view comprises
both a consistent graph and a view-specific graph, we formulate a new
tensor-based target graph learning paradigm. Owing to the benefits of tensor
singular value decomposition (t-SVD) in uncovering high-order correlations,
this model is capable of achieving a complete understanding of the target
graph. Furthermore, we develop an iterative algorithm to solve the proposed
objective optimization problem. Experiments conducted on real-world datasets
have demonstrated the superior performance of the proposed method over some
state-of-the-art multi-view clustering methods. The source code has been
released on https://github.com/lshi91/CSTGL-Code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Multi-modal Models are Good Class-Incremental Learners <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18383v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18383v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xusheng Cao, Haori Lu, Linlan Huang, Xialei Liu, Ming-Ming Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In class-incremental learning (CIL) scenarios, the phenomenon of catastrophic
forgetting caused by the classifier's bias towards the current task has long
posed a significant challenge. It is mainly caused by the characteristic of
discriminative models. With the growing popularity of the generative
multi-modal models, we would explore replacing discriminative models with
generative ones for CIL. However, transitioning from discriminative to
generative models requires addressing two key challenges. The primary challenge
lies in transferring the generated textual information into the classification
of distinct categories. Additionally, it requires formulating the task of CIL
within a generative framework. To this end, we propose a novel generative
multi-modal model (GMM) framework for class-incremental learning. Our approach
directly generates labels for images using an adapted generative model. After
obtaining the detailed text, we use a text encoder to extract text features and
employ feature matching to determine the most similar label as the
classification prediction. In the conventional CIL settings, we achieve
significantly better results in long-sequence task scenarios. Under the
Few-shot CIL setting, we have improved by at least 14\% accuracy over all the
current state-of-the-art methods with significantly less forgetting. Our code
is available at \url{https://github.com/DoubleClass/GMM}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IIP-Mixer:Intra-Inter Patch Mixing Architecture for Battery Remaining
  Useful Life Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18379v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18379v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangzai Ye, Li Feng, Jianlan Guo, Yuqiang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately estimating the Remaining Useful Life (RUL) of lithium-ion
batteries is crucial for maintaining the safe and stable operation of
rechargeable battery management systems. However, this task is often
challenging due to the complex temporal dynamics involved. Recently,
attention-based networks, such as Transformers and Informer, have been the
popular architecture in time series forecasting. Despite their effectiveness,
these models with abundant parameters necessitate substantial training time to
unravel temporal patterns. To tackle these challenges, we propose a simple
MLP-Mixer-based architecture named 'Intra-Inter Patch Mixer' (IIP-Mixer), which
is an architecture based exclusively on multi-layer perceptrons (MLPs),
extracting information by mixing operations along both intra-patch and
inter-patch dimensions for battery RUL prediction. The proposed IIP-Mixer
comprises parallel dual-head mixer layers: the intra-patch mixing MLP,
capturing local temporal patterns in the short-term period, and the inter-patch
mixing MLP, capturing global temporal patterns in the long-term period.
Notably, to address the varying importance of features in RUL prediction, we
introduce a weighted loss function in the MLP-Mixer-based architecture, marking
the first time such an approach has been employed. Our experiments demonstrate
that IIP-Mixer achieves competitive performance in battery RUL prediction,
outperforming other popular time-series frameworks
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stragglers-Aware Low-Latency Synchronous Federated Learning via
  Layer-Wise Model Updates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18375v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18375v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Natalie Lang, Alejandro Cohen, Nir Shlezinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synchronous federated learning (FL) is a popular paradigm for collaborative
edge learning. It typically involves a set of heterogeneous devices locally
training neural network (NN) models in parallel with periodic centralized
aggregations. As some of the devices may have limited computational resources
and varying availability, FL latency is highly sensitive to stragglers.
Conventional approaches discard incomplete intra-model updates done by
stragglers, alter the amount of local workload and architecture, or resort to
asynchronous settings; which all affect the trained model performance under
tight training latency constraints. In this work, we propose straggler-aware
layer-wise federated learning (SALF) that leverages the optimization procedure
of NNs via backpropagation to update the global model in a layer-wise fashion.
SALF allows stragglers to synchronously convey partial gradients, having each
layer of the global model be updated independently with a different
contributing set of users. We provide a theoretical analysis, establishing
convergence guarantees for the global model under mild assumptions on the
distribution of the participating devices, revealing that SALF converges at the
same asymptotic rate as FL with no timing limitations. This insight is matched
with empirical observations, demonstrating the performance gains of SALF
compared to alternative mechanisms mitigating the device heterogeneity gap in
FL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ship in Sight: Diffusion Models for Ship-Image Super Resolution <span class="chip">IJCNN</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18370v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18370v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luigi Sigillo, Riccardo Fosco Gramaccioni, Alessandro Nicolosi, Danilo Comminiello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, remarkable advancements have been achieved in the field of
image generation, primarily driven by the escalating demand for high-quality
outcomes across various image generation subtasks, such as inpainting,
denoising, and super resolution. A major effort is devoted to exploring the
application of super-resolution techniques to enhance the quality of
low-resolution images. In this context, our method explores in depth the
problem of ship image super resolution, which is crucial for coastal and port
surveillance. We investigate the opportunity given by the growing interest in
text-to-image diffusion models, taking advantage of the prior knowledge that
such foundation models have already learned. In particular, we present a
diffusion-model-based architecture that leverages text conditioning during
training while being class-aware, to best preserve the crucial details of the
ships during the generation of the super-resoluted image. Since the specificity
of this task and the scarcity availability of off-the-shelf data, we also
introduce a large labeled ship dataset scraped from online ship images, mostly
from ShipSpotting\footnote{\url{www.shipspotting.com}} website. Our method
achieves more robust results than other deep learning models previously
employed for super resolution, as proven by the multiple experiments performed.
Moreover, we investigate how this model can benefit downstream tasks, such as
classification and object detection, thus emphasizing practical implementation
in a real-world scenario. Experimental results show flexibility, reliability,
and impressive performance of the proposed framework over state-of-the-art
methods for different tasks. The code is available at:
https://github.com/LuigiSigillo/ShipinSight .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 2024 International Joint Conference on Neural Networks
  (IJCNN)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intent-Aware DRL-Based Uplink Dynamic Scheduler for 5G-NR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18364v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18364v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Salwa Mostafa, Mateus P. Mota, Alvaro Valcarce, Mehdi Bennis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the problem of supporting Industrial Internet of Things user
equipment (IIoT UEs) with intent (i.e., requested quality of service (QoS)) and
random traffic arrival. A deep reinforcement learning (DRL) based centralized
dynamic scheduler for time-frequency resources is proposed to learn how to
schedule the available communication resources among the IIoT UEs. The proposed
scheduler leverages an RL framework to adapt to the dynamic changes in the
wireless communication system and traffic arrivals. Moreover, a graph-based
reduction scheme is proposed to reduce the state and action space of the RL
framework to allow fast convergence and a better learning strategy. Simulation
results demonstrate the effectiveness of the proposed intelligent scheduler in
guaranteeing the expressed intent of IIoT UEs compared to several traditional
scheduling schemes, such as round-robin, semi-static, and heuristic approaches.
The proposed scheduler also outperforms the contention-free and
contention-based schemes in maximizing the number of successfully computed
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Supervised Multiple Kernel Learning approaches for multi-omics data
  integration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18355v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18355v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mitja Briscik, Gabriele Tazza, Marie-Agnes Dillies, László Vidács, Sébastien Dejean
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in high-throughput technologies have originated an ever-increasing
availability of omics datasets. The integration of multiple heterogeneous data
sources is currently an issue for biology and bioinformatics. Multiple kernel
learning (MKL) has shown to be a flexible and valid approach to consider the
diverse nature of multi-omics inputs, despite being an underused tool in
genomic data mining.We provide novel MKL approaches based on different kernel
fusion strategies.To learn from the meta-kernel of input kernels, we
adaptedunsupervised integration algorithms for supervised tasks with support
vector machines.We also tested deep learning architectures for kernel fusion
and classification.The results show that MKL-based models can compete with more
complex, state-of-the-art, supervised multi-omics integrative approaches.
Multiple kernel learning offers a natural framework for predictive models in
multi-omics genomic data. Our results offer a direction for bio-data mining
research and further development of methods for heterogeneous data integration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating Diverse Agricultural Data for Vision-Based Farming
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mikolaj Cieslak, Umabharathi Govindarajan, Alejandro Garcia, Anuradha Chandrashekar, Torsten Hädrich, Aleksander Mendoza-Drosik, Dominik L. Michels, Sören Pirk, Chia-Chun Fu, Wojciech Pałubicki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a specialized procedural model for generating synthetic
agricultural scenes, focusing on soybean crops, along with various weeds. This
model is capable of simulating distinct growth stages of these plants, diverse
soil conditions, and randomized field arrangements under varying lighting
conditions. The integration of real-world textures and environmental factors
into the procedural generation process enhances the photorealism and
applicability of the synthetic data. Our dataset includes 12,000 images with
semantic labels, offering a comprehensive resource for computer vision tasks in
precision agriculture, such as semantic segmentation for autonomous weed
control. We validate our model's effectiveness by comparing the synthetic data
against real agricultural images, demonstrating its potential to significantly
augment training data for machine learning models in agriculture. This approach
not only provides a cost-effective solution for generating high-quality,
diverse data but also addresses specific needs in agricultural vision tasks
that are not fully covered by general-purpose models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Quantum Fuzzy-based Approach for Real-Time Detection of Solar Coronal
  Holes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanmoy Bandyopadhyay, Suman Kundu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The detection and analysis of the solar coronal holes (CHs) is an important
field of study in the domain of solar physics. Mainly, it is required for the
proper prediction of the geomagnetic storms which directly or indirectly affect
various space and ground-based systems. For the detection of CHs till date, the
solar scientist depends on manual hand-drawn approaches. However, with the
advancement of image processing technologies, some automated image segmentation
methods have been used for the detection of CHs. In-spite of this, fast and
accurate detection of CHs are till a major issues. Here in this work, a novel
quantum computing-based fast fuzzy c-mean technique has been developed for fast
detection of the CHs region. The task has been carried out in two stages, in
first stage the solar image has been segmented using a quantum computing based
fast fuzzy c-mean (QCFFCM) and in the later stage the CHs has been extracted
out from the segmented image based on image morphological operation. In the
work, quantum computing has been used to optimize the cost function of the fast
fuzzy c-mean (FFCM) algorithm, where quantum approximate optimization algorithm
(QAOA) has been used to optimize the quadratic part of the cost function. The
proposed method has been tested for 193 \AA{} SDO/AIA full-disk solar image
datasets and has been compared with the existing techniques. The outcome shows
the comparable performance of the proposed method with the existing one within
a very lesser time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Artificial Neural Twin -- Process Optimization and Continual
  Learning in Distributed Process Chains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18343v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18343v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johannes Emmert, Ronald Mendez, Houman Mirzaalian Dastjerdi, Christopher Syben, Andreas Maier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Industrial process optimization and control is crucial to increase economic
and ecologic efficiency. However, data sovereignty, differing goals, or the
required expert knowledge for implementation impede holistic implementation.
Further, the increasing use of data-driven AI-methods in process models and
industrial sensory often requires regular fine-tuning to accommodate
distribution drifts. We propose the Artificial Neural Twin, which combines
concepts from model predictive control, deep learning, and sensor networks to
address these issues. Our approach introduces differentiable data fusion to
estimate the state of distributed process steps and their dependence on input
data. By treating the interconnected process steps as a quasi neural-network,
we can backpropagate loss gradients for process optimization or model
fine-tuning to process parameters or AI models respectively. The concept is
demonstrated on a virtual machine park simulated in Unity, consisting of bulk
material processes in plastic recycling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Macroscale fracture surface segmentation via semi-supervised learning
  considering the structural similarity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johannes Rosenberger, Johannes Tlatlik, Sebastian Münstermann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To this date the safety assessment of materials, used for example in the
nuclear power sector, commonly relies on a fracture mechanical analysis
utilizing macroscopic concepts, where a global load quantity K or J is compared
to the materials fracture toughness curve. Part of the experimental effort
involved in these concepts is dedicated to the quantitative analysis of
fracture surfaces. Within the scope of this study a methodology for the
semi-supervised training of deep learning models for fracture surface
segmentation on a macroscopic level was established. Therefore, three distinct
and unique datasets were created to analyze the influence of structural
similarity on the segmentation capability. The structural similarity differs
due to the assessed materials and specimen, as well as imaging-induced variance
due to fluctuations in image acquisition in different laboratories. The
datasets correspond to typical isolated laboratory conditions, complex
real-world circumstances, and a curated subset of the two. We implemented a
weak-to-strong consistency regularization for semi-supervised learning. On the
heterogeneous dataset we were able to train robust and well-generalizing models
that learned feature representations from images across different domains
without observing a significant drop in prediction quality. Furthermore, our
approach reduced the number of labeled images required for training by a factor
of 6. To demonstrate the success of our method and the benefit of our approach
for the fracture mechanics assessment, we utilized the models for initial crack
size measurements with the area average method. For the laboratory setting, the
deep learning assisted measurements proved to have the same quality as manual
measurements. For models trained on the heterogeneous dataset, very good
measurement accuracies with mean deviations smaller than 1 % could be
achieved...
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>During review title changed to: Deep learning based initial crack
  size measurements utilizing macroscale fracture surface segmentation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Dataset</span> for Pharmacovigilance in German, French, and Japanese:
  Annotating Adverse Drug Reactions across Languages <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18336v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18336v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lisa Raithel, Hui-Syuan Yeh, Shuntaro Yada, Cyril Grouin, Thomas Lavergne, Aurélie Névéol, Patrick Paroubek, Philippe Thomas, Tomohiro Nishiyama, Sebastian Möller, Eiji Aramaki, Yuji Matsumoto, Roland Roller, Pierre Zweigenbaum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User-generated data sources have gained significance in uncovering Adverse
Drug Reactions (ADRs), with an increasing number of discussions occurring in
the digital world. However, the existing clinical corpora predominantly revolve
around scientific articles in English. This work presents a multilingual corpus
of texts concerning ADRs gathered from diverse sources, including patient fora,
social media, and clinical reports in German, French, and Japanese. Our corpus
contains annotations covering 12 entity types, four attribute types, and 13
relation types. It contributes to the development of real-world multilingual
language models for healthcare. We provide statistics to highlight certain
challenges associated with the corpus and conduct preliminary experiments
resulting in strong baselines for extracting entities and relations between
these entities, both within and across languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tracking-Assisted Object Detection with Event Cameras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ting-Kang Yen, Igor Morawski, Shusil Dangi, Kai He, Chung-Yi Lin, Jia-Fong Yeh, Hung-Ting Su, Winston Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event-based object detection has recently garnered attention in the computer
vision community due to the exceptional properties of event cameras, such as
high dynamic range and no motion blur. However, feature asynchronism and
sparsity cause invisible objects due to no relative motion to the camera,
posing a significant challenge in the task. Prior works have studied various
memory mechanisms to preserve as many features as possible at the current time,
guided by temporal clues. While these implicit-learned memories retain some
short-term information, they still struggle to preserve long-term features
effectively. In this paper, we consider those invisible objects as
pseudo-occluded objects and aim to reveal their features. Firstly, we introduce
visibility attribute of objects and contribute an auto-labeling algorithm to
append additional visibility labels on an existing event camera dataset.
Secondly, we exploit tracking strategies for pseudo-occluded objects to
maintain their permanence and retain their bounding boxes, even when features
have not been available for a very long time. These strategies can be treated
as an explicit-learned memory guided by the tracking objective to record the
displacements of objects across frames. Lastly, we propose a spatio-temporal
feature aggregation module to enrich the latent features and a consistency loss
to increase the robustness of the overall pipeline. We conduct comprehensive
experiments to verify our method's effectiveness where still objects are
retained but real occluded objects are discarded. The results demonstrate that
(1) the additional visibility labels can assist in supervised training, and (2)
our method outperforms state-of-the-art approaches with a significant
improvement of 7.9% absolute mAP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Privacy-Preserving Distributed Nonnegative Matrix Factorization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ehsan Lari, Reza Arablouei, Stefan Werner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nonnegative matrix factorization (NMF) is an effective data representation
tool with numerous applications in signal processing and machine learning.
However, deploying NMF in a decentralized manner over ad-hoc networks
introduces privacy concerns due to the conventional approach of sharing raw
data among network agents. To address this, we propose a privacy-preserving
algorithm for fully-distributed NMF that decomposes a distributed large data
matrix into left and right matrix factors while safeguarding each agent's local
data privacy. It facilitates collaborative estimation of the left matrix factor
among agents and enables them to estimate their respective right factors
without exposing raw data. To ensure data privacy, we secure information
exchanges between neighboring agents utilizing the Paillier cryptosystem, a
probabilistic asymmetric algorithm for public-key cryptography that allows
computations on encrypted data without decryption. Simulation results conducted
on synthetic and real-world datasets demonstrate the effectiveness of the
proposed algorithm in achieving privacy-preserving distributed NMF over ad-hoc
networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 figure, submitted to EUSIPCO 2024 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantum Algorithms: A New Frontier in Financial Crime Prevention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18322v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18322v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abraham Itzhak Weinberg, Alessio Faccia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Financial crimes fast proliferation and sophistication require novel
approaches that provide robust and effective solutions. This paper explores the
potential of quantum algorithms in combating financial crimes. It highlights
the advantages of quantum computing by examining traditional and Machine
Learning (ML) techniques alongside quantum approaches. The study showcases
advanced methodologies such as Quantum Machine Learning (QML) and Quantum
Artificial Intelligence (QAI) as powerful solutions for detecting and
preventing financial crimes, including money laundering, financial crime
detection, cryptocurrency attacks, and market manipulation. These quantum
approaches leverage the inherent computational capabilities of quantum
computers to overcome limitations faced by classical methods. Furthermore, the
paper illustrates how quantum computing can support enhanced financial risk
management analysis. Financial institutions can improve their ability to
identify and mitigate risks, leading to more robust risk management strategies
by exploiting the quantum advantage. This research underscores the
transformative impact of quantum algorithms on financial risk management. By
embracing quantum technologies, organisations can enhance their capabilities to
combat evolving threats and ensure the integrity and stability of financial
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Implementation of the Principal Component Analysis onto High-Performance
  Computer Facilities for Hyperspectral Dimensionality Reduction: Results and
  Comparisons 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        E. Martel, R. Lazcano, J. Lopez, D. Madroñal, R. Salvador, S. Lopez, E. Juarez, R. Guerra, C. Sanz, R. Sarmiento
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dimensionality reduction represents a critical preprocessing step in order to
increase the efficiency and the performance of many hyperspectral imaging
algorithms. However, dimensionality reduction algorithms, such as the Principal
Component Analysis (PCA), suffer from their computationally demanding nature,
becoming advisable for their implementation onto high-performance computer
architectures for applications under strict latency constraints. This work
presents the implementation of the PCA algorithm onto two different
high-performance devices, namely, an NVIDIA Graphics Processing Unit (GPU) and
a Kalray manycore, uncovering a highly valuable set of tips and tricks in order
to take full advantage of the inherent parallelism of these high-performance
computing platforms, and hence, reducing the time that is required to process a
given hyperspectral image. Moreover, the achieved results obtained with
different hyperspectral images have been compared with the ones that were
obtained with a field programmable gate array (FPGA)-based implementation of
the PCA algorithm that has been recently published, providing, for the first
time in the literature, a comprehensive analysis in order to highlight the pros
and cons of each option.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Modal Contrastive Learning for Online Clinical Time-Series
  Applications <span class="chip">ICLR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Baldenweg, Manuel Burger, Gunnar Rätsch, Rita Kuznetsova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electronic Health Record (EHR) datasets from Intensive Care Units (ICU)
contain a diverse set of data modalities. While prior works have successfully
leveraged multiple modalities in supervised settings, we apply advanced
self-supervised multi-modal contrastive learning techniques to ICU data,
specifically focusing on clinical notes and time-series for clinically relevant
online prediction tasks. We introduce a loss function Multi-Modal Neighborhood
Contrastive Loss (MM-NCL), a soft neighborhood function, and showcase the
excellent linear probe and zero-shot performance of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a Workshop Paper at TS4H@ICLR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A thermodynamically consistent physics-informed deep learning material
  model for short fiber/polymer nanocomposites 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18310v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18310v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Betim Bahtiri, Behrouz Arash, Sven Scheffler, Maximilian Jux, Raimund Rolfes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work proposes a physics-informed deep learning (PIDL)-based constitutive
model for investigating the viscoelastic-viscoplastic behavior of short
fiber-reinforced nanoparticle-filled epoxies under various ambient conditions.
The deep-learning model is trained to enforce thermodynamic principles, leading
to a thermodynamically consistent constitutive model. To accomplish this, a
long short-term memory network is combined with a feed-forward neural network
to predict internal variables required for characterizing the internal
dissipation of the nanocomposite materials. In addition, another feed-forward
neural network is used to indicate the free-energy function, which enables
defining the thermodynamic state of the entire system. The PIDL model is
initially developed for the three-dimensional case by generating synthetic data
from a classical constitutive model. The model is then trained by extracting
the data directly from cyclic loading-unloading experimental tests. Numerical
examples show that the PIDL model can accurately predict the mechanical
behavior of epoxy-based nanocomposites for different volume fractions of fibers
and nanoparticles under various hygrothermal conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2305.08102</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Super-Resolution of SOHO/MDI Magnetograms of Solar Active Regions Using
  SDO/HMI Data and an Attention-Aided Convolutional Neural Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18302v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18302v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunhui Xu, Jason T. L. Wang, Haimin Wang, Haodi Jiang, Qin Li, Yasser Abduallah, Yan Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image super-resolution has been an important subject in image processing and
recognition. Here, we present an attention-aided convolutional neural network
(CNN) for solar image super-resolution. Our method, named SolarCNN, aims to
enhance the quality of line-of-sight (LOS) magnetograms of solar active regions
(ARs) collected by the Michelson Doppler Imager (MDI) on board the Solar and
Heliospheric Observatory (SOHO). The ground-truth labels used for training
SolarCNN are the LOS magnetograms collected by the Helioseismic and Magnetic
Imager (HMI) on board the Solar Dynamics Observatory (SDO). Solar ARs consist
of strong magnetic fields in which magnetic energy can suddenly be released to
produce extreme space weather events, such as solar flares, coronal mass
ejections, and solar energetic particles. SOHO/MDI covers Solar Cycle 23, which
is stronger with more eruptive events than Cycle 24. Enhanced SOHO/MDI
magnetograms allow for better understanding and forecasting of violent events
of space weather. Experimental results show that SolarCNN improves the quality
of SOHO/MDI magnetograms in terms of the structural similarity index measure
(SSIM), Pearson's correlation coefficient (PCC), and the peak signal-to-noise
ratio (PSNR).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Selective Mixup Fine-Tuning for Optimizing Non-Decomposable Objectives <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shrinivas Ramasubramanian, Harsh Rangwani, Sho Takemori, Kunal Samanta, Yuhei Umeda, Venkatesh Babu Radhakrishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise in internet usage has led to the generation of massive amounts of
data, resulting in the adoption of various supervised and semi-supervised
machine learning algorithms, which can effectively utilize the colossal amount
of data to train models. However, before deploying these models in the real
world, these must be strictly evaluated on performance measures like worst-case
recall and satisfy constraints such as fairness. We find that current
state-of-the-art empirical techniques offer sub-optimal performance on these
practical, non-decomposable performance objectives. On the other hand, the
theoretical techniques necessitate training a new model from scratch for each
performance objective. To bridge the gap, we propose SelMix, a selective
mixup-based inexpensive fine-tuning technique for pre-trained models, to
optimize for the desired objective. The core idea of our framework is to
determine a sampling distribution to perform a mixup of features between
samples from particular classes such that it optimizes the given objective. We
comprehensively evaluate our technique against the existing empirical and
theoretically principled methods on standard benchmark datasets for imbalanced
classification. We find that proposed SelMix fine-tuning significantly improves
the performance for various practical non-decomposable objectives across
benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024 SpotLight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GeNet: A Graph Neural Network-based Anti-noise Task-Oriented Semantic
  Communication Paradigm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18296v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18296v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunhang Zheng, Kechao Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional approaches to semantic communication tasks rely on the knowledge
of the signal-to-noise ratio (SNR) to mitigate channel noise. However, these
methods necessitate training under specific SNR conditions, entailing
considerable time and computational resources. In this paper, we propose GeNet,
a Graph Neural Network (GNN)-based paradigm for semantic communication aimed at
combating noise, thereby facilitating Task-Oriented Communication (TOC). We
propose a novel approach where we first transform the input data image into
graph structures. Then we leverage a GNN-based encoder to extract semantic
information from the source data. This extracted semantic information is then
transmitted through the channel. At the receiver's end, a GNN-based decoder is
utilized to reconstruct the relevant semantic information from the source data
for TOC. Through experimental evaluation, we show GeNet's effectiveness in
anti-noise TOC while decoupling the SNR dependency. We further evaluate GeNet's
performance by varying the number of nodes, revealing its versatility as a new
paradigm for semantic communication. Additionally, we show GeNet's robustness
to geometric transformations by testing it with different rotation angles,
without resorting to data augmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few-Shot Recalibration of Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Lisa Li, Urvashi Khandelwal, Kelvin Guu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has uncovered promising ways to extract well-calibrated
confidence estimates from language models (LMs), where the model's confidence
score reflects how likely it is to be correct. However, while LMs may appear
well-calibrated over broad distributions, this often hides significant
miscalibration within narrower slices (e.g., systemic over-confidence in math
can balance out systemic under-confidence in history, yielding perfect
calibration in aggregate). To attain well-calibrated confidence estimates for
any slice of a distribution, we propose a new framework for few-shot
slice-specific recalibration. Specifically, we train a recalibration model that
takes in a few unlabeled examples from any given slice and predicts a curve
that remaps confidence scores to be more accurate for that slice. Our trained
model can recalibrate for arbitrary new slices, without using any labeled data
from that slice. This enables us to identify domain-specific confidence
thresholds above which the LM's predictions can be trusted, and below which it
should abstain. Experiments show that our few-shot recalibrator consistently
outperforms existing calibration methods, for instance improving calibration
error for PaLM2-Large on MMLU by 16%, as compared to temperature scaling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Clustering Change Sign Detection by Fusing Mixture Complexity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18269v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18269v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kento Urano, Ryo Yuki, Kenji Yamanishi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes an early detection method for cluster structural changes.
Cluster structure refers to discrete structural characteristics, such as the
number of clusters, when data are represented using finite mixture models, such
as Gaussian mixture models. We focused on scenarios in which the cluster
structure gradually changed over time. For finite mixture models, the concept
of mixture complexity (MC) measures the continuous cluster size by considering
the cluster proportion bias and overlap between clusters. In this paper, we
propose MC fusion as an extension of MC to handle situations in which multiple
mixture numbers are possible in a finite mixture model. By incorporating the
fusion of multiple models, our approach accurately captured the cluster
structure during transitional periods of gradual change. Moreover, we introduce
a method for detecting changes in the cluster structure by examining the
transition of MC fusion. We demonstrate the effectiveness of our method through
empirical analysis using both artificial and real-world datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DSF-GAN: DownStream Feedback Generative Adversarial Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oriel Perets, Nadav Rappoport
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Utility and privacy are two crucial measurements of the quality of synthetic
tabular data. While significant advancements have been made in privacy
measures, generating synthetic samples with high utility remains challenging.
To enhance the utility of synthetic samples, we propose a novel architecture
called the DownStream Feedback Generative Adversarial Network (DSF-GAN). This
approach incorporates feedback from a downstream prediction model during
training to augment the generator's loss function with valuable information.
Thus, DSF-GAN utilizes a downstream prediction task to enhance the utility of
synthetic samples. To evaluate our method, we tested it using two popular
datasets. Our experiments demonstrate improved model performance when training
on synthetic samples generated by DSF-GAN, compared to those generated by the
same GAN architecture without feedback. The evaluation was conducted on the
same validation set comprising real samples. All code and datasets used in this
research will be made openly available for ease of reproduction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Branch-Tuning: Balancing Stability and Plasticity for Continual
  Self-Supervised Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18266v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18266v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenzhuo Liu, Fei Zhu, Cheng-Lin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) has emerged as an effective paradigm for
deriving general representations from vast amounts of unlabeled data. However,
as real-world applications continually integrate new content, the high
computational and resource demands of SSL necessitate continual learning rather
than complete retraining. This poses a challenge in striking a balance between
stability and plasticity when adapting to new information. In this paper, we
employ Centered Kernel Alignment for quantitatively analyzing model stability
and plasticity, revealing the critical roles of batch normalization layers for
stability and convolutional layers for plasticity. Motivated by this, we
propose Branch-tuning, an efficient and straightforward method that achieves a
balance between stability and plasticity in continual SSL. Branch-tuning
consists of branch expansion and compression, and can be easily applied to
various SSL methods without the need of modifying the original methods,
retaining old data or models. We validate our method through incremental
experiments on various benchmark datasets, demonstrating its effectiveness and
practical value in real-world scenarios. We hope our work offers new insights
for future continual self-supervised learning research. The code will be made
publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18252v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18252v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwu Zhong, Zi-Yuan Hu, Michael R. Lyu, Liwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual representation learning has been a cornerstone in computer vision,
evolving from supervised learning with human-annotated labels to aligning
image-text pairs from the Internet. Despite recent advancements in multi-modal
large language models (MLLMs), the visual representations they rely on, such as
CLIP embeddings, often lack access to external world knowledge critical for
real-world visual reasoning. In this work, we propose Visual Table, a novel
visual representation tailored for MLLMs. It provides hierarchical text
descriptions of holistic visual scenes, consisting of a scene description and
multiple object-centric descriptions that encompass categories, attributes, and
knowledge at instance level. We further develop a scalable generator for visual
table generation and train it on small-scale annotations from GPT4V. Extensive
evaluations demonstrate that, with generated visual tables as additional visual
representations, our model can consistently outperform the state-of-the-art
(SOTA) MLLMs across diverse benchmarks. When visual tables serve as standalone
visual representations, our model can closely match or even beat the SOTA MLLMs
that are built on CLIP visual embeddings. Our code is available at
https://github.com/LaVi-Lab/Visual-Table.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://github.com/LaVi-Lab/Visual-Table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeuSDFusion: A Spatial-Aware Generative Model for 3D Shape Completion,
  Reconstruction, and Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18241v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18241v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruikai Cui, Weizhe Liu, Weixuan Sun, Senbo Wang, Taizhang Shang, Yang Li, Xibin Song, Han Yan, Zhennan Wu, Shenzhou Chen, Hongdong Li, Pan Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D shape generation aims to produce innovative 3D content adhering to
specific conditions and constraints. Existing methods often decompose 3D shapes
into a sequence of localized components, treating each element in isolation
without considering spatial consistency. As a result, these approaches exhibit
limited versatility in 3D data representation and shape generation, hindering
their ability to generate highly diverse 3D shapes that comply with the
specified constraints. In this paper, we introduce a novel spatial-aware 3D
shape generation framework that leverages 2D plane representations for enhanced
3D shape modeling. To ensure spatial coherence and reduce memory usage, we
incorporate a hybrid shape representation technique that directly learns a
continuous signed distance field representation of the 3D shape using
orthogonal 2D planes. Additionally, we meticulously enforce spatial
correspondences across distinct planes using a transformer-based autoencoder
structure, promoting the preservation of spatial relationships in the generated
3D shapes. This yields an algorithm that consistently outperforms
state-of-the-art 3D shape generation methods on various tasks, including
unconditional shape generation, multi-modal shape completion, single-view
reconstruction, and text-to-shape synthesis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Image Transformers for Prostate Cancer Detection from
  Ultrasound Data <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18233v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18233v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Harmanani, Paul F. R. Wilson, Fahimeh Fooladgar, Amoon Jamzad, Mahdi Gilany, Minh Nguyen Nhat To, Brian Wodlinger, Purang Abolmaesumi, Parvin Mousavi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  PURPOSE: Deep learning methods for classifying prostate cancer (PCa) in
ultrasound images typically employ convolutional networks (CNNs) to detect
cancer in small regions of interest (ROI) along a needle trace region. However,
this approach suffers from weak labelling, since the ground-truth
histopathology labels do not describe the properties of individual ROIs.
Recently, multi-scale approaches have sought to mitigate this issue by
combining the context awareness of transformers with a CNN feature extractor to
detect cancer from multiple ROIs using multiple-instance learning (MIL). In
this work, we present a detailed study of several image transformer
architectures for both ROI-scale and multi-scale classification, and a
comparison of the performance of CNNs and transformers for ultrasound-based
prostate cancer classification. We also design a novel multi-objective learning
strategy that combines both ROI and core predictions to further mitigate label
noise. METHODS: We evaluate 3 image transformers on ROI-scale cancer
classification, then use the strongest model to tune a multi-scale classifier
with MIL. We train our MIL models using our novel multi-objective learning
strategy and compare our results to existing baselines. RESULTS: We find that
for both ROI-scale and multi-scale PCa detection, image transformer backbones
lag behind their CNN counterparts. This deficit in performance is even more
noticeable for larger models. When using multi-objective learning, we can
improve performance of MIL, with a 77.9% AUROC, a sensitivity of 75.9%, and a
specificity of 66.3%. CONCLUSION: Convolutional networks are better suited for
modelling sparse datasets of prostate ultrasounds, producing more robust
features than transformers in PCa detection. Multi-scale methods remain the
best architecture for this task, with multi-objective learning presenting an
effective way to improve performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>early draft, 7 pages; Accepted to SPIE Medical Imaging 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fourier or Wavelet bases as counterpart self-attention in spikformer for
  efficient visual classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18228v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18228v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyu Wang, Duzhen Zhang, Tilelin Zhang, Bo Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Energy-efficient spikformer has been proposed by integrating the biologically
plausible spiking neural network (SNN) and artificial Transformer, whereby the
Spiking Self-Attention (SSA) is used to achieve both higher accuracy and lower
computational cost. However, it seems that self-attention is not always
necessary, especially in sparse spike-form calculation manners. In this paper,
we innovatively replace vanilla SSA (using dynamic bases calculating from Query
and Key) with spike-form Fourier Transform, Wavelet Transform, and their
combinations (using fixed triangular or wavelets bases), based on a key
hypothesis that both of them use a set of basis functions for information
transformation. Hence, the Fourier-or-Wavelet-based spikformer (FWformer) is
proposed and verified in visual classification tasks, including both static
image and event-based video datasets. The FWformer can achieve comparable or
even higher accuracies ($0.4\%$-$1.5\%$), higher running speed ($9\%$-$51\%$
for training and $19\%$-$70\%$ for inference), reduced theoretical energy
consumption ($20\%$-$25\%$), and reduced GPU memory usage ($4\%$-$26\%$),
compared to the standard spikformer. Our result indicates the continuous
refinement of new Transformers, that are inspired either by biological
discovery (spike-form), or information theory (Fourier or Wavelet Transform),
is promising.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 2 figures. arXiv admin note: substantial text overlap with
  arXiv:2308.02557</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Transformer-Based Framework for Payload Malware Detection and
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyle Stein, Arash Mahyari, Guillermo Francia III, Eman El-Sheikh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As malicious cyber threats become more sophisticated in breaching computer
networks, the need for effective intrusion detection systems (IDSs) becomes
crucial. Techniques such as Deep Packet Inspection (DPI) have been introduced
to allow IDSs analyze the content of network packets, providing more context
for identifying potential threats. IDSs traditionally rely on using
anomaly-based and signature-based detection techniques to detect unrecognized
and suspicious activity. Deep learning techniques have shown great potential in
DPI for IDSs due to their efficiency in learning intricate patterns from the
packet content being transmitted through the network. In this paper, we propose
a revolutionary DPI algorithm based on transformers adapted for the purpose of
detecting malicious traffic with a classifier head. Transformers learn the
complex content of sequence data and generalize them well to similar scenarios
thanks to their self-attention mechanism. Our proposed method uses the raw
payload bytes that represent the packet contents and is deployed as
man-in-the-middle. The payload bytes are used to detect malicious packets and
classify their types. Experimental results on the UNSW-NB15 and CIC-IOT23
datasets demonstrate that our transformer-based model is effective in
distinguishing malicious from benign traffic in the test dataset, attaining an
average accuracy of 79\% using binary classification and 72\% on the
multi-classification experiment, both using solely payload bytes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty-Aware Deployment of Pre-trained Language-Conditioned
  Imitation Learning Policies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18222v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18222v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Wu, Bruce D. Lee, Kostas Daniilidis, Bernadette Bucher, Nikolai Matni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale robotic policies trained on data from diverse tasks and robotic
platforms hold great promise for enabling general-purpose robots; however,
reliable generalization to new environment conditions remains a major
challenge. Toward addressing this challenge, we propose a novel approach for
uncertainty-aware deployment of pre-trained language-conditioned imitation
learning agents. Specifically, we use temperature scaling to calibrate these
models and exploit the calibrated model to make uncertainty-aware decisions by
aggregating the local information of candidate actions. We implement our
approach in simulation using three such pre-trained models, and showcase its
potential to significantly enhance task completion rates. The accompanying code
is accessible at the link:
https://github.com/BobWu1998/uncertainty_quant_all.git
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Two-Dimensional to Three-Dimensional Environment with Q-Learning:
  Modeling Autonomous Navigation with <span class="highlight-title">Reinforcement</span> Learning and no Libraries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18219v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18219v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ergon Cugler de Moraes Silva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) algorithms have become indispensable tools in
artificial intelligence, empowering agents to acquire optimal decision-making
policies through interactions with their environment and feedback mechanisms.
This study explores the performance of RL agents in both two-dimensional (2D)
and three-dimensional (3D) environments, aiming to research the dynamics of
learning across different spatial dimensions. A key aspect of this
investigation is the absence of pre-made libraries for learning, with the
algorithm developed exclusively through computational mathematics. The
methodological framework centers on RL principles, employing a Q-learning agent
class and distinct environment classes tailored to each spatial dimension. The
research aims to address the question: How do reinforcement learning agents
adapt and perform in environments of varying spatial dimensions, particularly
in 2D and 3D settings? Through empirical analysis, the study evaluates agents'
learning trajectories and adaptation processes, revealing insights into the
efficacy of RL algorithms in navigating complex, multi-dimensional spaces.
Reflections on the findings prompt considerations for future research,
particularly in understanding the dynamics of learning in higher-dimensional
environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Minimax Optimal Fair Classification with Bounded Demographic Disparity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18216v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18216v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianli Zeng, Guang Cheng, Edgar Dobriban
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mitigating the disparate impact of statistical machine learning methods is
crucial for ensuring fairness. While extensive research aims to reduce
disparity, the effect of using a \emph{finite dataset} -- as opposed to the
entire population -- remains unclear. This paper explores the statistical
foundations of fair binary classification with two protected groups, focusing
on controlling demographic disparity, defined as the difference in acceptance
rates between the groups. Although fairness may come at the cost of accuracy
even with infinite data, we show that using a finite sample incurs additional
costs due to the need to estimate group-specific acceptance thresholds. We
study the minimax optimal classification error while constraining demographic
disparity to a user-specified threshold. To quantify the impact of fairness
constraints, we introduce a novel measure called \emph{fairness-aware excess
risk} and derive a minimax lower bound on this measure that all classifiers
must satisfy. Furthermore, we propose FairBayes-DDP+, a group-wise thresholding
method with an offset that we show attains the minimax lower bound. Our lower
bound proofs involve several innovations. Experiments support that
FairBayes-DDP+ controls disparity at the user-specified level, while being
faster and having a more favorable fairness-accuracy tradeoff than several
baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Theoretical Guarantees for the Subspace-Constrained Tyler's Estimator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18658v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18658v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gilad Lerman, Feng Yu, Teng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work analyzes the subspace-constrained Tyler's estimator (STE) designed
for recovering a low-dimensional subspace within a dataset that may be highly
corrupted with outliers. It assumes a weak inlier-outlier model and allows the
fraction of inliers to be smaller than a fraction that leads to computational
hardness of the robust subspace recovery problem. It shows that in this
setting, if the initialization of STE, which is an iterative algorithm,
satisfies a certain condition, then STE can effectively recover the underlying
subspace. It further shows that under the generalized haystack model, STE
initialized by the Tyler's M-estimator (TME), can recover the subspace when the
fraction of iniliers is too small for TME to handle.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Statistical Inference of Optimal Allocations I: Regularities and their
  Implications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18248v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18248v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Feng, Han Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we develp a functional differentiability approach for solving
statistical optimal allocation problems. We first derive Hadamard
differentiability of the value function through a detailed analysis of the
general properties of the sorting operator. Central to our framework are the
concept of Hausdorff measure and the area and coarea integration formulas from
geometric measure theory. Building on our Hadamard differentiability results,
we demonstrate how the functional delta method can be used to directly derive
the asymptotic properties of the value function process for binary constrained
optimal allocation problems, as well as the two-step ROC curve estimator.
Moreover, leveraging profound insights from geometric functional analysis on
convex and local Lipschitz functionals, we obtain additional generic Fr\'echet
differentiability results for the value functions of optimal allocation
problems. These compelling findings motivate us to study carefully the first
order approximation of the optimal social welfare. In this paper, we then
present a double / debiased estimator for the value functions. Importantly, the
conditions outlined in the Hadamard differentiability section validate the
margin assumption from the statistical classification literature employing
plug-in methods that justifies a faster convergence rate.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A 4D Hybrid Algorithm to Scale Parallel Training to Thousands of GPUs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.13525v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.13525v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddharth Singh, Prajwal Singhania, Aditya K. Ranjan, Zack Sating, Abhinav Bhatele
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large communication costs are a critical bottleneck in training
state-of-the-art neural networks on distributed systems. This paper introduces
AxoNN, a novel four-dimensional (4D) parallelization approach, inspired by
Agarwal's algorithm for matrix multiplication, for parallelizing tensor
computations in deep learning, AxoNN employs two key strategies to minimize
communication overhead. First, we optimize communication by overlapping
expensive collective operations (reduce-scatter, all-gather, all-reduce) with
computations. Our experiments with a 20-billion parameter transformer model
demonstrate that these optimizations deliver nearly 53\% improvement. Second,
we present an analytical model to assist users in identifying
communication-minimizing configurations within the vast search space defined by
our 4D algorithm. This model empowers practitioners by simplifying the tuning
process for their specific training workloads. When training an 80-billion
parameter model on 1024 GPUs of Perlmutter, AxoNN surpasses Megatron-LM, a
state-of-the-art framework, by a significant 26%. Additionally, it achieves 57%
of the theoretical peak FLOP/s.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CrystalBox: Future-Based Explanations for Input-Driven Deep RL Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.13483v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.13483v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sagar Patel, Sangeetha Abdu Jyothi, Nina Narodytska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present CrystalBox, a novel, model-agnostic, posthoc explainability
framework for Deep Reinforcement Learning (DRL) controllers in the large family
of input-driven environments which includes computer systems. We combine the
natural decomposability of reward functions in input-driven environments with
the explanatory power of decomposed returns. We propose an efficient algorithm
to generate future-based explanations across both discrete and continuous
control environments. Using applications such as adaptive bitrate streaming and
congestion control, we demonstrate CrystalBox's capability to generate
high-fidelity explanations. We further illustrate its higher utility across
three practical use cases: contrastive explanations, network observability, and
guided reward design, as opposed to prior explainability techniques that
identify salient features.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalization Bounds: Perspectives from Information Theory and
  PAC-Bayes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.04381v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.04381v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fredrik Hellström, Giuseppe Durisi, Benjamin Guedj, Maxim Raginsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A fundamental question in theoretical machine learning is generalization.
Over the past decades, the PAC-Bayesian approach has been established as a
flexible framework to address the generalization capabilities of machine
learning algorithms, and design new ones. Recently, it has garnered increased
interest due to its potential applicability for a variety of learning
algorithms, including deep neural networks. In parallel, an
information-theoretic view of generalization has developed, wherein the
relation between generalization and various information measures has been
established. This framework is intimately connected to the PAC-Bayesian
approach, and a number of results have been independently discovered in both
strands. In this monograph, we highlight this strong connection and present a
unified treatment of PAC-Bayesian and information-theoretic generalization
bounds. We present techniques and results that the two perspectives have in
common, and discuss the approaches and interpretations that differ. In
particular, we demonstrate how many proofs in the area share a modular
structure, through which the underlying ideas can be intuited. We pay special
attention to the conditional mutual information (CMI) framework; analytical
studies of the information complexity of learning algorithms; and the
application of the proposed methods to deep learning. This monograph is
intended to provide a comprehensive introduction to information-theoretic
generalization bounds and their connection to PAC-Bayes, serving as a
foundation from which the most recent developments are accessible. It is aimed
broadly towards researchers with an interest in generalization and theoretical
machine learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>228 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decoupled Data Consistency with Diffusion Purification for Image
  Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06054v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06054v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Soo Min Kwon, Ismail R. Alkhouri, Saiprasad Ravishankar, Qing Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have recently gained traction as a powerful class of deep
generative priors, excelling in a wide range of image restoration tasks due to
their exceptional ability to model data distributions. To solve image
restoration problems, many existing techniques achieve data consistency by
incorporating additional likelihood gradient steps into the reverse sampling
process of diffusion models. However, the additional gradient steps pose a
challenge for real-world practical applications as they incur a large
computational overhead, thereby increasing inference time. They also present
additional difficulties when using accelerated diffusion model samplers, as the
number of data consistency steps is limited by the number of reverse sampling
steps. In this work, we propose a novel diffusion-based image restoration
solver that addresses these issues by decoupling the reverse process from the
data consistency steps. Our method involves alternating between a
reconstruction phase to maintain data consistency and a refinement phase that
enforces the prior via diffusion purification. Our approach demonstrates
versatility, making it highly adaptable for efficient problem-solving in latent
space. Additionally, it reduces the necessity for numerous sampling steps
through the integration of consistency models. The efficacy of our approach is
validated through comprehensive experiments across various image restoration
tasks, including image denoising, deblurring, inpainting, and super-resolution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedSN: A Novel Federated Learning Framework over LEO Satellite Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01483v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01483v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Lin, Zhe Chen, Zihan Fang, Xianhao Chen, Xiong Wang, Yue Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, a large number of Low Earth Orbit (LEO) satellites have been
launched and deployed successfully in space by commercial companies, such as
SpaceX. Due to multimodal sensors equipped by the LEO satellites, they serve
not only for communication but also for various machine learning applications,
such as space modulation recognition, remote sensing image classification, etc.
However, the ground station (GS) may be incapable of downloading such a large
volume of raw sensing data for centralized model training due to the limited
contact time with LEO satellites (e.g. 5 minutes). Therefore, federated
learning (FL) has emerged as the promising solution to address this problem via
on-device training. Unfortunately, to enable FL on LEO satellites, we still
face three critical challenges that are i) heterogeneous computing and memory
capabilities, ii) limited uplink rate, and iii) model staleness. To this end,
we propose FedSN as a general FL framework to tackle the above challenges, and
fully explore data diversity on LEO satellites. Specifically, we first present
a novel sub-structure scheme to enable heterogeneous local model training
considering different computing, memory, and communication constraints on LEO
satellites. Additionally, we propose a pseudo-synchronous model aggregation
strategy to dynamically schedule model aggregation for compensating model
staleness. To further demonstrate the effectiveness of the FedSN, we evaluate
it using space modulation recognition and remote sensing image classification
tasks by leveraging the data from real-world satellite networks. Extensive
experimental results demonstrate that FedSN framework achieves higher accuracy,
lower computing, and communication overhead than the state-of-the-art
benchmarks and the effectiveness of each components in FedSN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simplified Diffusion Schrödinger Bridge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14623v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14623v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhicong Tang, Tiankai Hang, Shuyang Gu, Dong Chen, Baining Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel theoretical simplification of the Diffusion
Schr\"odinger Bridge (DSB) that facilitates its unification with Score-based
Generative Models (SGMs), addressing the limitations of DSB in complex data
generation and enabling faster convergence and enhanced performance. By
employing SGMs as an initial solution for DSB, our approach capitalizes on the
strengths of both frameworks, ensuring a more efficient training process and
improving the performance of SGM. We also propose a reparameterization
technique that, despite theoretical approximations, practically improves the
network's fitting capabilities. Our extensive experimental evaluations confirm
the effectiveness of the simplified DSB, demonstrating its significant
improvements. We believe the contributions of this work pave the way for
advanced generative modeling. The code is available at
https://github.com/checkcrab/SDSB.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Preventing Arbitrarily High Confidence on Far-Away Data in
  Point-Estimated Discriminative Neural Networks <span class="chip">AISTATS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.03683v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.03683v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad Rashid, Serena Hacker, Guojun Zhang, Agustinus Kristiadi, Pascal Poupart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discriminatively trained, deterministic neural networks are the de facto
choice for classification problems. However, even though they achieve
state-of-the-art results on in-domain test sets, they tend to be overconfident
on out-of-distribution (OOD) data. For instance, ReLU networks - a popular
class of neural network architectures - have been shown to almost always yield
high confidence predictions when the test data are far away from the training
set, even when they are trained with OOD data. We overcome this problem by
adding a term to the output of the neural network that corresponds to the logit
of an extra class, that we design to dominate the logits of the original
classes as we move away from the training data.This technique provably prevents
arbitrarily high confidence on far-away test data while maintaining a simple
discriminative point-estimate training. Evaluation on various benchmarks
demonstrates strong performance against competitive baselines on both far-away
and realistic OOD data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AISTATS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03100v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03100v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeqian Ju, Yuancheng Wang, Kai Shen, Xu Tan, Detai Xin, Dongchao Yang, Yanqing Liu, Yichong Leng, Kaitao Song, Siliang Tang, Zhizheng Wu, Tao Qin, Xiang-Yang Li, Wei Ye, Shikun Zhang, Jiang Bian, Lei He, Jinyu Li, Sheng Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recent large-scale text-to-speech (TTS) models have achieved
significant progress, they still fall short in speech quality, similarity, and
prosody. Considering speech intricately encompasses various attributes (e.g.,
content, prosody, timbre, and acoustic details) that pose significant
challenges for generation, a natural idea is to factorize speech into
individual subspaces representing different attributes and generate them
individually. Motivated by it, we propose NaturalSpeech 3, a TTS system with
novel factorized diffusion models to generate natural speech in a zero-shot
way. Specifically, 1) we design a neural codec with factorized vector
quantization (FVQ) to disentangle speech waveform into subspaces of content,
prosody, timbre, and acoustic details; 2) we propose a factorized diffusion
model to generate attributes in each subspace following its corresponding
prompt. With this factorization design, NaturalSpeech 3 can effectively and
efficiently model intricate speech with disentangled subspaces in a
divide-and-conquer way. Experiments show that NaturalSpeech 3 outperforms the
state-of-the-art TTS systems on quality, similarity, prosody, and
intelligibility, and achieves on-par quality with human recordings.
Furthermore, we achieve better performance by scaling to 1B parameters and 200K
hours of training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Achieving human-level quality and naturalness on multi-speaker
  datasets (e.g., LibriSpeech) in a zero-shot way</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nesting Particle Filters for Experimental Design in Dynamical Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07868v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07868v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sahel Iqbal, Adrien Corenflos, Simo Särkkä, Hany Abdulsamad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel approach to Bayesian experimental design
for non-exchangeable data that formulates it as risk-sensitive policy
optimization. We develop the Inside-Out SMC$^2$ algorithm, a nested sequential
Monte Carlo technique to infer optimal designs, and embed it into a particle
Markov chain Monte Carlo framework to perform gradient-based policy
amortization. Our approach is distinct from other amortized experimental design
techniques, as it does not rely on contrastive estimators. Numerical validation
on a set of dynamical systems showcases the efficacy of our method in
comparison to other state-of-the-art strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive <span class="highlight-title">Review</span> of Community Detection in Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11798v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11798v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiakang Li, Songning Lai, Zhihao Shuai, Yuan Tan, Yifan Jia, Mianyang Yu, Zichen Song, Xiaokang Peng, Ziyang Xu, Yongxin Ni, Haifeng Qiu, Jiayu Yang, Yutong Liu, Yonggang Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The study of complex networks has significantly advanced our understanding of
community structures which serves as a crucial feature of real-world graphs.
Detecting communities in graphs is a challenging problem with applications in
sociology, biology, and computer science. Despite the efforts of an
interdisciplinary community of scientists, a satisfactory solution to this
problem has not yet been achieved. This review article delves into the topic of
community detection in graphs, which serves as a thorough exposition of various
community detection methods from perspectives of modularity-based method,
spectral clustering, probabilistic modelling, and deep learning. Along with the
methods, a new community detection method designed by us is also presented.
Additionally, the performance of these methods on the datasets with and without
ground truth is compared. In conclusion, this comprehensive review provides a
deep understanding of community detection in graphs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Input Convex Lipschitz RNN: A Fast and Robust Approach for Engineering
  Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.07494v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.07494v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Wang, P S Pravin, Zhe Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computational efficiency and non-adversarial robustness are critical factors
in real-world engineering applications. Yet, conventional neural networks often
fall short in addressing both simultaneously, or even separately. Drawing
insights from natural physical systems and existing literature, it is known
that an input convex architecture enhances computational efficiency, while a
Lipschitz-constrained architecture bolsters non-adversarial robustness. By
leveraging the strengths of convexity and Lipschitz continuity, we develop a
novel network architecture, termed Input Convex Lipschitz Recurrent Neural
Networks. This model is explicitly designed for fast and robust
optimization-based tasks and outperforms existing recurrent units across a
spectrum of engineering tasks in terms of computational efficiency and
non-adversarial robustness, including real-world solar irradiance prediction
for Solar PV system planning at LHT Holdings in Singapore and real-time Model
Predictive Control optimization for a nonlinear chemical reactor.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and
  Ethics) Evaluation: A <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.03123v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.03123v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunder Ali Khowaja, Parus Khuwaja, Kapal Dev, Weizheng Wang, Lewis Nkenyereye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ChatGPT is another large language model (LLM) vastly available for the
consumers on their devices but due to its performance and ability to converse
effectively, it has gained a huge popularity amongst research as well as
industrial community. Recently, many studies have been published to show the
effectiveness, efficiency, integration, and sentiments of chatGPT and other
LLMs. In contrast, this study focuses on the important aspects that are mostly
overlooked, i.e. sustainability, privacy, digital divide, and ethics and
suggests that not only chatGPT but every subsequent entry in the category of
conversational bots should undergo Sustainability, PrivAcy, Digital divide, and
Ethics (SPADE) evaluation. This paper discusses in detail the issues and
concerns raised over chatGPT in line with aforementioned characteristics. We
also discuss the recent EU AI Act briefly in accordance with the SPADE
evaluation. We support our hypothesis by some preliminary data collection and
visualizations along with hypothesized facts. We also suggest mitigations and
recommendations for each of the concerns. Furthermore, we also suggest some
policies and recommendations for EU AI policy act concerning ethics, digital
divide, and sustainability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 8 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Empowering Data Mesh with Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17878v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17878v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyuan Li, Salman Toor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evolution of data architecture has seen the rise of data lakes, aiming to
solve the bottlenecks of data management and promote intelligent
decision-making. However, this centralized architecture is limited by the
proliferation of data sources and the growing demand for timely analysis and
processing. A new data paradigm, Data Mesh, is proposed to overcome these
challenges. Data Mesh treats domains as a first-class concern by distributing
the data ownership from the central team to each data domain, while keeping the
federated governance to monitor domains and their data products. Many
multi-million dollar organizations like Paypal, Netflix, and Zalando have
already transformed their data analysis pipelines based on this new
architecture. In this decentralized architecture where data is locally
preserved by each domain team, traditional centralized machine learning is
incapable of conducting effective analysis across multiple domains, especially
for security-sensitive organizations. To this end, we introduce a pioneering
approach that incorporates Federated Learning into Data Mesh. To the best of
our knowledge, this is the first open-source applied work that represents a
critical advancement toward the integration of federated learning methods into
the Data Mesh paradigm, underscoring the promising prospects for
privacy-preserving and decentralized data analysis strategies within Data Mesh
architecture.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stochastic Approximation with Delayed Updates: Finite-Time Rates under
  Markovian Sampling <span class="chip">AISTATS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11800v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11800v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arman Adibi, Nicolo Dal Fabbro, Luca Schenato, Sanjeev Kulkarni, H. Vincent Poor, George J. Pappas, Hamed Hassani, Aritra Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by applications in large-scale and multi-agent reinforcement
learning, we study the non-asymptotic performance of stochastic approximation
(SA) schemes with delayed updates under Markovian sampling. While the effect of
delays has been extensively studied for optimization, the manner in which they
interact with the underlying Markov process to shape the finite-time
performance of SA remains poorly understood. In this context, our first main
contribution is to show that under time-varying bounded delays, the delayed SA
update rule guarantees exponentially fast convergence of the \emph{last
iterate} to a ball around the SA operator's fixed point. Notably, our bound is
\emph{tight} in its dependence on both the maximum delay $\tau_{max}$, and the
mixing time $\tau_{mix}$. To achieve this tight bound, we develop a novel
inductive proof technique that, unlike various existing delayed-optimization
analyses, relies on establishing uniform boundedness of the iterates. As such,
our proof may be of independent interest. Next, to mitigate the impact of the
maximum delay on the convergence rate, we provide the first finite-time
analysis of a delay-adaptive SA scheme under Markovian sampling. In particular,
we show that the exponent of convergence of this scheme gets scaled down by
$\tau_{avg}$, as opposed to $\tau_{max}$ for the vanilla delayed SA rule; here,
$\tau_{avg}$ denotes the average delay across all iterations. Moreover, the
adaptive scheme requires no prior knowledge of the delay sequence for step-size
tuning. Our theoretical findings shed light on the finite-time effects of
delays for a broad class of algorithms, including TD learning, Q-learning, and
stochastic gradient descent under Markovian sampling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 27th International Conference on Artificial
  Intelligence and Statistics (AISTATS) 2024!</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Negative Evidential Deep Learning for Open-set Semi-supervised
  Learning <span class="chip">AAAI2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12091v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12091v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Yu, Danruo Deng, Furui Liu, Yueming Jin, Qi Dou, Guangyong Chen, Pheng-Ann Heng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised learning (SSL) methods assume that labeled data, unlabeled
data and test data are from the same distribution. Open-set semi-supervised
learning (Open-set SSL) considers a more practical scenario, where unlabeled
data and test data contain new categories (outliers) not observed in labeled
data (inliers). Most previous works focused on outlier detection via binary
classifiers, which suffer from insufficient scalability and inability to
distinguish different types of uncertainty. In this paper, we propose a novel
framework, Adaptive Negative Evidential Deep Learning (ANEDL) to tackle these
limitations. Concretely, we first introduce evidential deep learning (EDL) as
an outlier detector to quantify different types of uncertainty, and design
different uncertainty metrics for self-training and inference. Furthermore, we
propose a novel adaptive negative optimization strategy, making EDL more
tailored to the unlabeled dataset containing both inliers and outliers. As
demonstrated empirically, our proposed method outperforms existing
state-of-the-art methods across four datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Guided Distant Supervision for Multilingual Relation Extraction Data:
  Adapting to a New Language <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17143v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17143v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alistair Plum, Tharindu Ranasinghe, Christoph Purschke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relation extraction is essential for extracting and understanding
biographical information in the context of digital humanities and related
subjects. There is a growing interest in the community to build datasets
capable of training machine learning models to extract relationships. However,
annotating such datasets can be expensive and time-consuming, in addition to
being limited to English. This paper applies guided distant supervision to
create a large biographical relationship extraction dataset for German. Our
dataset, composed of more than 80,000 instances for nine relationship types, is
the largest biographical German relationship extraction dataset. We also create
a manually annotated dataset with 2000 instances to evaluate the models and
release it together with the dataset compiled using guided distant supervision.
We train several state-of-the-art machine learning models on the automatically
created dataset and release them as well. Furthermore, we experiment with
multilingual and cross-lingual experiments that could benefit many low-resource
languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024 (The 2024 Joint International Conference
  on Computational Linguistics, Language Resources and Evaluation)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Byzantine-resilient Federated Learning With Adaptivity to Data
  Heterogeneity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13374v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13374v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyuan Zuo, Xingrun Yan, Rongfei Fan, Han Hu, Hangguan Shan, Tony Q. S. Quek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper deals with federated learning (FL) in the presence of malicious
Byzantine attacks and data heterogeneity. A novel Robust Average Gradient
Algorithm (RAGA) is proposed, which leverages the geometric median for
aggregation and can freely select the round number for local updating.
Different from most existing resilient approaches, which perform convergence
analysis based on strongly-convex loss function or homogeneously distributed
dataset, we conduct convergence analysis for not only strongly-convex but also
non-convex loss function over heterogeneous dataset. According to our
theoretical analysis, as long as the fraction of dataset from malicious users
is less than half, RAGA can achieve convergence at rate
$\mathcal{O}({1}/{T^{2/3- \delta}})$ where $T$ is the iteration number and
$\delta \in (0, 2/3)$ for non-convex loss function, and at linear rate for
strongly-convex loss function. Moreover, stationary point or global optimal
solution is proved to obtainable as data heterogeneity vanishes. Experimental
results corroborate the robustness of RAGA to Byzantine attacks and verifies
the advantage of RAGA over baselines on convergence performance under various
intensity of Byzantine attacks, for heterogeneous dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Demystifying Misconceptions in Social Bots Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.17251v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.17251v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefano Cresci, Kai-Cheng Yang, Angelo Spognardi, Roberto Di Pietro, Filippo Menczer, Marinella Petrocchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research on social bots aims at advancing knowledge and providing solutions
to one of the most debated forms of online manipulation. Yet, social bot
research is plagued by widespread biases, hyped results, and misconceptions
that set the stage for ambiguities, unrealistic expectations, and seemingly
irreconcilable findings. Overcoming such issues is instrumental towards
ensuring reliable solutions and reaffirming the validity of the scientific
method. In this contribution, we review some recent results in social bots
research, highlighting and revising factual errors as well as methodological
and conceptual biases. More importantly, we demystify common misconceptions,
addressing fundamental points on how social bots research is discussed. Our
analysis surfaces the need to discuss research about online disinformation and
manipulation in a rigorous, unbiased, and responsible way. This article
bolsters such effort by identifying and refuting common fallacious arguments
used by both proponents and opponents of social bots research, as well as
providing directions toward sound methodologies for future research in the
field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LCANets++: Robust Audio Classification using Multi-layer Neural Networks
  with Lateral Competition <span class="chip">ICASSP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12882v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12882v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sayanton V. Dibbo, Juston S. Moore, Garrett T. Kenyon, Michael A. Teti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio classification aims at recognizing audio signals, including speech
commands or sound events. However, current audio classifiers are susceptible to
perturbations and adversarial attacks. In addition, real-world audio
classification tasks often suffer from limited labeled data. To help bridge
these gaps, previous work developed neuro-inspired convolutional neural
networks (CNNs) with sparse coding via the Locally Competitive Algorithm (LCA)
in the first layer (i.e., LCANets) for computer vision. LCANets learn in a
combination of supervised and unsupervised learning, reducing dependency on
labeled samples. Motivated by the fact that auditory cortex is also sparse, we
extend LCANets to audio recognition tasks and introduce LCANets++, which are
CNNs that perform sparse coding in multiple layers via LCA. We demonstrate that
LCANets++ are more robust than standard CNNs and LCANets against perturbations,
e.g., background noise, as well as black-box and white-box attacks, e.g.,
evasion and fast gradient sign (FGSM) attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 2024 IEEE International Conference on Acoustics, Speech
  and Signal Processing Workshops (ICASSPW)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeepMachining: Online Prediction of Machining Errors of Lathe Machines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16451v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16451v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang-Li Lu, Hwai-Jung Hsu, Che-Wei Chou, H. T. Kung, Chen-Hsin Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We describe DeepMachining, a deep learning-based AI system for online
prediction of machining errors of lathe machine operations. We have built and
evaluated DeepMachining based on manufacturing data from factories.
Specifically, we first pretrain a deep learning model for a given lathe
machine's operations to learn the salient features of machining states. Then,
we fine-tune the pretrained model to adapt to specific machining tasks. We
demonstrate that DeepMachining achieves high prediction accuracy for multiple
tasks that involve different workpieces and cutting tools. To the best of our
knowledge, this work is one of the first factory experiments using pre-trained
deep-learning models to predict machining errors of lathe machines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recurrent Action Transformer with Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.09459v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.09459v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexey Staroverov, Egor Cherepanov, Dmitry Yudin, Alexey K. Kovalev, Aleksandr I. Panov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the use of transformers in offline reinforcement learning has
become a rapidly developing area. This is due to their ability to treat the
agent's trajectory in the environment as a sequence, thereby reducing the
policy learning problem to sequence modeling. In environments where the agent's
decisions depend on past events, it is essential to capture both the event
itself and the decision point in the context of the model. However, the
quadratic complexity of the attention mechanism limits the potential for
context expansion. One solution to this problem is to enhance transformers with
memory mechanisms. In this paper, we propose the Recurrent Action Transformer
with Memory (RATE) - a model that incorporates recurrent memory. To evaluate
our model, we conducted extensive experiments on both memory-intensive
environments (VizDoom-Two-Color, T-Maze) and classic Atari games and MuJoCo
control environments. The results show that the use of memory can significantly
improve performance in memory-intensive environments while maintaining or
improving results in classic environments. We hope that our findings will
stimulate research on memory mechanisms for transformers applicable to offline
reinforcement learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Pre-Training of Time-Series Data for Unsupervised Fault
  Detection in Semiconductor Manufacturing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11427v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11427v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sewoong Lee, JinKyou Choi, Min Su Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces TRACE-GPT, which stands for Time-seRies
Anomaly-detection with Convolutional Embedding and Generative Pre-trained
Transformers. TRACE-GPT is designed to pre-train univariate time-series sensor
data and detect faults on unlabeled datasets in semiconductor manufacturing. In
semiconductor industry, classifying abnormal time-series sensor data from
normal data is important because it is directly related to wafer defect.
However, small, unlabeled, and even mixed training data without enough
anomalies make classification tasks difficult. In this research, we capture
features of time-series data with temporal convolutional embedding and
Generative Pre-trained Transformer (GPT) to classify abnormal sequences from
normal sequences using cross entropy loss. We prove that our model shows better
performance than previous unsupervised models with both an open dataset, the
University of California Riverside (UCR) time-series classification archive,
and the process log of our Chemical Vapor Deposition (CVD) equipment. Our model
has the highest F1 score at Equal Error Rate (EER) across all datasets and is
only 0.026 below the supervised state-of-the-art baseline on the open dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attacks, Defenses and Evaluations for LLM Conversation Safety: A <span class="highlight-title">Survey</span> <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09283v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09283v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichen Dong, Zhanhui Zhou, Chao Yang, Jing Shao, Yu Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are now commonplace in conversation
applications. However, their risks of misuse for generating harmful responses
have raised serious societal concerns and spurred recent research on LLM
conversation safety. Therefore, in this survey, we provide a comprehensive
overview of recent studies, covering three critical aspects of LLM conversation
safety: attacks, defenses, and evaluations. Our goal is to provide a structured
summary that enhances understanding of LLM conversation safety and encourages
further investigation into this important subject. For easy reference, we have
categorized all the studies mentioned in this survey according to our taxonomy,
available at: https://github.com/niconi19/LLM-conversation-safety.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ No-Regret Learning in Bilateral Trade via Global Budget Balance <span class="chip">STOC 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12370v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12370v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martino Bernasconi, Matteo Castiglioni, Andrea Celli, Federico Fusco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bilateral trade models the problem of intermediating between two rational
agents -- a seller and a buyer -- both characterized by a private valuation for
an item they want to trade. We study the online learning version of the
problem, in which at each time step a new seller and buyer arrive and the
learner has to set prices for them without any knowledge about their
(adversarially generated) valuations.
  In this setting, known impossibility results rule out the existence of
no-regret algorithms when budget balanced has to be enforced at each time step.
In this paper, we introduce the notion of \emph{global budget balance}, which
only requires the learner to fulfill budget balance over the entire time
horizon. Under this natural relaxation, we provide the first no-regret
algorithms for adversarial bilateral trade under various feedback models.
First, we show that in the full-feedback model, the learner can guarantee
$\tilde O(\sqrt{T})$ regret against the best fixed prices in hindsight, and
that this bound is optimal up to poly-logarithmic terms. Second, we provide a
learning algorithm guaranteeing a $\tilde O(T^{3/4})$ regret upper bound with
one-bit feedback, which we complement with a $\Omega(T^{5/7})$ lower bound that
holds even in the two-bit feedback model. Finally, we introduce and analyze an
alternative benchmark that is provably stronger than the best fixed prices in
hindsight and is inspired by the literature on bandits with knapsacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at STOC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Few-Shot Detection of Machine-Generated Text using Style Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.06712v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.06712v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafael Rivera Soto, Kailin Koch, Aleem Khan, Barry Chen, Marcus Bishop, Nicholas Andrews
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of instruction-tuned language models that convincingly mimic human
writing poses a significant risk of abuse. However, such abuse may be
counteracted with the ability to detect whether a piece of text was composed by
a language model rather than a human author. Some previous approaches to this
problem have relied on supervised methods by training on corpora of confirmed
human- and machine- written documents. Unfortunately, model under-specification
poses an unavoidable challenge for neural network-based detectors, making them
brittle in the face of data shifts, such as the release of newer language
models producing still more fluent text than the models used to train the
detectors. Other approaches require access to the models that may have
generated a document in question, which is often impractical. In light of these
challenges, we pursue a fundamentally different approach not relying on samples
from language models of concern at training time. Instead, we propose to
leverage representations of writing style estimated from human-authored text.
Indeed, we find that features effective at distinguishing among human authors
are also effective at distinguishing human from machine authors, including
state-of-the-art large language models like Llama-2, ChatGPT, and GPT-4.
Furthermore, given a handful of examples composed by each of several specific
language models of interest, our approach affords the ability to predict which
model generated a given document. The code and data to reproduce our
experiments are available at
https://github.com/LLNL/LUAR/tree/main/fewshot_iclr2024.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ABScribe: Rapid <span class="highlight-title">Exploration</span> & Organization of Multiple Writing
  Variations in Human-AI Co-Writing Tasks using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00117v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00117v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohi Reza, Nathan Laundry, Ilya Musabirov, Peter Dushniku, Zhi Yuan "Michael" Yu, Kashish Mittal, Tovi Grossman, Michael Liut, Anastasia Kuzminykh, Joseph Jay Williams
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exploring alternative ideas by rewriting text is integral to the writing
process. State-of-the-art Large Language Models (LLMs) can simplify writing
variation generation. However, current interfaces pose challenges for
simultaneous consideration of multiple variations: creating new variations
without overwriting text can be difficult, and pasting them sequentially can
clutter documents, increasing workload and disrupting writers' flow. To tackle
this, we present ABScribe, an interface that supports rapid, yet visually
structured, exploration and organization of writing variations in human-AI
co-writing tasks. With ABScribe, users can swiftly modify variations using LLM
prompts, which are auto-converted into reusable buttons. Variations are stored
adjacently within text fields for rapid in-place comparisons using mouse-over
interactions on a popup toolbar. Our user study with 12 writers shows that
ABScribe significantly reduces task workload (d = 1.20, p < 0.001), enhances
user perceptions of the revision process (d = 2.41, p < 0.001) compared to a
popular baseline workflow, and provides insights into how writers explore
variations using LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CHI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CroSel: Cross <span class="highlight-title">Selection</span> of Confident Pseudo Labels for Partial-Label
  Learning <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10365v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10365v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyu Tian, Hongxin Wei, Yiqun Wang, Lei Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Partial-label learning (PLL) is an important weakly supervised learning
problem, which allows each training example to have a candidate label set
instead of a single ground-truth label. Identification-based methods have been
widely explored to tackle label ambiguity issues in PLL, which regard the true
label as a latent variable to be identified. However, identifying the true
labels accurately and completely remains challenging, causing noise in pseudo
labels during model training. In this paper, we propose a new method called
CroSel, which leverages historical predictions from the model to identify true
labels for most training examples. First, we introduce a cross selection
strategy, which enables two deep models to select true labels of partially
labeled data for each other. Besides, we propose a novel consistency
regularization term called co-mix to avoid sample waste and tiny noise caused
by false selection. In this way, CroSel can pick out the true labels of most
examples with high precision. Extensive experiments demonstrate the superiority
of CroSel, which consistently outperforms previous state-of-the-art methods on
benchmark datasets. Additionally, our method achieves over 90\% accuracy and
quantity for selecting true labels on CIFAR-type datasets under various
settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Machine Learning Optimized Orthogonal Basis Piecewise Polynomial
  Approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08579v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08579v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hannes Waclawek, Stefan Huber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Piecewise Polynomials (PPs) are utilized in several engineering disciplines,
like trajectory planning, to approximate position profiles given in the form of
a set of points. While the approximation target along with domain-specific
requirements, like Ck -continuity, can be formulated as a system of equations
and a result can be computed directly, such closed-form solutions posses
limited flexibility with respect to polynomial degrees, polynomial bases or
adding further domain-specific requirements. Sufficiently complex optimization
goals soon call for the use of numerical methods, like gradient descent. Since
gradient descent lies at the heart of training Artificial Neural Networks
(ANNs), modern Machine Learning (ML) frameworks like TensorFlow come with a set
of gradient-based optimizers potentially suitable for a wide range of
optimization problems beyond the training task for ANNs. Our approach is to
utilize the versatility of PP models and combine it with the potential of
modern ML optimizers for the use in function approximation in 1D trajectory
planning in the context of electronic cam design. We utilize available
optimizers of the ML framework TensorFlow directly, outside of the scope of
ANNs, to optimize model parameters of our PP model. In this paper, we show how
an orthogonal polynomial basis contributes to improving approximation and
continuity optimization performance. Utilizing Chebyshev polynomials of the
first kind, we develop a novel regularization approach enabling clearly
improved convergence behavior. We show that, using this regularization
approach, Chebyshev basis performs better than power basis for all relevant
optimizers in the combined approximation and continuity optimization setting
and demonstrate usability of the presented approach within the electronic cam
domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to LION18</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Challenging Common Paradigms in Multi-Task Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.04698v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.04698v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cathrin Elich, Lukas Kirchdorfer, Jan M. Köhler, Lukas Schott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While multi-task learning (MTL) has gained significant attention in recent
years, its underlying mechanisms remain poorly understood. Recent methods did
not yield consistent performance improvements over single task learning (STL)
baselines, underscoring the importance of gaining more profound insights about
challenges specific to MTL. In our study, we challenge paradigms in MTL in the
context of STL: First, the impact of the choice of optimizer has only been
mildly investigated in MTL. We show the pivotal role of common STL tools such
as the Adam optimizer in MTL empirically in various experiments. To further
investigate Adam's effectiveness, we theoretical derive a partial loss-scale
invariance under mild assumptions. Second, the notion of gradient conflicts has
often been phrased as a specific problem in MTL. We delve into the role of
gradient conflicts in MTL and compare it to STL. For angular gradient alignment
we find no evidence that this is a unique problem in MTL. We emphasize
differences in gradient magnitude as the main distinguishing factor. Lastly, we
compare the transferability of features learned through MTL and STL on common
image corruptions, and find light evidence that MTL can lead to superior
transferability. Overall, we find surprising similarities between STL and MTL
suggesting to consider methods from both fields in a broader context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>-</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hourglass Tokenizer for Efficient Transformer-Based 3D Human Pose
  Estimation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12028v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12028v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Li, Mengyuan Liu, Hong Liu, Pichao Wang, Jialun Cai, Nicu Sebe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have been successfully applied in the field of video-based 3D
human pose estimation. However, the high computational costs of these video
pose transformers (VPTs) make them impractical on resource-constrained devices.
In this paper, we present a plug-and-play pruning-and-recovering framework,
called Hourglass Tokenizer (HoT), for efficient transformer-based 3D human pose
estimation from videos. Our HoT begins with pruning pose tokens of redundant
frames and ends with recovering full-length tokens, resulting in a few pose
tokens in the intermediate transformer blocks and thus improving the model
efficiency. To effectively achieve this, we propose a token pruning cluster
(TPC) that dynamically selects a few representative tokens with high semantic
diversity while eliminating the redundancy of video frames. In addition, we
develop a token recovering attention (TRA) to restore the detailed
spatio-temporal information based on the selected tokens, thereby expanding the
network output to the original full-length temporal resolution for fast
inference. Extensive experiments on two benchmark datasets (i.e., Human3.6M and
MPI-INF-3DHP) demonstrate that our method can achieve both high efficiency and
estimation accuracy compared to the original VPT models. For instance, applying
to MotionBERT and MixSTE on Human3.6M, our HoT can save nearly 50% FLOPs
without sacrificing accuracy and nearly 40% FLOPs with only 0.2% accuracy drop,
respectively. Code and models are available at
https://github.com/NationalGAILab/HoT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024, Open Sourced</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Limit Order Book Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09267v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09267v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Briola, Silvia Bartolucci, Tomaso Aste
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We exploit cutting-edge deep learning methodologies to explore the
predictability of high-frequency Limit Order Book mid-price changes for a
heterogeneous set of stocks traded on the NASDAQ exchange. In so doing, we
release `LOBFrame', an open-source code base to efficiently process large-scale
Limit Order Book data and quantitatively assess state-of-the-art deep learning
models' forecasting capabilities. Our results are twofold. We demonstrate that
the stocks' microstructural characteristics influence the efficacy of deep
learning methods and that their high forecasting power does not necessarily
correspond to actionable trading signals. We argue that traditional machine
learning metrics fail to adequately assess the quality of forecasts in the
Limit Order Book context. As an alternative, we propose an innovative
operational framework that evaluates predictions' practicality by focusing on
the probability of accurately forecasting complete transactions. This work
offers academics and practitioners an avenue to make informed and robust
decisions on the application of deep learning techniques, their scope and
limitations, effectively exploiting emergent statistical properties of the
Limit Order Book.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages, 14 figures, 12 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01739v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01739v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu Zhou, Yang You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To help the open-source community have a better understanding of
Mixture-of-Experts (MoE) based large language models (LLMs), we train and
release OpenMoE, a series of fully open-sourced and reproducible decoder-only
MoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T
tokens. Our investigation confirms that MoE-based LLMs can offer a more
favorable cost-effectiveness trade-off than dense LLMs, highlighting the
potential effectiveness for future LLM development.
  One more important contribution of this study is an in-depth analysis of the
routing mechanisms within our OpenMoE models, leading to three significant
findings: Context-Independent Specialization, Early Routing Learning, and
Drop-towards-the-End. We discovered that routing decisions in MoE models are
predominantly based on token IDs, with minimal context relevance. The
token-to-expert assignments are determined early in the pre-training phase and
remain largely unchanged. This imperfect routing can result in performance
degradation, particularly in sequential tasks like multi-turn conversations,
where tokens appearing later in a sequence are more likely to be dropped.
Finally, we rethink our design based on the above-mentioned observations and
analysis. To facilitate future MoE LLM development, we propose potential
strategies for mitigating the issues we found and further improving
off-the-shelf MoE LLM designs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VIGraph: Generative Self-supervised Learning for Class-Imbalanced Node
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01191v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01191v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulan Hu, Sheng Ouyang, Zhirui Yang, Yong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class imbalance in graph data presents significant challenges for node
classification. While existing methods, such as SMOTE-based approaches,
partially mitigate this issue, they still exhibit limitations in constructing
imbalanced graphs. Generative self-supervised learning (SSL) methods,
exemplified by graph autoencoders (GAEs), offer a promising solution by
directly generating minority nodes from the data itself, yet their potential
remains underexplored. In this paper, we delve into the shortcomings of
SMOTE-based approaches in the construction of imbalanced graphs. Furthermore,
we introduce VIGraph, a simple yet effective generative SSL approach that
relies on the Variational GAE as the fundamental model. VIGraph strictly
adheres to the concept of imbalance when constructing imbalanced graphs and
innovatively leverages the variational inference (VI) ability of Variational
GAE to generate nodes for minority classes. VIGraph introduces comprehensive
training strategies, including cross-view contrastive learning at the decoding
phase to capture semantic knowledge, adjacency matrix reconstruction to
preserve graph structure, and alignment strategy to ensure stable training.
VIGraph can generate high-quality nodes directly usable for classification,
eliminating the need to integrate the generated nodes back to the graph as well
as additional retraining found in SMOTE-based methods. We conduct extensive
experiments, results from which demonstrate the superiority and generality of
our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A2V: A Semi-Supervised Domain Adaptation Framework for Brain Vessel
  Segmentation via Two-Phase Training Angiography-to-Venography Translation <span class="chip">BMVC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.06075v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.06075v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Galati, Daniele Falcetta, Rosa Cortese, Barbara Casolla, Ferran Prados, Ninon Burgos, Maria A. Zuluaga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a semi-supervised domain adaptation framework for brain vessel
segmentation from different image modalities. Existing state-of-the-art methods
focus on a single modality, despite the wide range of available cerebrovascular
imaging techniques. This can lead to significant distribution shifts that
negatively impact the generalization across modalities. By relying on annotated
angiographies and a limited number of annotated venographies, our framework
accomplishes image-to-image translation and semantic segmentation, leveraging a
disentangled and semantically rich latent space to represent heterogeneous data
and perform image-level adaptation from source to target domains. Moreover, we
reduce the typical complexity of cycle-based architectures and minimize the use
of adversarial training, which allows us to build an efficient and intuitive
model with stable training. We evaluate our method on magnetic resonance
angiographies and venographies. While achieving state-of-the-art performance in
the source domain, our method attains a Dice score coefficient in the target
domain that is only 8.9% lower, highlighting its promising potential for robust
cerebrovascular image segmentation across different modalities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 34th British Machine Vision Conference (BMVC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Planning to Go Out-of-Distribution in Offline-to-Online <span class="highlight-title">Reinforcement</span>
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05723v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05723v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Trevor McInroe, Adam Jelley, Stefano V. Albrecht, Amos Storkey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline pretraining with a static dataset followed by online fine-tuning
(offline-to-online, or OtO) is a paradigm well matched to a real-world RL
deployment process. In this scenario, we aim to find the best-performing policy
within a limited budget of online interactions. Previous work in the OtO
setting has focused on correcting for bias introduced by the policy-constraint
mechanisms of offline RL algorithms. Such constraints keep the learned policy
close to the behavior policy that collected the dataset, but we show this can
unnecessarily limit policy performance if the behavior policy is far from
optimal. Instead, we forgo constraints and frame OtO RL as an exploration
problem that aims to maximize the benefit of online data-collection. We first
study the major online RL exploration methods based on intrinsic rewards and
UCB in the OtO setting, showing that intrinsic rewards add training instability
through reward-function modification, and UCB methods are myopic and it is
unclear which learned-component's ensemble to use for action selection. We then
introduce an algorithm for planning to go out-of-distribution (PTGOOD) that
avoids these issues. PTGOOD uses a non-myopic planning procedure that targets
exploration in relatively high-reward regions of the state-action space
unlikely to be visited by the behavior policy. By leveraging concepts from the
Conditional Entropy Bottleneck, PTGOOD encourages data collected online to
provide new information relevant to improving the final deployment policy
without altering rewards. We show empirically in several continuous control
tasks that PTGOOD significantly improves agent returns during online
fine-tuning and avoids the suboptimal policy convergence that many of our
baselines exhibit in several environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 17 figures, preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ World Models via <span class="highlight-title">Policy</span>-Guided Trajectory Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.08533v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.08533v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marc Rigter, Jun Yamada, Ingmar Posner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  World models are a powerful tool for developing intelligent agents. By
predicting the outcome of a sequence of actions, world models enable policies
to be optimised via on-policy reinforcement learning (RL) using synthetic data,
i.e. in "in imagination". Existing world models are autoregressive in that they
interleave predicting the next state with sampling the next action from the
policy. Prediction error inevitably compounds as the trajectory length grows.
In this work, we propose a novel world modelling approach that is not
autoregressive and generates entire on-policy trajectories in a single pass
through a diffusion model. Our approach, Policy-Guided Trajectory Diffusion
(PolyGRAD), leverages a denoising model in addition to the gradient of the
action distribution of the policy to diffuse a trajectory of initially random
states and actions into an on-policy synthetic trajectory. We analyse the
connections between PolyGRAD, score-based generative models, and
classifier-guided diffusion models. Our results demonstrate that PolyGRAD
outperforms state-of-the-art baselines in terms of trajectory prediction error
for short trajectories, with the exception of autoregressive diffusion. For
short trajectories, PolyGRAD obtains similar errors to autoregressive
diffusion, but with lower computational requirements. For long trajectories,
PolyGRAD obtains comparable performance to baselines. Our experiments
demonstrate that PolyGRAD enables performant policies to be trained via
on-policy RL in imagination for MuJoCo continuous control domains. Thus,
PolyGRAD introduces a new paradigm for accurate on-policy world modelling
without autoregressive sampling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in TMLR, March 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Emerging Trends in Federated Learning: From Model Fusion to Federated X
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.12920v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.12920v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaoxiong Ji, Yue Tan, Teemu Saravirta, Zhiqin Yang, Yixin Liu, Lauri Vasankari, Shirui Pan, Guodong Long, Anwar Walid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning is a new learning paradigm that decouples data collection
and model training via multi-party computation and model aggregation. As a
flexible learning setting, federated learning has the potential to integrate
with other learning frameworks. We conduct a focused survey of federated
learning in conjunction with other learning algorithms. Specifically, we
explore various learning algorithms to improve the vanilla federated averaging
algorithm and review model fusion methods such as adaptive aggregation,
regularization, clustered methods, and Bayesian methods. Following the emerging
trends, we also discuss federated learning in the intersection with other
learning paradigms, termed federated X learning, where X includes multitask
learning, meta-learning, transfer learning, unsupervised learning, and
reinforcement learning. In addition to reviewing state-of-the-art studies, this
paper also identifies key challenges and applications in this field, while also
highlighting promising future directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the International Journal of Machine Learning and
  Cybernetics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable Non-Cartesian Magnetic Resonance Imaging with R2D2 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17905v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17905v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwei Chen, Chao Tang, Amir Aghabiglou, Chung San Chu, Yves Wiaux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new approach for non-Cartesian magnetic resonance image
reconstruction. While unrolled architectures provide robustness via
data-consistency layers, embedding measurement operators in Deep Neural Network
(DNN) can become impractical at large scale. Alternative Plug-and-Play (PnP)
approaches, where the denoising DNNs are blind to the measurement setting, are
not affected by this limitation and have also proven effective, but their
highly iterative nature also affects scalability. To address this scalability
challenge, we leverage the "Residual-to-Residual DNN series for high-Dynamic
range imaging (R2D2)" approach recently introduced in astronomical imaging.
R2D2's reconstruction is formed as a series of residual images, iteratively
estimated as outputs of DNNs taking the previous iteration's image estimate and
associated data residual as inputs. The method can be interpreted as a learned
version of the Matching Pursuit algorithm. We demonstrate R2D2 in simulation,
considering radial k-space sampling acquisition sequences. Our preliminary
results suggest that R2D2 achieves: (i) suboptimal performance compared to its
unrolled incarnation R2D2-Net, which is however non-scalable due to the
necessary embedding of NUFFT-based data-consistency layers; (ii) superior
reconstruction quality to a scalable version of R2D2-Net embedding an FFT-based
approximation for data consistency; (iii) superior reconstruction quality to
PnP, while only requiring few iterations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IEEE EUSIPCO 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ High Dimensional Distributed Gradient Descent with Arbitrary Number of
  Byzantine Attackers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.13352v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.13352v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Puning Zhao, Zhiguo Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust distributed learning with Byzantine failures has attracted extensive
research interests in recent years. However, most of existing methods suffer
from curse of dimensionality, which is increasingly serious with the growing
complexity of modern machine learning models. In this paper, we design a new
method that is suitable for high dimensional problems, under arbitrary number
of Byzantine attackers. The core of our design is a direct high dimensional
semi-verified mean estimation method. Our idea is to identify a subspace first.
The components of mean value perpendicular to this subspace can be estimated
via gradient vectors uploaded from worker machines, while the components within
this subspace are estimated using auxiliary dataset. We then use our new method
as the aggregator of distributed learning problems. Our theoretical analysis
shows that the new method has minimax optimal statistical rates. In particular,
the dependence on dimensionality is significantly improved compared with
previous works.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Functional Graph Convolutional Networks: A unified multi-task and
  multi-modal learning framework to facilitate health and social-care insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10158v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10158v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobia Boschi, Francesca Bonin, Rodrigo Ordonez-Hurtado, Cécile Rousseau, Alessandra Pascale, John Dinsmore
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel Functional Graph Convolutional Network (funGCN)
framework that combines Functional Data Analysis and Graph Convolutional
Networks to address the complexities of multi-task and multi-modal learning in
digital health and longitudinal studies. With the growing importance of health
solutions to improve health care and social support, ensure healthy lives, and
promote well-being at all ages, funGCN offers a unified approach to handle
multivariate longitudinal data for multiple entities and ensures
interpretability even with small sample sizes. Key innovations include
task-specific embedding components that manage different data types, the
ability to perform classification, regression, and forecasting, and the
creation of a knowledge graph for insightful data interpretation. The efficacy
of funGCN is validated through simulation experiments and a real-data
application.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Concept-Based Causal Transition and Symbolic Reasoning for
  Visual Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03325v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03325v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilue Qian, Peiyu Yu, Ying Nian Wu, Yao Su, Wei Wang, Lifeng Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual planning simulates how humans make decisions to achieve desired goals
in the form of searching for visual causal transitions between an initial
visual state and a final visual goal state. It has become increasingly
important in egocentric vision with its advantages in guiding agents to perform
daily tasks in complex environments. In this paper, we propose an interpretable
and generalizable visual planning framework consisting of i) a novel
Substitution-based Concept Learner (SCL) that abstracts visual inputs into
disentangled concept representations, ii) symbol abstraction and reasoning that
performs task planning via the self-learned symbols, and iii) a Visual Causal
Transition model (ViCT) that grounds visual causal transitions to semantically
similar real-world actions. Given an initial state, we perform goal-conditioned
visual planning with a symbolic reasoning method fueled by the learned
representations and causal transitions to reach the goal state. To verify the
effectiveness of the proposed model, we collect a large-scale visual planning
dataset based on AI2-THOR, dubbed as CCTP. Extensive experiments on this
challenging dataset demonstrate the superior performance of our method in
visual task planning. Empirically, we show that our framework can generalize to
unseen task trajectories, unseen object categories, and real-world data.
Further details of this work are provided at
https://fqyqc.github.io/ConTranPlan/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Centered Masking for Language-Image Pre-Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15837v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15837v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingliang Liang, Martha Larson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Gaussian masking for Language-Image Pre-Training (GLIP) a novel,
straightforward, and effective technique for masking image patches during
pre-training of a vision-language model. GLIP builds on Fast Language-Image
Pre-Training (FLIP), which randomly masks image patches while training a CLIP
model. GLIP replaces random masking with centered masking, that uses a Gaussian
distribution and is inspired by the importance of image patches at the center
of the image. GLIP retains the same computational savings as FLIP, while
improving performance across a range of downstream datasets and tasks, as
demonstrated by our experimental results. We show the benefits of GLIP to be
easy to obtain, requiring no delicate tuning of the Gaussian, and also
applicable to data sets containing images without an obvious center focus.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Asymptotic Bayes risk of semi-supervised learning with uncertain
  labeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17767v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17767v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victor Leger, Romain Couillet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article considers a semi-supervised classification setting on a Gaussian
mixture model, where the data is not labeled strictly as usual, but instead
with uncertain labels. Our main aim is to compute the Bayes risk for this
model. We compare the behavior of the Bayes risk and the best known algorithm
for this model. This comparison eventually gives new insights over the
algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identifying the Correlation Between Language Distance and Cross-Lingual
  Transfer in a Multilingual Representation Space <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.02151v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.02151v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fred Philippy, Siwen Guo, Shohreh Haddadan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior research has investigated the impact of various linguistic features on
cross-lingual transfer performance. In this study, we investigate the manner in
which this effect can be mapped onto the representation space. While past
studies have focused on the impact on cross-lingual alignment in multilingual
language models during fine-tuning, this study examines the absolute evolution
of the respective language representation spaces produced by MLLMs. We place a
specific emphasis on the role of linguistic characteristics and investigate
their inter-correlation with the impact on representation spaces and
cross-lingual transfer performance. Additionally, this paper provides
preliminary evidence of how these findings can be leveraged to enhance transfer
to linguistically distant languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGTYP Workshop 2023 (co-located with EACL 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Physics-embedded Deep Learning Framework for Cloth Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12820v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12820v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiwei Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Delicate cloth simulations have long been desired in computer graphics.
Various methods were proposed to improve engaged force interactions, collision
handling, and numerical integrations. Deep learning has the potential to
achieve fast and real-time simulation, but common neural network structures
often demand many parameters to capture cloth dynamics. This paper proposes a
physics-embedded learning framework that directly encodes physical features of
cloth simulation. The convolutional neural network is used to represent spatial
correlations of the mass-spring system, after which three branches are designed
to learn linear, nonlinear, and time derivate features of cloth physics. The
framework can also integrate with other external forces and collision handling
through either traditional simulators or sub neural networks. The model is
tested across different cloth animation cases, without training with new data.
Agreement with baselines and predictive realism successfully validate its
generalization ability. Inference efficiency of the proposed model also defeats
traditional physics simulation. This framework is also designed to easily
integrate with other visual refinement techniques like wrinkle carving, which
leaves significant chances to incorporate prevailing macing learning techniques
in 3D cloth amination.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A derivation is incomplete, and updations are being processed</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMP++: Motion Manifold Primitives with Parametric Curve Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17072v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17072v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yonghyeon Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motion Manifold Primitives (MMP), a manifold-based approach for encoding
basic motion skills, can produce diverse trajectories, enabling the system to
adapt to unseen constraints. Nonetheless, we argue that current MMP models lack
crucial functionalities of movement primitives, such as temporal and via-points
modulation, found in traditional approaches. This shortfall primarily stems
from MMP's reliance on discrete-time trajectories. To overcome these
limitations, we introduce Motion Manifold Primitives++ (MMP++), a new model
that integrates the strengths of both MMP and traditional methods by
incorporating parametric curve representations into the MMP framework.
Furthermore, we identify a significant challenge with MMP++: performance
degradation due to geometric distortions in the latent space, meaning that
similar motions are not closely positioned. To address this, Isometric Motion
Manifold Primitives++ (IMMP++) is proposed to ensure the latent space
accurately preserves the manifold's geometry. Our experimental results across
various applications, including 2-DoF planar motions, 7-DoF robot arm motions,
and SE(3) trajectory planning, show that MMP++ and IMMP++ outperform existing
methods in trajectory generation tasks, achieving substantial improvements in
some cases. Moreover, they enable the modulation of latent coordinates and
via-points, thereby allowing efficient online adaptation to dynamic
environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages. This work has been submitted to the IEEE for possible
  publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Regret-Based Defense in Adversarial <span class="highlight-title">Reinforcement</span> Learning <span class="chip">AAMAS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.06912v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.06912v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roman Belaire, Pradeep Varakantham, Thanh Nguyen, David Lo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Reinforcement Learning (DRL) policies have been shown to be vulnerable
to small adversarial noise in observations. Such adversarial noise can have
disastrous consequences in safety-critical environments. For instance, a
self-driving car receiving adversarially perturbed sensory observations about
nearby signs (e.g., a stop sign physically altered to be perceived as a speed
limit sign) or objects (e.g., cars altered to be recognized as trees) can be
fatal. Existing approaches for making RL algorithms robust to an
observation-perturbing adversary have focused on reactive approaches that
iteratively improve against adversarial examples generated at each iteration.
While such approaches have been shown to provide improvements over regular RL
methods, they are reactive and can fare significantly worse if certain
categories of adversarial examples are not generated during training. To that
end, we pursue a more proactive approach that relies on directly optimizing a
well-studied robustness measure, regret instead of expected value. We provide a
principled approach that minimizes maximum regret over a "neighborhood" of
observations to the received "observation". Our regret criterion can be used to
modify existing value- and policy-based Deep RL methods. We demonstrate that
our approaches provide a significant improvement in performance across a wide
variety of benchmarks against leading approaches for robust Deep RL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAMAS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Sample</span> Efficient <span class="highlight-title">Reinforcement</span> Learning with Partial Dynamics Knowledge <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.12558v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.12558v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meshal Alharbi, Mardavij Roozbehani, Munther Dahleh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The problem of sample complexity of online reinforcement learning is often
studied in the literature without taking into account any partial knowledge
about the system dynamics that could potentially accelerate the learning
process. In this paper, we study the sample complexity of online Q-learning
methods when some prior knowledge about the dynamics is available or can be
learned efficiently. We focus on systems that evolve according to an additive
disturbance model of the form $S_{h+1} = f(S_h, A_h) + W_h$, where $f$
represents the underlying system dynamics, and $W_h$ are unknown disturbances
independent of states and actions. In the setting of finite episodic Markov
decision processes with $S$ states, $A$ actions, and episode length $H$, we
present an optimistic Q-learning algorithm that achieves
$\tilde{\mathcal{O}}(\text{Poly}(H)\sqrt{T})$ regret under perfect knowledge of
$f$, where $T$ is the total number of interactions with the system. This is in
contrast to the typical $\tilde{\mathcal{O}}(\text{Poly}(H)\sqrt{SAT})$ regret
for existing Q-learning methods. Further, if only a noisy estimate $\hat{f}$ of
$f$ is available, our method can learn an approximately optimal policy in a
number of samples that is independent of the cardinalities of state and action
spaces. The sub-optimality gap depends on the approximation error $\hat{f}-f$,
as well as the Lipschitz constant of the corresponding optimal value function.
Our approach does not require modeling of the transition probabilities and
enjoys the same memory complexity as model-free methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in the 38th Annual AAAI Conference on Artificial
  Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Weakly Supervised AUC Optimization: A Unified Partial AUC Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14258v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14258v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Xie, Yu Liu, Hao-Yuan He, Ming Li, Zhi-Hua Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since acquiring perfect supervision is usually difficult, real-world machine
learning tasks often confront inaccurate, incomplete, or inexact supervision,
collectively referred to as weak supervision. In this work, we present WSAUC, a
unified framework for weakly supervised AUC optimization problems, which covers
noisy label learning, positive-unlabeled learning, multi-instance learning, and
semi-supervised learning scenarios. Within the WSAUC framework, we first frame
the AUC optimization problems in various weakly supervised scenarios as a
common formulation of minimizing the AUC risk on contaminated sets, and
demonstrate that the empirical risk minimization problems are consistent with
the true AUC. Then, we introduce a new type of partial AUC, specifically, the
reversed partial AUC (rpAUC), which serves as a robust training objective for
AUC maximization in the presence of contaminated labels. WSAUC offers a
universal solution for AUC optimization in various weakly supervised scenarios
by maximizing the empirical rpAUC. Theoretical and experimental results under
multiple settings support the effectiveness of WSAUC on a range of weakly
supervised AUC optimization tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE TPAMI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Foundation Model Makes Clustering A Better Initialization For Cold-Start
  Active Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02561v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02561v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Yuan, Chuan Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Active learning selects the most informative samples from the unlabelled
dataset to annotate in the context of a limited annotation budget. While
numerous methods have been proposed for subsequent sample selection based on an
initialized model, scant attention has been paid to the indispensable phase of
active learning: selecting samples for model cold-start initialization. Most of
the previous studies resort to random sampling or naive clustering. However,
random sampling is prone to fluctuation, and naive clustering suffers from
convergence speed, particularly when dealing with high-dimensional data such as
imaging data. In this work, we propose to integrate foundation models with
clustering methods to select samples for cold-start active learning
initialization. Foundation models refer to those trained on massive datasets by
the self-supervised paradigm and capable of generating informative and
compacted embeddings for various downstream tasks. Leveraging these embeddings
to replace raw features such as pixel values, clustering quickly converges and
identifies better initial samples. For a comprehensive comparison, we included
a classic ImageNet-supervised model to acquire embeddings. Experiments on two
clinical tasks of image classification and segmentation demonstrated that
foundation model-based clustering efficiently pinpointed informative initial
samples, leading to models showcasing enhanced performance than the baseline
methods. We envisage that this study provides an effective paradigm for future
cold-start active learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Expectations Versus Reality: Evaluating Intrusion Detection Systems in
  Practice 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17458v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17458v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jake Hesford, Daniel Cheng, Alan Wan, Larry Huynh, Seungho Kim, Hyoungshick Kim, Jin B. Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our paper provides empirical comparisons between recent IDSs to provide an
objective comparison between them to help users choose the most appropriate
solution based on their requirements. Our results show that no one solution is
the best, but is dependent on external variables such as the types of attacks,
complexity, and network environment in the dataset. For example, BoT_IoT and
Stratosphere IoT datasets both capture IoT-related attacks, but the deep neural
network performed the best when tested using the BoT_IoT dataset while HELAD
performed the best when tested using the Stratosphere IoT dataset. So although
we found that a deep neural network solution had the highest average F1 scores
on tested datasets, it is not always the best-performing one. We further
discuss difficulties in using IDS from literature and project repositories,
which complicated drawing definitive conclusions regarding IDS selection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CBQ: Cross-Block Quantization for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.07950v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.07950v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Ding, Xiaoyu Liu, Zhijun Tu, Yun Zhang, Wei Li, Jie Hu, Hanting Chen, Yehui Tang, Zhiwei Xiong, Baoqun Yin, Yunhe Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Post-training quantization (PTQ) has played a key role in compressing large
language models (LLMs) with ultra-low costs. However, existing PTQ methods only
focus on handling the outliers within one layer or one block, which ignores the
dependency of blocks and leads to severe performance degradation in low-bit
settings. In this paper, we propose CBQ, a cross-block reconstruction-based PTQ
method for LLMs. CBQ employs a cross-block dependency using a homologous
reconstruction scheme, establishing long-range dependencies across multiple
blocks to minimize error accumulation. Furthermore, CBQ incorporates a
coarse-to-fine preprocessing (CFP) strategy for suppressing weight and
activation outliers, coupled with an adaptive LoRA-Rounding technique for
precise weight quantization. These innovations enable CBQ to not only handle
extreme outliers effectively but also improve overall quantization accuracy.
Extensive experiments show that CBQ achieves superior low-bit quantization
(W4A4, W4A8, W2A16) and outperforms existing state-of-the-art methods across
various LLMs and datasets. Notably, CBQ quantizes the 4-bit LLAMA1-65B model
within only 4.3 hours on a single GPU, achieving a commendable tradeoff between
performance and quantization efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simple <span class="highlight-title">Policy</span> Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.16025v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.16025v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengpeng Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  PPO (Proximal Policy Optimization) algorithm has demonstrated excellent
performance in many fields, and it is considered as a simple version of TRPO
(Trust Region Policy Optimization) algorithm. However, the ratio clipping
operation in PPO may not always effectively enforce the trust region
constraints, this can be a potential factor affecting the stability of the
algorithm. In this paper, we propose Simple Policy Optimization (SPO)
algorithm, which introduces a novel clipping method for KL divergence between
the old and current policies. Extensive experimental results in Atari 2600
environments indicate that, compared to the mainstream variants of PPO, SPO
achieves better sample efficiency, extremely low KL divergence, and higher
policy entropy, and is robust to the increase in network depth or complexity.
More importantly, SPO maintains the simplicity of an unconstrained first-order
algorithm. Code is available at
https://github.com/MyRepositories-hub/Simple-Policy-Optimization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BridgeTower: Building Bridges Between Encoders in Vision-Language
  Representation Learning <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.08657v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.08657v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Xu, Chenfei Wu, Shachar Rosenman, Vasudev Lal, Wanxiang Che, Nan Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language (VL) models with the Two-Tower architecture have dominated
visual-language representation learning in recent years. Current VL models
either use lightweight uni-modal encoders and learn to extract, align and fuse
both modalities simultaneously in a deep cross-modal encoder, or feed the
last-layer uni-modal representations from the deep pre-trained uni-modal
encoders into the top cross-modal encoder. Both approaches potentially restrict
vision-language representation learning and limit model performance. In this
paper, we propose BridgeTower, which introduces multiple bridge layers that
build a connection between the top layers of uni-modal encoders and each layer
of the cross-modal encoder. This enables effective bottom-up cross-modal
alignment and fusion between visual and textual representations of different
semantic levels of pre-trained uni-modal encoders in the cross-modal encoder.
Pre-trained with only 4M images, BridgeTower achieves state-of-the-art
performance on various downstream vision-language tasks. In particular, on the
VQAv2 test-std set, BridgeTower achieves an accuracy of 78.73%, outperforming
the previous state-of-the-art model METER by 1.09% with the same pre-training
data and almost negligible additional parameters and computational costs.
Notably, when further scaling the model, BridgeTower achieves an accuracy of
81.15%, surpassing models that are pre-trained on orders-of-magnitude larger
datasets. Code and checkpoints are available at
https://github.com/microsoft/BridgeTower.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2023, Oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discovering and Mitigating Visual Biases through Keyword Explanation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.11104v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.11104v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Younghyun Kim, Sangwoo Mo, Minkyu Kim, Kyungmin Lee, Jaeho Lee, Jinwoo Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Addressing biases in computer vision models is crucial for real-world AI
deployments. However, mitigating visual biases is challenging due to their
unexplainable nature, often identified indirectly through visualization or
sample statistics, which necessitates additional human supervision for
interpretation. To tackle this issue, we propose the Bias-to-Text (B2T)
framework, which interprets visual biases as keywords. Specifically, we extract
common keywords from the captions of mispredicted images to identify potential
biases in the model. We then validate these keywords by measuring their
similarity to the mispredicted images using a vision-language scoring model.
The keyword explanation form of visual bias offers several advantages, such as
a clear group naming for bias discovery and a natural extension for debiasing
using these group names. Our experiments demonstrate that B2T can identify
known biases, such as gender bias in CelebA, background bias in Waterbirds, and
distribution shifts in ImageNet-R/C. Additionally, B2T uncovers novel biases in
larger datasets, such as Dollar Street and ImageNet. For example, we discovered
a contextual bias between "bee" and "flower" in ImageNet. We also highlight
various applications of B2T keywords, including debiased training, CLIP
prompting, and model comparison.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024. First two authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CAFE: Towards Compact, Adaptive, and Fast Embedding for Large-scale
  Recommendation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.03256v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.03256v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hailin Zhang, Zirui Liu, Boxuan Chen, Yikai Zhao, Tong Zhao, Tong Yang, Bin Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the growing memory demands of embedding tables in Deep Learning
Recommendation Models (DLRMs) pose great challenges for model training and
deployment. Existing embedding compression solutions cannot simultaneously meet
three key design requirements: memory efficiency, low latency, and adaptability
to dynamic data distribution. This paper presents CAFE, a Compact, Adaptive,
and Fast Embedding compression framework that addresses the above requirements.
The design philosophy of CAFE is to dynamically allocate more memory resources
to important features (called hot features), and allocate less memory to
unimportant ones. In CAFE, we propose a fast and lightweight sketch data
structure, named HotSketch, to capture feature importance and report hot
features in real time. For each reported hot feature, we assign it a unique
embedding. For the non-hot features, we allow multiple features to share one
embedding by using hash embedding technique. Guided by our design philosophy,
we further propose a multi-level hash embedding framework to optimize the
embedding tables of non-hot features. We theoretically analyze the accuracy of
HotSketch, and analyze the model convergence against deviation. Extensive
experiments show that CAFE significantly outperforms existing embedding
compression methods, yielding 3.92% and 3.68% superior testing AUC on Criteo
Kaggle dataset and CriteoTB dataset at a compression ratio of 10000x. The
source codes of CAFE are available at GitHub.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Assessing the overall and partial causal well-specification of nonlinear
  additive noise models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16502v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16502v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christoph Schultheiss, Peter Bühlmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a method to detect model misspecifications in nonlinear causal
additive and potentially heteroscedastic noise models. We aim to identify
predictor variables for which we can infer the causal effect even in cases of
such misspecification. We develop a general framework based on knowledge of the
multivariate observational data distribution. We then propose an algorithm for
finite sample data, discuss its asymptotic properties, and illustrate its
performance on simulated and real data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shotgun crystal structure prediction using machine-learned formation
  energies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.02158v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.02158v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Liu, Hiromasa Tamaki, Tomoyasu Yokoyama, Kensuke Wakasugi, Satoshi Yotsuhashi, Minoru Kusaba, Ryo Yoshida
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stable or metastable crystal structures of assembled atoms can be predicted
by finding the global or local minima of the energy surface defined on the
space of the atomic configurations. Generally, this requires repeated
first-principles energy calculations that are impractical for large systems,
such as those containing more than 30 atoms in the unit cell. Here, we have
made significant progress in solving the crystal structure prediction problem
with a simple but powerful machine-learning workflow; using a machine-learning
surrogate for first-principles energy calculations, we performed non-iterative,
single-shot screening using a large library of virtually created crystal
structures. The present method relies on two key technical components: transfer
learning, which enables a highly accurate energy prediction of pre-relaxed
crystalline states given only a small set of training samples from
first-principles calculations, and generative models to create promising and
diverse crystal structures for screening. Here, first-principles calculations
were performed only to generate the training samples, and for the optimization
of a dozen or fewer finally narrowed-down crystal structures. Our shotgun
method proved to be computationally less demanding compared to conventional
methods, which heavily rely on iterations of first-principles calculations, and
achieved an exceptional prediction accuracy, reaching 92.2% in a benchmark task
involving the prediction of 90 different crystal structures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Frequentist Guarantees of Distributed (Non)-Bayesian Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08214v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08214v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bohan Wu, César A. Uribe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by the need to analyze large, decentralized datasets, distributed
Bayesian inference has become a critical research area across multiple fields,
including statistics, electrical engineering, and economics. This paper
establishes Frequentist properties, such as posterior consistency, asymptotic
normality, and posterior contraction rates, for the distributed (non-)Bayes
Inference problem among agents connected via a communication network. Our
results show that, under appropriate assumptions on the communication graph,
distributed Bayesian inference retains parametric efficiency while enhancing
robustness in uncertainty quantification. We also explore the trade-off between
statistical efficiency and communication efficiency by examining how the design
and size of the communication graph impact the posterior contraction rate.
Furthermore, We extend our analysis to time-varying graphs and apply our
results to exponential family models, distributed logistic regression, and
decentralized detection models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NLP-based detection of systematic anomalies among the narratives of
  consumer complaints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.11138v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.11138v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peiheng Gao, Ning Sun, Xuefeng Wang, Chen Yang, Ričardas Zitikis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop an NLP-based procedure for detecting systematic nonmeritorious
consumer complaints, simply called systematic anomalies, among complaint
narratives. While classification algorithms are used to detect pronounced
anomalies, in the case of smaller and frequent systematic anomalies, the
algorithms may falter due to a variety of reasons, including technical ones as
well as natural limitations of human analysts. Therefore, as the next step
after classification, we convert the complaint narratives into quantitative
data, which are then analyzed using an algorithm for detecting systematic
anomalies. We illustrate the entire procedure using complaint narratives from
the Consumer Complaint Database of the Consumer Financial Protection Bureau.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Many-Objective Evolutionary Influence Maximization: Balancing Spread,
  Budget, Fairness, and Time <span class="chip">GECCO
  24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18755v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18755v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elia Cunegatti, Leonardo Lucio Custode, Giovanni Iacca
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Influence Maximization (IM) problem seeks to discover the set of nodes in
a graph that can spread the information propagation at most. This problem is
known to be NP-hard, and it is usually studied by maximizing the influence
(spread) and, optionally, optimizing a second objective, such as minimizing the
seed set size or maximizing the influence fairness. However, in many practical
scenarios multiple aspects of the IM problem must be optimized at the same
time. In this work, we propose a first case study where several IM-specific
objective functions, namely budget, fairness, communities, and time, are
optimized on top of the maximization of influence and minimization of the seed
set size. To this aim, we introduce MOEIM (Many-Objective Evolutionary
Algorithm for Influence Maximization) a Multi-Objective Evolutionary Algorithm
(MOEA) based on NSGA-II incorporating graph-aware operators and a smart
initialization. We compare MOEIM in two experimental settings, including a
total of nine graph datasets, two heuristic methods, a related MOEA, and a
state-of-the-art Deep Learning approach. The experiments show that MOEIM
overall outperforms the competitors in most of the tested many-objective
settings. To conclude, we also investigate the correlation between the
objectives, leading to novel insights into the topic. The codebase is available
at https://github.com/eliacunegatti/MOEIM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in Genetic and Evolutionary Computation Conference (GECCO
  24 Companion), July 14 18, 2024, Melbourne, VIC, Australia. ACM, New York,
  NY, USA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">survey</span> on learning models of spiking neural membrane systems and
  spiking neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prithwineel Paul, Petr Sosik, Lucie Ciencialova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking neural networks (SNN) are a biologically inspired model of neural
networks with certain brain-like properties. In the past few decades, this
model has received increasing attention in computer science community, owing
also to the successful phenomenon of deep learning. In SNN, communication
between neurons takes place through the spikes and spike trains. This
differentiates these models from the ``standard'' artificial neural networks
(ANN) where the frequency of spikes is replaced by real-valued signals. Spiking
neural P systems (SNPS) can be considered a branch of SNN based more on the
principles of formal automata, with many variants developed within the
framework of the membrane computing theory. In this paper, we first briefly
compare structure and function, advantages and drawbacks of SNN and SNPS. A key
part of the article is a survey of recent results and applications of machine
learning and deep learning models of both SNN and SNPS formalisms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fourier or Wavelet bases as counterpart self-attention in spikformer for
  efficient visual classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18228v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18228v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyu Wang, Duzhen Zhang, Tilelin Zhang, Bo Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Energy-efficient spikformer has been proposed by integrating the biologically
plausible spiking neural network (SNN) and artificial Transformer, whereby the
Spiking Self-Attention (SSA) is used to achieve both higher accuracy and lower
computational cost. However, it seems that self-attention is not always
necessary, especially in sparse spike-form calculation manners. In this paper,
we innovatively replace vanilla SSA (using dynamic bases calculating from Query
and Key) with spike-form Fourier Transform, Wavelet Transform, and their
combinations (using fixed triangular or wavelets bases), based on a key
hypothesis that both of them use a set of basis functions for information
transformation. Hence, the Fourier-or-Wavelet-based spikformer (FWformer) is
proposed and verified in visual classification tasks, including both static
image and event-based video datasets. The FWformer can achieve comparable or
even higher accuracies ($0.4\%$-$1.5\%$), higher running speed ($9\%$-$51\%$
for training and $19\%$-$70\%$ for inference), reduced theoretical energy
consumption ($20\%$-$25\%$), and reduced GPU memory usage ($4\%$-$26\%$),
compared to the standard spikformer. Our result indicates the continuous
refinement of new Transformers, that are inspired either by biological
discovery (spike-form), or information theory (Fourier or Wavelet Transform),
is promising.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 2 figures. arXiv admin note: substantial text overlap with
  arXiv:2308.02557</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Evolutionary Network Architecture Search Framework with Adaptive
  Multimodal Fusion for Hand Gesture Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhang Xia, Shihao Song, Zhanglu Hou, Junwen Xu, Juan Zou, Yuan Liu, Shengxiang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hand gesture recognition (HGR) based on multimodal data has attracted
considerable attention owing to its great potential in applications. Various
manually designed multimodal deep networks have performed well in multimodal
HGR (MHGR), but most of existing algorithms require a lot of expert experience
and time-consuming manual trials. To address these issues, we propose an
evolutionary network architecture search framework with the adaptive multimodel
fusion (AMF-ENAS). Specifically, we design an encoding space that
simultaneously considers fusion positions and ratios of the multimodal data,
allowing for the automatic construction of multimodal networks with different
architectures through decoding. Additionally, we consider three input streams
corresponding to intra-modal surface electromyography (sEMG), intra-modal
accelerometer (ACC), and inter-modal sEMG-ACC. To automatically adapt to
various datasets, the ENAS framework is designed to automatically search a MHGR
network with appropriate fusion positions and ratios. To the best of our
knowledge, this is the first time that ENAS has been utilized in MHGR to tackle
issues related to the fusion position and ratio of multimodal data.
Experimental results demonstrate that AMF-ENAS achieves state-of-the-art
performance on the Ninapro DB2, DB3, and DB7 datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Cyber Response Time on Temporal Active Directory Networks
  Using Decoys <span class="chip">GECCO 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18162v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18162v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huy Q. Ngo, Mingyu Guo, Hung Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Microsoft Active Directory (AD) is the default security management system for
Window domain network. We study the problem of placing decoys in AD network to
detect potential attacks. We model the problem as a Stackelberg game between an
attacker and a defender on AD attack graphs where the defender employs a set of
decoys to detect the attacker on their way to Domain Admin (DA). Contrary to
previous works, we consider time-varying (temporal) attack graphs. We proposed
a novel metric called response time, to measure the effectiveness of our decoy
placement in temporal attack graphs. Response time is defined as the duration
from the moment attackers trigger the first decoy to when they compromise the
DA. Our goal is to maximize the defender's response time to the worst-case
attack paths. We establish the NP-hard nature of the defender's optimization
problem, leading us to develop Evolutionary Diversity Optimization (EDO)
algorithms. EDO algorithms identify diverse sets of high-quality solutions for
the optimization problem. Despite the polynomial nature of the fitness
function, it proves experimentally slow for larger graphs. To enhance
scalability, we proposed an algorithm that exploits the static nature of AD
infrastructure in the temporal setting. Then, we introduce tailored repair
operations, ensuring the convergence to better results while maintaining
scalability for larger graphs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be appear in ACM GECCO 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-26T00:00:00Z">2024-03-26</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal
  Propagation Analysis for Large Language Models <span class="chip">ICLR
  2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18159v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18159v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kartikeya Bhardwaj, Nilesh Prasad Pandey, Sweta Priyadarshi, Kyunggeun Lee, Jun Ma, Harris Teague
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large generative models, such as large language models (LLMs) and diffusion
models have as revolutionized the fields of NLP and computer vision
respectively. However, their slow inference, high computation and memory
requirement makes it challenging to deploy them on edge devices. In this study,
we propose a light-weight quantization aware fine tuning technique using
knowledge distillation (KD-QAT) to improve the performance of 4-bit weight
quantized LLMs using commonly available datasets to realize a popular language
use case, on device chat applications. To improve this paradigm of finetuning,
as main contributions, we provide insights into stability of KD-QAT by
empirically studying the gradient propagation during training to better
understand the vulnerabilities of KD-QAT based approaches to low-bit
quantization errors. Based on our insights, we propose ov-freeze, a simple
technique to stabilize the KD-QAT process. Finally, we experiment with the
popular 7B LLaMAv2-Chat model at 4-bit quantization level and demonstrate that
ov-freeze results in near float-point precision performance, i.e., less than
0.7% loss of accuracy on Commonsense Reasoning benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Practical ML for Low Resource Settings Workshop at ICLR
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models Produce Responses Perceived to be Empathic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18148v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18148v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoon Kyung Lee, Jina Suh, Hongli Zhan, Junyi Jessy Li, Desmond C. Ong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated surprising performance on many
tasks, including writing supportive messages that display empathy. Here, we had
these models generate empathic messages in response to posts describing common
life experiences, such as workplace situations, parenting, relationships, and
other anxiety- and anger-eliciting situations. Across two studies (N=192, 202),
we showed human raters a variety of responses written by several models (GPT4
Turbo, Llama2, and Mistral), and had people rate these responses on how
empathic they seemed to be. We found that LLM-generated responses were
consistently rated as more empathic than human-written responses. Linguistic
analyses also show that these models write in distinct, predictable ``styles",
in terms of their use of punctuation, emojis, and certain words. These results
highlight the potential of using LLMs to enhance human peer support in contexts
where empathy is important.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Real-Time Rescheduling Algorithm for Multi-robot Plan Execution <span class="chip">ICAPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18145v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18145v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying Feng, Adittyo Paul, Zhe Chen, Jiaoyang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One area of research in multi-agent path finding is to determine how
replanning can be efficiently achieved in the case of agents being delayed
during execution. One option is to reschedule the passing order of agents,
i.e., the sequence in which agents visit the same location. In response, we
propose Switchable-Edge Search (SES), an A*-style algorithm designed to find
optimal passing orders. We prove the optimality of SES and evaluate its
efficiency via simulations. The best variant of SES takes less than 1 second
for small- and medium-sized problems and runs up to 4 times faster than
baselines for large-sized problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICAPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Juru: Legal Brazilian Large Language Model from Reputable Sources 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18140v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18140v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roseval Malaquias Junior, Ramon Pires, Roseli Romero, Rodrigo Nogueira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The high computational cost associated with pretraining large language models
limits their research. Two strategies have emerged to address this issue:
domain specialization and pretraining with high-quality data. To explore these
strategies, we specialized the Sabi\'a-2 Small model with 1.9 billion unique
tokens from reputable Brazilian legal sources and conducted few-shot
evaluations on legal and general knowledge exams. Our model, Juru, demonstrates
the benefits of domain specialization with a reduced amount of pretraining
data. However, this specialization comes at the expense of degrading
performance in other knowledge areas within the same language. This study
contributes to the growing body of scientific evidence showing that pretraining
data selection may enhance the performance of large language models, enabling
the exploration of these models at a lower cost.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Securing GNNs: Explanation-Based Identification of Backdoored Training
  Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18136v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18136v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jane Downer, Ren Wang, Binghui Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have gained popularity in numerous domains, yet
they are vulnerable to backdoor attacks that can compromise their performance
and ethical application. The detection of these attacks is crucial for
maintaining the reliability and security of GNN classification tasks, but
effective detection techniques are lacking. Following an initial investigation,
we observed that while graph-level explanations can offer limited insights,
their effectiveness in detecting backdoor triggers is inconsistent and
incomplete. To bridge this gap, we extract and transform secondary outputs of
GNN explanation mechanisms, designing seven novel metrics that more effectively
detect backdoor attacks. Additionally, we develop an adaptive attack to
rigorously evaluate our approach. We test our method on multiple benchmark
datasets and examine its efficacy against various attack models. Our results
show that our method can achieve high detection performance, marking a
significant advancement in safeguarding GNNs against backdoor attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AE SemRL: Learning Semantic Association Rules with Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18133v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18133v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erkan Karabulut, Victoria Degeler, Paul Groth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Association Rule Mining (ARM) is the task of learning associations among data
features in the form of logical rules. Mining association rules from
high-dimensional numerical data, for example, time series data from a large
number of sensors in a smart environment, is a computationally intensive task.
In this study, we propose an Autoencoder-based approach to learn and extract
association rules from time series data (AE SemRL). Moreover, we argue that in
the presence of semantic information related to time series data sources,
semantics can facilitate learning generalizable and explainable association
rules. Despite enriching time series data with additional semantic features, AE
SemRL makes learning association rules from high-dimensional data feasible. Our
experiments show that semantic association rules can be extracted from a latent
representation created by an Autoencoder and this method has in the order of
hundreds of times faster execution time than state-of-the-art ARM approaches in
many scenarios. We believe that this study advances a new way of extracting
associations from representations and has the potential to inspire more
research in this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recommendation of data-free class-incremental learning algorithms by
  simulating future data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18132v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18132v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eva Feillet, Adrian Popescu, Céline Hudelot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class-incremental learning deals with sequential data streams composed of
batches of classes. Various algorithms have been proposed to address the
challenging case where samples from past classes cannot be stored. However,
selecting an appropriate algorithm for a user-defined setting is an open
problem, as the relative performance of these algorithms depends on the
incremental settings. To solve this problem, we introduce an algorithm
recommendation method that simulates the future data stream. Given an initial
set of classes, it leverages generative models to simulate future classes from
the same visual domain. We evaluate recent algorithms on the simulated stream
and recommend the one which performs best in the user-defined incremental
setting. We illustrate the effectiveness of our method on three large datasets
using six algorithms and six incremental settings. Our method outperforms
competitive baselines, and performance is close to that of an oracle choosing
the best algorithm in each setting. This work contributes to facilitate the
practical deployment of incremental learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with
  Autoformalization <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18120v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18120v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Peng Zhou, Charles Staats, Wenda Li, Christian Szegedy, Kilian Q. Weinberger, Yuhuai Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLM), such as Google's Minerva and OpenAI's GPT
families, are becoming increasingly capable of solving mathematical
quantitative reasoning problems. However, they still make unjustified logical
and computational errors in their reasoning steps and answers. In this paper,
we leverage the fact that if the training corpus of LLMs contained sufficiently
many examples of formal mathematics (e.g. in Isabelle, a formal theorem proving
environment), they can be prompted to translate i.e. autoformalize informal
mathematical statements into formal Isabelle code -- which can be verified
automatically for internal consistency. This provides a mechanism to
automatically reject solutions whose formalized versions are inconsistent
within themselves or with the formalized problem statement. We evaluate our
method on GSM8K, MATH and MultiArith datasets and demonstrate that our approach
provides a consistently better heuristic than vanilla majority voting -- the
previously best method to identify correct answers, by more than 12% on GSM8K.
In our experiments it improves results consistently across all datasets and LLM
model sizes. The code can be found at https://github.com/jinpz/dtv.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models as Financial Data Annotators: A Study on
  Effectiveness and Efficiency <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18152v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18152v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Toyin Aguda, Suchetha Siddagangappa, Elena Kochkina, Simerjot Kaur, Dongsheng Wang, Charese Smiley, Sameena Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collecting labeled datasets in finance is challenging due to scarcity of
domain experts and higher cost of employing them. While Large Language Models
(LLMs) have demonstrated remarkable performance in data annotation tasks on
general domain datasets, their effectiveness on domain specific datasets
remains underexplored. To address this gap, we investigate the potential of
LLMs as efficient data annotators for extracting relations in financial
documents. We compare the annotations produced by three LLMs (GPT-4, PaLM 2,
and MPT Instruct) against expert annotators and crowdworkers. We demonstrate
that the current state-of-the-art LLMs can be sufficient alternatives to
non-expert crowdworkers. We analyze models using various prompts and parameter
settings and find that customizing the prompts for each relation group by
providing specific examples belonging to those groups is paramount.
Furthermore, we introduce a reliability index (LLM-RelIndex) used to identify
outputs that may require expert attention. Finally, we perform an extensive
time, cost and error analysis and provide recommendations for the collection
and usage of automated annotations in domain-specific settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ For those who don't know (how) to ask: Building a <span class="highlight-title">dataset</span> of technology
  questions for digital newcomers <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18125v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18125v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evan Lucas, Kelly S. Steelman, Leo C. Ureel, Charles Wallace
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While the rise of large language models (LLMs) has created rich new
opportunities to learn about digital technology, many on the margins of this
technology struggle to gain and maintain competency due to lexical or
conceptual barriers that prevent them from asking appropriate questions.
Although there have been many efforts to understand factuality of LLM-created
content and ability of LLMs to answer questions, it is not well understood how
unclear or nonstandard language queries affect the model outputs. We propose
the creation of a dataset that captures questions of digital newcomers and
outsiders, utilizing data we have compiled from a decade's worth of one-on-one
tutoring. In this paper we lay out our planned efforts and some potential uses
of this dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the AI4ED workshop at AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChatGPT Role-play <span class="highlight-title">Dataset</span>: Analysis of User Motives and Model
  Naturalness <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18121v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18121v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yufei Tao, Ameeta Agrawal, Judit Dombi, Tetyana Sydorenko, Jung In Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in interactive large language models like ChatGPT have
revolutionized various domains; however, their behavior in natural and
role-play conversation settings remains underexplored. In our study, we address
this gap by deeply investigating how ChatGPT behaves during conversations in
different settings by analyzing its interactions in both a normal way and a
role-play setting. We introduce a novel dataset of broad range of human-AI
conversations annotated with user motives and model naturalness to examine (i)
how humans engage with the conversational AI model, and (ii) how natural are AI
model responses. Our study highlights the diversity of user motives when
interacting with ChatGPT and variable AI naturalness, showing not only the
nuanced dynamics of natural conversations between humans and AI, but also
providing new avenues for improving the effectiveness of human-AI
communication.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models for Education: A <span class="highlight-title">Survey</span> and Outlook 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18105v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18105v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shen Wang, Tianlong Xu, Hang Li, Chaoli Zhang, Joleen Liang, Jiliang Tang, Philip S. Yu, Qingsong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of Large Language Models (LLMs) has brought in a new era of
possibilities in the realm of education. This survey paper summarizes the
various technologies of LLMs in educational settings from multifaceted
perspectives, encompassing student and teacher assistance, adaptive learning,
and commercial tools. We systematically review the technological advancements
in each perspective, organize related datasets and benchmarks, and identify the
risks and challenges associated with deploying LLMs in education. Furthermore,
we outline future research opportunities, highlighting the potential promising
directions. Our survey aims to provide a comprehensive technological picture
for educators, researchers, and policymakers to harness the power of LLMs to
revolutionize educational practices and foster a more effective personalized
learning environment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GPTs and Language Barrier: A Cross-Lingual Legal QA Examination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18098v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18098v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ha-Thanh Nguyen, Hiroaki Yamada, Ken Satoh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we explore the application of Generative Pre-trained
Transformers (GPTs) in cross-lingual legal Question-Answering (QA) systems
using the COLIEE Task 4 dataset. In the COLIEE Task 4, given a statement and a
set of related legal articles that serve as context, the objective is to
determine whether the statement is legally valid, i.e., if it can be inferred
from the provided contextual articles or not, which is also known as an
entailment task. By benchmarking four different combinations of English and
Japanese prompts and data, we provide valuable insights into GPTs' performance
in multilingual legal QA scenarios, contributing to the development of more
efficient and accurate cross-lingual QA solutions in the legal domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NLP 2024, Kobe, Japan</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Legal Document Retrieval: A Multi-Phase Approach with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18093v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18093v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hai-Long Nguyen, Duc-Minh Nguyen, Tan-Minh Nguyen, Ha-Thanh Nguyen, Thi-Hai-Yen Vuong, Ken Satoh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models with billions of parameters, such as GPT-3.5, GPT-4,
and LLaMA, are increasingly prevalent. Numerous studies have explored effective
prompting techniques to harness the power of these LLMs for various research
problems. Retrieval, specifically in the legal data domain, poses a challenging
task for the direct application of Prompting techniques due to the large number
and substantial length of legal articles. This research focuses on maximizing
the potential of prompting by placing it as the final phase of the retrieval
system, preceded by the support of two phases: BM25 Pre-ranking and BERT-based
Re-ranking. Experiments on the COLIEE 2023 dataset demonstrate that integrating
prompting techniques on LLMs into the retrieval system significantly improves
retrieval accuracy. However, error analysis reveals several existing issues in
the retrieval system that still need resolution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>JURISIN 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spectral Convolutional Transformer: Harmonizing Real vs. Complex
  Multi-View Spectral Operators for Vision Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18063v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18063v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Badri N. Patro, Vinay P. Namboodiri, Vijay S. Agneeswaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers used in vision have been investigated through diverse
architectures - ViT, PVT, and Swin. These have worked to improve the attention
mechanism and make it more efficient. Differently, the need for including local
information was felt, leading to incorporating convolutions in transformers
such as CPVT and CvT. Global information is captured using a complex Fourier
basis to achieve global token mixing through various methods, such as AFNO,
GFNet, and Spectformer. We advocate combining three diverse views of data -
local, global, and long-range dependence. We also investigate the simplest
global representation using only the real domain spectral representation -
obtained through the Hartley transform. We use a convolutional operator in the
initial layers to capture local information. Through these two contributions,
we are able to optimize and obtain a spectral convolution transformer (SCT)
that provides improved performance over the state-of-the-art methods while
reducing the number of parameters. Through extensive experiments, we show that
SCT-C-small gives state-of-the-art performance on the ImageNet dataset and
reaches 84.5\% top-1 accuracy, while SCT-C-Large reaches 85.9\% and SCT-C-Huge
reaches 86.4\%. We evaluate SCT on transfer learning on datasets such as
CIFAR-10, CIFAR-100, Oxford Flower, and Stanford Car. We also evaluate SCT on
downstream tasks i.e. instance segmentation on the MSCOCO dataset. The project
page is available on this webpage.\url{https://github.com/badripatro/sct}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18058v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18058v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuelin Bai, Xinrun Du, Yiming Liang, Yonggang Jin, Ziqiang Liu, Junting Zhou, Tianyu Zheng, Xincheng Zhang, Nuo Ma, Zekun Wang, Ruibin Yuan, Haihong Wu, Hongquan Lin, Wenhao Huang, Jiajun Zhang, Wenhu Chen, Chenghua Lin, Jie Fu, Min Yang, Shiwen Ni, Ge Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there have been significant advancements in large language models
(LLMs), particularly focused on the English language. These advancements have
enabled these LLMs to understand and execute complex instructions with
unprecedented accuracy and fluency. However, despite these advancements, there
remains a noticeable gap in the development of Chinese instruction tuning. The
unique linguistic features and cultural depth of the Chinese language pose
challenges for instruction tuning tasks. Existing datasets are either derived
from English-centric LLMs or are ill-suited for aligning with the interaction
patterns of real-world Chinese users. To bridge this gap, we introduce
COIG-CQIA, a high-quality Chinese instruction tuning dataset. Our aim is to
build a diverse, wide-ranging instruction-tuning dataset to better align model
behavior with human interactions. To this end, we collect a high-quality
human-written corpus from various sources on the Chinese Internet, including
Q&A communities, Wikis, examinations, and existing NLP datasets. This corpus
was rigorously filtered and carefully processed to form the COIG-CQIA dataset.
Furthermore, we train models of various scales on different subsets of CQIA,
following in-depth evaluation and analyses. The findings from our experiments
offer valuable insights for selecting and developing Chinese instruction-tuning
datasets. We also find that models trained on CQIA-Subset achieve competitive
results in human assessment as well as knowledge and security benchmarks. Data
are available at https://huggingface.co/datasets/m-a-p/COIG-CQIA
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Supervisory Prompt Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18051v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18051v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean Ghislain Billa, Min Oh, Liang Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The performance of Large Language Models (LLMs) relies heavily on the quality
of prompts, which are often manually engineered and task-specific, making them
costly and non-scalable. We propose a novel approach, Supervisory Prompt
Training (SPT). SPT automates the generation of highly effective prompts using
a dual LLM system. In this system, one LLM, the generator, performs a task
while the other, the corrector, provides feedback and generates improved
prompts. In contrast to earlier techniques, both the generator and corrector
collaboratively and continuously improve their prompts over time. We also
introduce the concept of \textit{impact scores} to measure the sentence-level
effectiveness of the prompts. Our method was tested on four benchmarks, testing
the level of hallucinations in LLMs. Notably, we were able to increase the
accuracy of GPT-4 on GSM8K from 65.8\% to 94.1\% (28.3\% increase). SPT
advances LLMs by refining prompts to enhance performance and reduce
hallucinations, offering an efficient and scalable alternative to traditional
model fine-tuning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Impact of Syntactic and Semantic Proximity on Machine Translation
  with Back-Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18031v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18031v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Guerin, Shane Steinert-Threlkeld, Emmanuel Chemla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised on-the-fly back-translation, in conjunction with multilingual
pretraining, is the dominant method for unsupervised neural machine
translation. Theoretically, however, the method should not work in general. We
therefore conduct controlled experiments with artificial languages to determine
what properties of languages make back-translation an effective training
method, covering lexical, syntactic, and semantic properties. We find, contrary
to popular belief, that (i) parallel word frequency distributions, (ii)
partially shared vocabulary, and (iii) similar syntactic structure across
languages are not sufficient to explain the success of back-translation. We
show however that even crude semantic signal (similar lexical fields across
languages) does improve alignment of two languages through back-translation. We
conjecture that rich semantic dependencies, parallel across languages, are at
the root of the success of unsupervised methods based on back-translation.
Overall, the success of unsupervised machine translation was far from being
analytically guaranteed. Instead, it is another proof that languages of the
world share deep similarities, and we hope to show how to identify which of
these similarities can serve the development of unsupervised, cross-linguistic
tools.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Pre-trained Language Model Sensitivity via Mask Specific
  losses: A case study on Biomedical NER <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18025v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18025v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Micheal Abaho, Danushka Bollegala, Gary Leeming, Dan Joyce, Iain E Buchan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adapting language models (LMs) to novel domains is often achieved through
fine-tuning a pre-trained LM (PLM) on domain-specific data. Fine-tuning
introduces new knowledge into an LM, enabling it to comprehend and efficiently
perform a target domain task. Fine-tuning can however be inadvertently
insensitive if it ignores the wide array of disparities (e.g in word meaning)
between source and target domains. For instance, words such as chronic and
pressure may be treated lightly in social conversations, however, clinically,
these words are usually an expression of concern. To address insensitive
fine-tuning, we propose Mask Specific Language Modeling (MSLM), an approach
that efficiently acquires target domain knowledge by appropriately weighting
the importance of domain-specific terms (DS-terms) during fine-tuning. MSLM
jointly masks DS-terms and generic words, then learns mask-specific losses by
ensuring LMs incur larger penalties for inaccurately predicting DS-terms
compared to generic words. Results of our analysis show that MSLM improves LMs
sensitivity and detection of DS-terms. We empirically show that an optimal
masking rate not only depends on the LM, but also on the dataset and the length
of sequences. Our proposed masking strategy outperforms advanced masking
strategies such as span- and PMI-based masking.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper alrerady accepted for publishing by the NAACL 2024 conference
  (main conference paper)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enriching Word Usage Graphs with Cluster Definitions <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18024v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18024v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mariia Fedorova, Andrey Kutuzov, Nikolay Arefyev, Dominik Schlechtweg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a dataset of word usage graphs (WUGs), where the existing WUGs for
multiple languages are enriched with cluster labels functioning as sense
definitions. They are generated from scratch by fine-tuned encoder-decoder
language models. The conducted human evaluation has shown that these
definitions match the existing clusters in WUGs better than the definitions
chosen from WordNet by two baseline systems. At the same time, the method is
straightforward to use and easy to extend to new languages. The resulting
enriched datasets can be extremely helpful for moving on to explainable
semantic change modeling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DORE: A <span class="highlight-title">Dataset</span> For Portuguese Definition Generation <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18018v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18018v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Beatriz Dimas Furtado, Tharindu Ranasinghe, Frédéric Blain, Ruslan Mitkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Definition modelling (DM) is the task of automatically generating a
dictionary definition for a specific word. Computational systems that are
capable of DM can have numerous applications benefiting a wide range of
audiences. As DM is considered a supervised natural language generation
problem, these systems require large annotated datasets to train the machine
learning (ML) models. Several DM datasets have been released for English and
other high-resource languages. While Portuguese is considered a
mid/high-resource language in most natural language processing tasks and is
spoken by more than 200 million native speakers, there is no DM dataset
available for Portuguese. In this research, we fill this gap by introducing
DORE; the first dataset for Definition MOdelling for PoRtuguEse containing more
than 100,000 definitions. We also evaluate several deep learning based DM
models on DORE and report the results. The dataset and the findings of this
paper will facilitate research and study of Portuguese in wider contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024 (The 2024 Joint International Conference
  on Computational Linguistics, Language Resources and Evaluation)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LISA: Layerwise Importance Sampling for Memory-Efficient Large Language
  Model Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17919v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17919v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Pan, Xiang Liu, Shizhe Diao, Renjie Pi, Jipeng Zhang, Chi Han, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The machine learning community has witnessed impressive advancements since
the first appearance of large language models (LLMs), yet their huge memory
consumption has become a major roadblock to large-scale training. Parameter
Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been
proposed to alleviate this problem, but their performance still fails to match
full parameter training in most large-scale fine-tuning settings. Attempting to
complement this deficiency, we investigate layerwise properties of LoRA on
fine-tuning tasks and observe an uncommon skewness of weight norms across
different layers. Utilizing this key observation, a surprisingly simple
training strategy is discovered, which outperforms both LoRA and full parameter
training in a wide range of settings with memory costs as low as LoRA. We name
it Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA,
which applies the idea of importance sampling to different layers in LLMs and
randomly freeze most middle layers during optimization. Experimental results
show that with similar or less GPU memory consumption, LISA surpasses LoRA or
even full parameter tuning in downstream fine-tuning tasks, where LISA
consistently outperforms LoRA by over $11\%$-$37\%$ in terms of MT-Bench
scores. On large models, specifically LLaMA-2-70B, LISA achieves on-par or
better performance than LoRA on MT-Bench, GSM8K, and PubMedQA, demonstrating
its effectiveness across different domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Unreasonable Ineffectiveness of the Deeper Layers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17887v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17887v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso, Daniel A. Roberts
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We empirically study a simple layer-pruning strategy for popular families of
open-weight pretrained LLMs, finding minimal degradation of performance on
different question-answering benchmarks until after a large fraction (up to
half) of the layers are removed. To prune these models, we identify the optimal
block of layers to prune by considering similarity across layers; then, to
"heal" the damage, we perform a small amount of finetuning. In particular, we
use parameter-efficient finetuning (PEFT) methods, specifically quantization
and Low Rank Adapters (QLoRA), such that each of our experiments can be
performed on a single A100 GPU. From a practical perspective, these results
suggest that layer pruning methods can complement other PEFT strategies to
further reduce computational resources of finetuning on the one hand, and can
improve the memory and latency of inference on the other hand. From a
scientific perspective, the robustness of these LLMs to the deletion of layers
implies either that current pretraining methods are not properly leveraging the
parameters in the deeper layers of the network or that the shallow layers play
a critical role in storing knowledge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 + 10 pages, 5 + 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring LLMs as a Source of Targeted Synthetic Textual Data to
  Minimize High Confidence Misclassifications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17860v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17860v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Lippmann, Matthijs Spaan, Jie Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language Processing (NLP) models optimized for predictive performance
often make high confidence errors and suffer from vulnerability to adversarial
and out-of-distribution data. Existing work has mainly focused on mitigation of
such errors using either humans or an automated approach. In this study, we
explore the usage of large language models (LLMs) for data augmentation as a
potential solution to the issue of NLP models making wrong predictions with
high confidence during classification tasks. We compare the effectiveness of
synthetic data generated by LLMs with that of human data obtained via the same
procedure. For mitigation, humans or LLMs provide natural language
characterizations of high confidence misclassifications to generate synthetic
data, which are then used to extend the training set. We conduct an extensive
evaluation of our approach on three classification tasks and demonstrate its
effectiveness in reducing the number of high confidence misclassifications
present in the model, all while maintaining the same level of accuracy.
Moreover, we find that the cost gap between humans and LLMs surpasses an order
of magnitude, as LLMs attain human-like performance while being more scalable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chroniclin<span class="highlight-title">gAme</span>ricaQA: A Large-scale Question Answering <span class="highlight-title">Dataset</span> based on
  Historical American Newspaper Pages <span class="chip">SIGIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhawna Piryani, Jamshid Mozafari, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question answering (QA) and Machine Reading Comprehension (MRC) tasks have
significantly advanced in recent years due to the rapid development of deep
learning techniques and, more recently, large language models. At the same
time, many benchmark datasets have become available for QA and MRC tasks.
However, most existing large-scale benchmark datasets have been created
predominantly using synchronous document collections like Wikipedia or the Web.
Archival document collections, such as historical newspapers, contain valuable
information from the past that is still not widely used to train large language
models. To further contribute to advancing QA and MRC tasks and to overcome the
limitation of previous datasets, we introduce ChroniclingAmericaQA, a
large-scale dataset with 485K question-answer pairs created based on the
historical newspaper collection Chronicling America. Our dataset is constructed
from a subset of the Chronicling America newspaper collection spanning 120
years. One of the significant challenges for utilizing digitized historical
newspaper collections is the low quality of OCR text. Therefore, to enable
realistic testing of QA models, our dataset can be used in three different
ways: answering questions from raw and noisy content, answering questions from
cleaner, corrected version of the content, as well as answering questions from
scanned images of newspaper pages. This and the fact that ChroniclingAmericaQA
spans the longest time period among available QA datasets make it quite a
unique and useful resource.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SIGIR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Verbing Weirds Language (Models): Evaluation of English Zero-Derivation
  in Five LLMs <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17856v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17856v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David R. Mortensen, Valentina Izrailevitch, Yunze Xiao, Hinrich Schütze, Leonie Weissweiler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lexical-syntactic flexibility, in the form of conversion (or zero-derivation)
is a hallmark of English morphology. In conversion, a word with one part of
speech is placed in a non-prototypical context, where it is coerced to behave
as if it had a different part of speech. However, while this process affects a
large part of the English lexicon, little work has been done to establish the
degree to which language models capture this type of generalization. This paper
reports the first study on the behavior of large language models with reference
to conversion. We design a task for testing lexical-syntactic flexibility --
the degree to which models can generalize over words in a construction with a
non-prototypical part of speech. This task is situated within a natural
language inference paradigm. We test the abilities of five language models --
two proprietary models (GPT-3.5 and GPT-4), three open-source models (Mistral
7B, Falcon 40B, and Llama 2 70B). We find that GPT-4 performs best on the task,
followed by GPT-3.5, but that the open source language models are also able to
perform it and that the 7B parameter Mistral displays as little difference
between its baseline performance on the natural language inference task and the
non-prototypical syntactic category task, as the massive GPT-4.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using Domain Knowledge to Guide Dialog Structure Induction via Neural
  Probabilistic Soft Logic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Connor Pryor, Quan Yuan, Jeremiah Liu, Mehran Kazemi, Deepak Ramachandran, Tania Bedrax-Weiss, Lise Getoor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dialog Structure Induction (DSI) is the task of inferring the latent dialog
structure (i.e., a set of dialog states and their temporal transitions) of a
given goal-oriented dialog. It is a critical component for modern dialog system
design and discourse analysis. Existing DSI approaches are often purely
data-driven, deploy models that infer latent states without access to domain
knowledge, underperform when the training corpus is limited/noisy, or have
difficulty when test dialogs exhibit distributional shifts from the training
domain. This work explores a neural-symbolic approach as a potential solution
to these problems. We introduce Neural Probabilistic Soft Logic Dialogue
Structure Induction (NEUPSL DSI), a principled approach that injects symbolic
knowledge into the latent space of a generative neural model. We conduct a
thorough empirical investigation on the effect of NEUPSL DSI learning on hidden
representation quality, few-shot learning, and out-of-domain generalization
performance. Over three dialog structure induction datasets and across
unsupervised and semi-supervised settings for standard and cross-domain
generalization, the injection of symbolic knowledge using NEUPSL DSI provides a
consistent boost in performance over the canonical baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ArabicaQA: A Comprehensive <span class="highlight-title">Dataset</span> for Arabic Question Answering <span class="chip">SIGIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17848v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17848v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrahman Abdallah, Mahmoud Kasem, Mahmoud Abdalla, Mohamed Mahmoud, Mohamed Elkasaby, Yasser Elbendary, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we address the significant gap in Arabic natural language
processing (NLP) resources by introducing ArabicaQA, the first large-scale
dataset for machine reading comprehension and open-domain question answering in
Arabic. This comprehensive dataset, consisting of 89,095 answerable and 3,701
unanswerable questions created by crowdworkers to look similar to answerable
ones, along with additional labels of open-domain questions marks a crucial
advancement in Arabic NLP resources. We also present AraDPR, the first dense
passage retrieval model trained on the Arabic Wikipedia corpus, specifically
designed to tackle the unique challenges of Arabic text retrieval. Furthermore,
our study includes extensive benchmarking of large language models (LLMs) for
Arabic question answering, critically evaluating their performance in the
Arabic language context. In conclusion, ArabicaQA, AraDPR, and the benchmarking
of LLMs in Arabic question answering offer significant advancements in the
field of Arabic NLP. The dataset and code are publicly accessible for further
research https://github.com/DataScienceUIBK/ArabicaQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SIGIR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot
  Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrhman Werby, Chenguang Huang, Martin Büchner, Abhinav Valada, Wolfram Burgard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent open-vocabulary robot mapping methods enrich dense geometric maps with
pre-trained visual-language features. While these maps allow for the prediction
of point-wise saliency maps when queried for a certain language concept,
large-scale environments and abstract queries beyond the object level still
pose a considerable hurdle, ultimately limiting language-grounded robotic
navigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D
scene graph mapping approach for language-grounded robot navigation. Leveraging
open-vocabulary vision foundation models, we first obtain state-of-the-art
open-vocabulary segment-level maps in 3D and subsequently construct a 3D scene
graph hierarchy consisting of floor, room, and object concepts, each enriched
with open-vocabulary features. Our approach is able to represent multi-story
buildings and allows robotic traversal of those using a cross-floor Voronoi
graph. HOV-SG is evaluated on three distinct datasets and surpasses previous
baselines in open-vocabulary semantic accuracy on the object, room, and floor
level while producing a 75% reduction in representation size compared to dense
open-vocabulary maps. In order to prove the efficacy and generalization
capabilities of HOV-SG, we showcase successful long-horizon
language-conditioned robot navigation within real-world multi-storage
environments. We provide code and trial video data at http://hovsg.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and video are available at http://hovsg.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Language Model (GLM): A new graph-based approach to detect social
  instabilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17816v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17816v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wallyson Lemes de Oliveira, Vahid Shamsaddini, Ali Ghofrani, Rahul Singh Inda, Jithendra Sai Veeramaneni, Étienne Voutaz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This scientific report presents a novel methodology for the early prediction
of important political events using News datasets. The methodology leverages
natural language processing, graph theory, clique analysis, and semantic
relationships to uncover hidden predictive signals within the data. Initially,
we designed a preliminary version of the method and tested it on a few events.
This analysis revealed limitations in the initial research phase. We then
enhanced the model in two key ways: first, we added a filtration step to only
consider politically relevant news before further processing; second, we
adjusted the input features to make the alert system more sensitive to
significant spikes in the data. After finalizing the improved methodology, we
tested it on eleven events including US protests, the Ukraine war, and French
protests. Results demonstrate the superiority of our approach compared to
baseline methods. Through targeted refinements, our model can now provide
earlier and more accurate predictions of major political events based on subtle
patterns in news data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are Compressed Language Models Less Subgroup Robust? <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17811v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17811v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonidas Gee, Andrea Zugarini, Novi Quadrianto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To reduce the inference cost of large language models, model compression is
increasingly used to create smaller scalable models. However, little is known
about their robustness to minority subgroups defined by the labels and
attributes of a dataset. In this paper, we investigate the effects of 18
different compression methods and settings on the subgroup robustness of BERT
language models. We show that worst-group performance does not depend on model
size alone, but also on the compression method used. Additionally, we find that
model compression does not always worsen the performance on minority subgroups.
Altogether, our analysis serves to further research into the subgroup
robustness of model compression.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 2023 Conference on Empirical Methods in Natural Language
  Processing (EMNLP 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding
  Model Mechanisms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17806v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17806v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Hanna, Sandro Pezzelle, Yonatan Belinkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many recent language model (LM) interpretability studies have adopted the
circuits framework, which aims to find the minimal computational subgraph, or
circuit, that explains LM behavior on a given task. Most studies determine
which edges belong in a LM's circuit by performing causal interventions on each
edge independently, but this scales poorly with model size. Edge attribution
patching (EAP), gradient-based approximation to interventions, has emerged as a
scalable but imperfect solution to this problem. In this paper, we introduce a
new method - EAP with integrated gradients (EAP-IG) - that aims to better
maintain a core property of circuits: faithfulness. A circuit is faithful if
all model edges outside the circuit can be ablated without changing the model's
performance on the task; faithfulness is what justifies studying circuits,
rather than the full model. Our experiments demonstrate that circuits found
using EAP are less faithful than those found using EAP-IG, even though both
have high node overlap with circuits found previously using causal
interventions. We conclude more generally that when using circuits to compare
the mechanisms models use to solve tasks, faithfulness, not overlap, is what
should be measured.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Text-to-Image Consistency via Automatic Prompt Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17804v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17804v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oscar Mañas, Pietro Astolfi, Melissa Hall, Candace Ross, Jack Urbanek, Adina Williams, Aishwarya Agrawal, Adriana Romero-Soriano, Michal Drozdzal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Impressive advances in text-to-image (T2I) generative models have yielded a
plethora of high performing models which are able to generate aesthetically
appealing, photorealistic images. Despite the progress, these models still
struggle to produce images that are consistent with the input prompt,
oftentimes failing to capture object quantities, relations and attributes
properly. Existing solutions to improve prompt-image consistency suffer from
the following challenges: (1) they oftentimes require model fine-tuning, (2)
they only focus on nearby prompt samples, and (3) they are affected by
unfavorable trade-offs among image quality, representation diversity, and
prompt-image consistency. In this paper, we address these challenges and
introduce a T2I optimization-by-prompting framework, OPT2I, which leverages a
large language model (LLM) to improve prompt-image consistency in T2I models.
Our framework starts from a user prompt and iteratively generates revised
prompts with the goal of maximizing a consistency score. Our extensive
validation on two datasets, MSCOCO and PartiPrompts, shows that OPT2I can boost
the initial consistency score by up to 24.9% in terms of DSG score while
preserving the FID and increasing the recall between generated and real data.
Our work paves the way toward building more reliable and robust T2I systems by
harnessing the power of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SciNews: From Scholarly Complexities to Public Narratives -- A <span class="highlight-title">Dataset</span>
  for Scientific News Report Generation <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17768v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17768v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongqi Pu, Yifan Wang, Jia Loy, Vera Demberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific news reports serve as a bridge, adeptly translating complex
research articles into reports that resonate with the broader public. The
automated generation of such narratives enhances the accessibility of scholarly
insights. In this paper, we present a new corpus to facilitate this paradigm
development. Our corpus comprises a parallel compilation of academic
publications and their corresponding scientific news reports across nine
disciplines. To demonstrate the utility and reliability of our dataset, we
conduct an extensive analysis, highlighting the divergences in readability and
brevity between scientific news narratives and academic manuscripts. We
benchmark our dataset employing state-of-the-art text generation models. The
evaluation process involves both automatic and human evaluation, which lays the
groundwork for future explorations into the automated generation of scientific
news reports. The dataset and code related to this work are available at
https://dongqi.me/projects/SciNews.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024 Main Conference Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Constructions Are So Difficult That Even Large Language Models Get Them
  Right for the Wrong Reasons <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17760v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17760v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijia Zhou, Leonie Weissweiler, Taiqi He, Hinrich Schütze, David R. Mortensen, Lori Levin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we make a contribution that can be understood from two
perspectives: from an NLP perspective, we introduce a small challenge dataset
for NLI with large lexical overlap, which minimises the possibility of models
discerning entailment solely based on token distinctions, and show that GPT-4
and Llama 2 fail it with strong bias. We then create further challenging
sub-tasks in an effort to explain this failure. From a Computational
Linguistics perspective, we identify a group of constructions with three
classes of adjectives which cannot be distinguished by surface features. This
enables us to probe for LLM's understanding of these constructions in various
ways, and we find that they fail in a variety of ways to distinguish between
them, suggesting that they don't adequately represent their meaning or capture
the lexical properties of phrasal heads.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can multiple-choice questions really be useful in detecting the
  abilities of LLMs? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17752v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17752v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wangyue Li, Liangzhi Li, Tong Xiang, Xiao Liu, Wei Deng, Noa Garcia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiple-choice questions (MCQs) are widely used in the evaluation of large
language models (LLMs) due to their simplicity and efficiency. However, there
are concerns about whether MCQs can truly measure LLM's capabilities,
particularly in knowledge-intensive scenarios where long-form generation (LFG)
answers are required. The misalignment between the task and the evaluation
method demands a thoughtful analysis of MCQ's efficacy, which we undertake in
this paper by evaluating nine LLMs on four question-answering (QA) datasets in
two languages: Chinese and English. We identify a significant issue: LLMs
exhibit an order sensitivity in bilingual MCQs, favoring answers located at
specific positions, i.e., the first position. We further quantify the gap
between MCQs and long-form generation questions (LFGQs) by comparing their
direct outputs, token logits, and embeddings. Our results reveal a relatively
low correlation between answers from MCQs and LFGQs for identical questions.
Additionally, we propose two methods to quantify the consistency and confidence
of LLMs' output, which can be generalized to other QA evaluation benchmarks.
Notably, our analysis challenges the idea that the higher the consistency, the
greater the accuracy. We also find MCQs to be less reliable than LFGQs in terms
of expected calibration error. Finally, the misalignment between MCQs and LFGQs
is not only reflected in the evaluation performance but also in the embedding
space. Our code and models can be accessed at
https://github.com/Meetyou-AI-Lab/Can-MC-Evaluate-LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UCxn: Typologically Informed Annotation of Constructions Atop Universal
  Dependencies <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17748v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17748v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonie Weissweiler, Nina Böbel, Kirian Guiller, Santiago Herrera, Wesley Scivetti, Arthur Lorenzi, Nurit Melnik, Archna Bhatia, Hinrich Schütze, Lori Levin, Amir Zeldes, Joakim Nivre, William Croft, Nathan Schneider
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Universal Dependencies (UD) project has created an invaluable collection
of treebanks with contributions in over 140 languages. However, the UD
annotations do not tell the full story. Grammatical constructions that convey
meaning through a particular combination of several morphosyntactic elements --
for example, interrogative sentences with special markers and/or word orders --
are not labeled holistically. We argue for (i) augmenting UD annotations with a
'UCxn' annotation layer for such meaning-bearing grammatical constructions, and
(ii) approaching this in a typologically informed way so that morphosyntactic
strategies can be compared across languages. As a case study, we consider five
construction families in ten languages, identifying instances of each
construction in UD treebanks through the use of morphosyntactic patterns. In
addition to findings regarding these particular constructions, our study yields
important insights on methodology for describing and identifying constructions
in language-general and language-particular ways, and lays the foundation for
future constructional enrichment of UD treebanks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sabiá-2: A New Generation of Portuguese Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09887v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09887v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thales Sales Almeida, Hugo Abonizio, Rodrigo Nogueira, Ramon Pires
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Sabi\'a-2, a family of large language models trained on
Portuguese texts. The models are evaluated on a diverse range of exams,
including entry-level tests for Brazilian universities, professional
certification exams, and graduate-level exams for various disciplines such as
accounting, economics, engineering, law and medicine. Our results reveal that
our best model so far, Sabi\'a-2 Medium, matches or surpasses GPT-4's
performance in 23 out of 64 exams and outperforms GPT-3.5 in 58 out of 64
exams. Notably, specialization has a significant impact on a model's
performance without the need to increase its size, allowing us to offer
Sabi\'a-2 Medium at a price per token that is 10 times cheaper than GPT-4.
Finally, we identified that math and coding are key abilities that need
improvement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HIVE: Harnessing Human Feedback for Instructional Visual Editing <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09618v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09618v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, Caiming Xiong, Ran Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incorporating human feedback has been shown to be crucial to align text
generated by large language models to human preferences. We hypothesize that
state-of-the-art instructional image editing models, where outputs are
generated based on an input image and an editing instruction, could similarly
benefit from human feedback, as their outputs may not adhere to the correct
instructions and preferences of users. In this paper, we present a novel
framework to harness human feedback for instructional visual editing (HIVE).
Specifically, we collect human feedback on the edited images and learn a reward
function to capture the underlying user preferences. We then introduce scalable
diffusion model fine-tuning methods that can incorporate human preferences
based on the estimated reward. Besides, to mitigate the bias brought by the
limitation of data, we contribute a new 1M training dataset, a 3.6K reward
dataset for rewards learning, and a 1K evaluation dataset to boost the
performance of instructional image editing. We conduct extensive empirical
experiments quantitatively and qualitatively, showing that HIVE is favored over
previous state-of-the-art instructional image editing approaches by a large
margin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In CVPR, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Batched Low-Rank Adaptation of Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05677v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05677v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yeming Wen, Swarat Chaudhuri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-Rank Adaptation (LoRA) has recently gained attention for fine-tuning
foundation models by incorporating trainable low-rank matrices, thereby
reducing the number of trainable parameters. While LoRA offers numerous
advantages, its applicability for real-time serving to a diverse and global
user base is constrained by its incapability to handle multiple task-specific
adapters efficiently. This imposes a performance bottleneck in scenarios
requiring personalized, task-specific adaptations for each incoming request. To
mitigate this constraint, we introduce Fast LoRA (FLoRA), a framework in which
each input example in a minibatch can be associated with its unique low-rank
adaptation weights, allowing for efficient batching of heterogeneous requests.
We empirically demonstrate that FLoRA retains the performance merits of LoRA,
showcasing competitive results on the MultiPL-E code generation benchmark
spanning over 8 languages and a multilingual speech recognition task across 6
languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Few-Shot Learning Focused <span class="highlight-title">Survey</span> on Recent Named Entity Recognition
  and Relation Classification Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.19055v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.19055v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sakher Khalil Alqaaidi, Elika Bozorgi, Afsaneh Shams, Krzysztof Kochut
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Named Entity Recognition (NER) and Relation Classification (RC) are important
steps in extracting information from unstructured text and formatting it into a
machine-readable format. We present a survey of recent deep learning models
that address named entity recognition and relation classification, with focus
on few-shot learning performance. Our survey is helpful for researchers in
knowing the recent techniques in text mining and extracting structured
information from raw text.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Good, but not always Fair: An Evaluation of Gender Bias for three
  commercial Machine Translation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05882v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05882v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Silvia Alma Piazzolla, Beatrice Savoldi, Luisa Bentivogli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine Translation (MT) continues to make significant strides in quality and
is increasingly adopted on a larger scale. Consequently, analyses have been
redirected to more nuanced aspects, intricate phenomena, as well as potential
risks that may arise from the widespread use of MT tools. Along this line, this
paper offers a meticulous assessment of three commercial MT systems - Google
Translate, DeepL, and Modern MT - with a specific focus on gender translation
and bias. For three language pairs (English/Spanish, English/Italian, and
English/French), we scrutinize the behavior of such systems at several levels
of granularity and on a variety of naturally occurring gender phenomena in
translation. Our study takes stock of the current state of online MT tools, by
revealing significant discrepancies in the gender translation of the three
systems, with each system displaying varying degrees of bias despite their
overall translation quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sentiment Analysis in Finance: From Transformers Back to eXplainable
  Lexicons (XLex) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03997v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03997v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maryan Rizinski, Hristijan Peshov, Kostadin Mishev, Milos Jovanovik, Dimitar Trajanov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lexicon-based sentiment analysis (SA) in finance leverages specialized,
manually annotated lexicons created by human experts to extract sentiment from
financial texts. Although lexicon-based methods are simple to implement and
fast to operate on textual data, they require considerable manual annotation
efforts to create, maintain, and update the lexicons. These methods are also
considered inferior to the deep learning-based approaches, such as transformer
models, which have become dominant in various NLP tasks due to their remarkable
performance. However, transformers require extensive data and computational
resources for both training and testing. Additionally, they involve significant
prediction times, making them unsuitable for real-time production environments
or systems with limited processing capabilities. In this paper, we introduce a
novel methodology named eXplainable Lexicons (XLex) that combines the
advantages of both lexicon-based methods and transformer models. We propose an
approach that utilizes transformers and SHapley Additive exPlanations (SHAP)
for explainability to learn financial lexicons. Our study presents four main
contributions. Firstly, we demonstrate that transformer-aided explainable
lexicons can enhance the vocabulary coverage of the benchmark Loughran-McDonald
(LM) lexicon, reducing the human involvement in annotating, maintaining, and
updating the lexicons. Secondly, we show that the resulting lexicon outperforms
the standard LM lexicon in SA of financial datasets. Thirdly, we illustrate
that the lexicon-based approach is significantly more efficient in terms of
model speed and size compared to transformers. Lastly, the XLex approach is
inherently more interpretable than transformer models as lexicon models rely on
predefined rules, allowing for better insights into the results of SA and
making the XLex approach a viable tool for financial decision-making.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published by IEEE Access DOI: 10.1109/ACCESS.2024.3349970 Link:
  https://ieeexplore.ieee.org/document/10380556</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GPT-4's assessment of its performance in a USMLE-based case study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09654v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09654v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Uttam Dhakal, Aniket Kumar Singh, Suman Devkota, Yogesh Sapkota, Bishal Lamichhane, Suprinsa Paudyal, Chandra Dhakal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates GPT-4's assessment of its performance in healthcare
applications. A simple prompting technique was used to prompt the LLM with
questions taken from the United States Medical Licensing Examination (USMLE)
questionnaire and it was tasked to evaluate its confidence score before posing
the question and after asking the question. The questionnaire was categorized
into two groups-questions with feedback (WF) and questions with no feedback(NF)
post-question. The model was asked to provide absolute and relative confidence
scores before and after each question. The experimental findings were analyzed
using statistical tools to study the variability of confidence in WF and NF
groups. Additionally, a sequential analysis was conducted to observe the
performance variation for the WF and NF groups. Results indicate that feedback
influences relative confidence but doesn't consistently increase or decrease
it. Understanding the performance of LLM is paramount in exploring its utility
in sensitive areas like healthcare. This study contributes to the ongoing
discourse on the reliability of AI, particularly of LLMs like GPT-4, within
healthcare, offering insights into how feedback mechanisms might be optimized
to enhance AI-assisted medical education and decision support.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comparing Pre-trained Human Language Models: Is it Better with Human
  Context as Groups, Individual Traits, or Both? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.12492v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.12492v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikita Soni, Niranjan Balasubramanian, H. Andrew Schwartz, Dirk Hovy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incorporating human context into language models is the next frontier for
human-centered natural language processing. Currently, two pre-training methods
exist: group-wise attributes (e.g., over-45-year-olds) or individual traits.
Group attributes are coarse -- not all 45-year-olds write the same way -- while
modeling individual traits allows for a more personalized representation, but
requires more complex modeling and data. So far, it is unclear which
pre-training approach benefits what tasks. We compare pre-training models with
human context via 1) group attributes, 2) individual users, and 3) a combined
approach on 5 user- and document-level tasks. We find that pre-training with
both group and individual features significantly improves the two user-level
regression tasks like age estimation and personality assessment. Pre-training
on individual users significantly improves the three document-level
classification tasks like stance and topic detection. It even does well for
downstream tasks without historical user data. Our results suggest both
approaches have specific use cases, opening new avenues for human-centered
language modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SmoothQuant: Accurate and Efficient Post-Training Quantization for Large
  Language Models <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.10438v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.10438v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, Song Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) show excellent performance but are compute- and
memory-intensive. Quantization can reduce memory and accelerate inference.
However, existing methods cannot maintain accuracy and hardware efficiency at
the same time. We propose SmoothQuant, a training-free, accuracy-preserving,
and general-purpose post-training quantization (PTQ) solution to enable 8-bit
weight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that
weights are easy to quantize while activations are not, SmoothQuant smooths the
activation outliers by offline migrating the quantization difficulty from
activations to weights with a mathematically equivalent transformation.
SmoothQuant enables an INT8 quantization of both weights and activations for
all the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG,
Llama-1/2, Falcon, Mistral, and Mixtral models. We demonstrate up to 1.56x
speedup and 2x memory reduction for LLMs with negligible loss in accuracy.
SmoothQuant enables serving 530B LLM within a single node. Our work offers a
turn-key solution that reduces hardware costs and democratizes LLMs. Code is
available at https://github.com/mit-han-lab/smoothquant.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2023. First two authors contributed equally to this work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The opportunities and risks of large language models in mental health 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14814v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14814v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hannah R. Lawrence, Renee A. Schneider, Susan B. Rubin, Maja J. Mataric, Daniel J. McDuff, Megan Jones Bell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Global rates of mental health concerns are rising and there is increasing
realization that existing models of mental healthcare will not adequately
expand to meet the demand. With the emergence of large language models (LLMs)
has come great optimism regarding their promise to create novel, large-scale
solutions to support mental health. Despite their nascence, LLMs have already
been applied to mental health-related tasks. In this review, we summarize the
extant literature on efforts to use LLMs to provide mental health education,
assessment, and intervention and highlight key opportunities for positive
impact in each area. We then highlight risks associated with LLMs application
to mental health and encourage adoption of strategies to mitigate these risks.
The urgent need for mental health support must be balanced with responsible
development, testing, and deployment of mental health LLMs. Especially critical
is ensuring that mental health LLMs are fine-tuned for mental health, enhance
mental health equity, adhere to ethical standards, and that people, including
those with lived experience with mental health concerns, are involved in all
stages from development through deployment. Prioritizing these efforts will
minimize potential harms to mental health and maximize the likelihood that LLMs
will positively impact mental health globally.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 2 tables, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leftover-Lunch: Advantage-based Offline <span class="highlight-title">Reinforcement</span> Learning for
  Language Models <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14718v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14718v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashutosh Baheti, Ximing Lu, Faeze Brahman, Ronan Le Bras, Maarten Sap, Mark Riedl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning with Human Feedback (RLHF) is the most prominent
method for Language Model (LM) alignment. However, RLHF is an unstable and
data-hungry process that continually requires new high-quality LM-generated
data for finetuning. We introduce Advantage-Leftover Lunch RL (A-LoL), a new
class of offline policy gradient algorithms that enable RL training on any
pre-existing data. By assuming the entire LM output sequence as a single
action, A-LoL allows incorporating sequence-level classifiers or human-designed
scoring functions as rewards. Subsequently, by using LM's value estimate, A-LoL
only trains on positive advantage (leftover) data points, making it resilient
to noise. Overall, A-LoL is an easy-to-implement, sample-efficient, and stable
LM training recipe.
  We demonstrate the effectiveness of A-LoL and its variants with a set of four
different language generation tasks. We compare against both online RL (PPO)
and recent preference-based (DPO, PRO) and reward-based (GOLD) offline RL
baselines. On the commonly-used RLHF benchmark, Helpful and Harmless Assistant
(HHA), LMs trained with A-LoL methods achieve the highest diversity while also
being rated more safe and helpful than the baselines according to humans.
Additionally, in the remaining three tasks, A-LoL could optimize multiple
distinct reward functions even when using noisy or suboptimal training data.
  We also release our experimental code. https://github.com/abaheti95/LoL-RL
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>published at ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LocalTweets to LocalHealth: A Mental Health Surveillance Framework Based
  on Twitter Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13452v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13452v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vijeta Deshpande, Minhwa Lee, Zonghai Yao, Zihao Zhang, Jason Brian Gibbons, Hong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior research on Twitter (now X) data has provided positive evidence of its
utility in developing supplementary health surveillance systems. In this study,
we present a new framework to surveil public health, focusing on mental health
(MH) outcomes. We hypothesize that locally posted tweets are indicative of
local MH outcomes and collect tweets posted from 765 neighborhoods (census
block groups) in the USA. We pair these tweets from each neighborhood with the
corresponding MH outcome reported by the Center for Disease Control (CDC) to
create a benchmark dataset, LocalTweets. With LocalTweets, we present the first
population-level evaluation task for Twitter-based MH surveillance systems. We
then develop an efficient and effective method, LocalHealth, for predicting MH
outcomes based on LocalTweets. When used with GPT3.5, LocalHealth achieves the
highest F1-score and accuracy of 0.7429 and 79.78\%, respectively, a 59\%
improvement in F1-score over the GPT3.5 in zero-shot setting. We also utilize
LocalHealth to extrapolate CDC's estimates to proxy unreported neighborhoods,
achieving an F1-score of 0.7291. Our work suggests that Twitter data can be
effectively leveraged to simulate neighborhood-level MH outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simple and Scalable Strategies to Continually Pre-train Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08763v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08763v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Ibrahim, Benjamin Thérien, Kshitij Gupta, Mats L. Richter, Quentin Anthony, Timothée Lesort, Eugene Belilovsky, Irina Rish
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are routinely pre-trained on billions of tokens,
only to start the process over again once new data becomes available. A much
more efficient solution is to continually pre-train these models, saving
significant compute compared to re-training. However, the distribution shift
induced by new data typically results in degraded performance on previous data
or poor adaptation to the new data. In this work, we show that a simple and
scalable combination of learning rate (LR) re-warming, LR re-decaying, and
replay of previous data is sufficient to match the performance of fully
re-training from scratch on all available data, as measured by the final loss
and the average score on several language model (LM) evaluation benchmarks.
Specifically, we show this for a weak but realistic distribution shift between
two commonly used LLM pre-training datasets (English$\rightarrow$English) and a
stronger distribution shift (English$\rightarrow$German) at the $405$M
parameter model scale with large dataset sizes (hundreds of billions of
tokens). Selecting the weak but realistic shift for larger-scale experiments,
we also find that our continual learning strategies match the re-training
baseline for a 10B parameter LLM. Our results demonstrate that LLMs can be
successfully updated via simple and scalable continual learning strategies,
matching the re-training baseline using only a fraction of the compute.
Finally, inspired by previous work, we propose alternatives to the cosine
learning rate schedule that help circumvent forgetting induced by LR re-warming
and that are not bound to a fixed token budget.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models Offer an Alternative to the Traditional Approach
  of Topic Modelling <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16248v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16248v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yida Mu, Chun Dong, Kalina Bontcheva, Xingyi Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topic modelling, as a well-established unsupervised technique, has found
extensive use in automatically detecting significant topics within a corpus of
documents. However, classic topic modelling approaches (e.g., LDA) have certain
drawbacks, such as the lack of semantic understanding and the presence of
overlapping topics. In this work, we investigate the untapped potential of
large language models (LLMs) as an alternative for uncovering the underlying
topics within extensive text corpora. To this end, we introduce a framework
that prompts LLMs to generate topics from a given set of documents and
establish evaluation protocols to assess the clustering efficacy of LLMs. Our
findings indicate that LLMs with appropriate prompts can stand out as a viable
alternative, capable of generating relevant topic titles and adhering to human
guidelines to refine and merge topics. Through in-depth experiments and
evaluation, we summarise the advantages and constraints of employing LLMs in
topic extraction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI and Generative AI for Research Discovery and Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.06795v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.06795v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mark Glickman, Yi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI and generative AI tools, including chatbots like ChatGPT that rely on
large language models (LLMs), have burst onto the scene this year, creating
incredible opportunities to increase work productivity and improve our lives.
Statisticians and data scientists have begun experiencing the benefits from the
availability of these tools in numerous ways, such as the generation of
programming code from text prompts to analyze data or fit statistical models.
One area that these tools can make a substantial impact is in research
discovery and summarization. Standalone tools and plugins to chatbots are being
developed that allow researchers to more quickly find relevant literature than
pre-2023 search tools. Furthermore, generative AI tools have improved to the
point where they can summarize and extract the key points from research
articles in succinct language. Finally, chatbots based on highly parameterized
LLMs can be used to simulate abductive reasoning, which provides researchers
the ability to make connections among related technical topics, which can also
be used for research discovery. We review the developments in AI and generative
AI for research discovery and summarization, and propose directions where these
types of tools are likely to head in the future that may be of interest to
statistician and data scientists.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generator-Retriever-Generator Approach for Open-Domain Question
  Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.11278v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.11278v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrahman Abdallah, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-domain question answering (QA) tasks usually require the retrieval of
relevant information from a large corpus to generate accurate answers. We
propose a novel approach called Generator-Retriever-Generator (GRG) that
combines document retrieval techniques with a large language model (LLM), by
first prompting the model to generate contextual documents based on a given
question. In parallel, a dual-encoder network retrieves documents that are
relevant to the question from an external corpus. The generated and retrieved
documents are then passed to the second LLM, which generates the final answer.
By combining document retrieval and LLM generation, our approach addresses the
challenges of open-domain QA, such as generating informative and contextually
relevant answers. GRG outperforms the state-of-the-art generate-then-read and
retrieve-then-read pipelines (GENREAD and RFiD) improving their performance by
at least by +5.2, +4.2, and +1.6 on TriviaQA, NQ, and WebQ datasets,
respectively. We provide code, datasets, and checkpoints at
https://github.com/abdoelsayed2016/GRG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AMuRD: Annotated Arabic-English Receipt <span class="highlight-title">Dataset</span> for Key Information
  Extraction and Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.09800v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.09800v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrahman Abdallah, Mahmoud Abdalla, Mohamed Elkasaby, Yasser Elbendary, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The extraction of key information from receipts is a complex task that
involves the recognition and extraction of text from scanned receipts. This
process is crucial as it enables the retrieval of essential content and
organizing it into structured documents for easy access and analysis. In this
paper, we present AMuRD, a novel multilingual human-annotated dataset
specifically designed for information extraction from receipts. This dataset
comprises $47,720$ samples and addresses the key challenges in information
extraction and item classification - the two critical aspects of data analysis
in the retail industry. Each sample includes annotations for item names and
attributes such as price, brand, and more. This detailed annotation facilitates
a comprehensive understanding of each item on the receipt. Furthermore, the
dataset provides classification into $44$ distinct product categories. This
classification feature allows for a more organized and efficient analysis of
the items, enhancing the usability of the dataset for various applications. In
our study, we evaluated various language model architectures, e.g., by
fine-tuning LLaMA models on the AMuRD dataset. Our approach yielded exceptional
results, with an F1 score of 97.43\% and accuracy of 94.99\% in information
extraction and classification, and an even higher F1 score of 98.51\% and
accuracy of 97.06\% observed in specific tasks. The dataset and code are
publicly accessible for further
researchhttps://github.com/Update-For-Integrated-Business-AI/AMuRD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training BERT Models to Carry Over a Coding System Developed on One
  Corpus to Another <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.03742v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.03742v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dalma Galambos, Pál Zsámboki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes how we train BERT models to carry over a coding system
developed on the paragraphs of a Hungarian literary journal to another. The aim
of the coding system is to track trends in the perception of literary
translation around the political transformation in 1989 in Hungary. To evaluate
not only task performance but also the consistence of the annotation, moreover,
to get better predictions from an ensemble, we use 10-fold crossvalidation.
Extensive hyperparameter tuning is used to obtain the best possible results and
fair comparisons. To handle label imbalance, we use loss functions and metrics
robust to it. Evaluation of the effect of domain shift is carried out by
sampling a test set from the target domain. We establish the sample size by
estimating the bootstrapped confidence interval via simulations. This way, we
show that our models can carry over one annotation system to the target domain.
Comparisons are drawn to provide insights such as learning multilabel
correlations and confidence penalty improve resistance to domain shift, and
domain adaptation on OCR-ed text on another domain improves performance almost
to the same extent as that on the corpus under study. See our code at
https://codeberg.org/zsamboki/bert-annotator-ensemble.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready version, to be presented at the 2024 Joint International
  Conference on Computational Linguistics, Language Resources and Evaluation
  (LREC-COLING 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Pre-training for Localized Instruction Generation of Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.15964v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.15964v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anil Batra, Davide Moltisanti, Laura Sevilla-Lara, Marcus Rohrbach, Frank Keller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Procedural videos show step-by-step demonstrations of tasks like recipe
preparation. Understanding such videos is challenging, involving the precise
localization of steps and the generation of textual instructions. Manually
annotating steps and writing instructions is costly, which limits the size of
current datasets and hinders effective learning. Leveraging large but noisy
video-transcript datasets for pre-training can boost performance, but demands
significant computational resources. Furthermore, transcripts contain
irrelevant content and exhibit style variation compared to instructions written
by human annotators. To mitigate both issues, we propose a technique,
Sieve-&-Swap, to automatically curate a smaller dataset: (i) Sieve filters
irrelevant transcripts and (ii) Swap enhances the quality of the text
instruction by automatically replacing the transcripts with human-written
instructions from a text-only recipe dataset. The curated dataset, three orders
of magnitude smaller than current web-scale datasets, enables efficient
training of large-scale models with competitive performance. We complement our
Sieve-\&-Swap approach with a Procedure Transformer (ProcX) for end-to-end step
localization and instruction generation for procedural videos. When this model
is pre-trained on our curated dataset, it achieves state-of-the-art performance
in zero-shot and finetuning settings on YouCook2 and Tasty, while using a
fraction of the computational resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This version has some missing experiments and elaborative technical
  details</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Blinded by Generated Contexts: How Language Models Merge Generated and
  Retrieved Contexts for Open-Domain QA? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11911v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11911v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hexiang Tan, Fei Sun, Wanli Yang, Yuanzhuo Wang, Qi Cao, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While auxiliary information has become a key to enhancing Large Language
Models (LLMs), relatively little is known about how LLMs merge these contexts,
specifically contexts generated by LLMs and those retrieved from external
sources. To investigate this, we formulate a systematic framework to identify
whether LLMs' responses, derived from the integration of generated and
retrieved contexts, are attributed to either generated or retrieved contexts.
To easily trace the origin of the response, we construct datasets with
conflicting contexts, i.e., each question is paired with both generated and
retrieved contexts, yet only one of them contains the correct answer. Our
experiments reveal a significant bias in several LLMs (GPT-4/3.5 and Llama2) to
favor generated contexts, even when they provide incorrect information. We
further identify two key factors contributing to this bias: i) contexts
generated by LLMs typically show greater similarity to the questions,
increasing their likelihood of being selected; ii) the segmentation process
used in retrieved contexts disrupts their completeness, thereby hindering their
full utilization in LLMs. Our analysis enhances the understanding of how LLMs
merge diverse contexts, offering valuable insights for advancing current
augmentation methods for LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Measuring Entrainment in Spontaneous Code-switched Speech <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07703v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07703v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debasmita Bhattacharya, Siying Ding, Alayna Nguyen, Julia Hirschberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is well-known that speakers who entrain to one another have more
successful conversations than those who do not. Previous research has shown
that interlocutors entrain on linguistic features in both written and spoken
monolingual domains. More recent work on code-switched communication has also
shown preliminary evidence of entrainment on certain aspects of code-switching
(CSW). However, such studies of entrainment in code-switched domains have been
extremely few and restricted to human-machine textual interactions. Our work
studies code-switched spontaneous speech between humans, finding that (1)
patterns of written and spoken entrainment in monolingual settings largely
generalize to code-switched settings, and (2) some patterns of entrainment on
code-switching in dialogue agent-generated text generalize to spontaneous
code-switched speech. Our findings give rise to important implications for the
potentially "universal" nature of entrainment as a communication phenomenon,
and potential applications in inclusive and interactive speech technology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Edits: camera-ready manuscript for NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decode Neural signal as Speech 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01748v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01748v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqian Yang, Yiqun Duan, Qiang Zhang, Renjing Xu, Hui Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decoding language from brain dynamics is an important open direction in the
realm of brain-computer interface (BCI), especially considering the rapid
growth of large language models. Compared to invasive-based signals which
require electrode implantation surgery, non-invasive neural signals (e.g. EEG,
MEG) have attracted increasing attention considering their safety and
generality. However, the exploration is not adequate in three aspects: 1)
previous methods mainly focus on EEG but none of the previous works address
this problem on MEG with better signal quality; 2) prior works have
predominantly used ``teacher-forcing" during generative decoding, which is
impractical; 3) prior works are mostly ``BART-based" not fully auto-regressive,
which performs better in other sequence tasks. In this paper, we explore the
brain-to-text translation of MEG signals in a speech-decoding formation. Here
we are the first to investigate a cross-attention-based ``whisper" model for
generating text directly from MEG signals without teacher forcing. Our model
achieves impressive BLEU-1 scores of 60.30 and 52.89 without pretraining \&
teacher-forcing on two major datasets (\textit{GWilliams} and
\textit{Schoffelen}). This paper conducts a comprehensive review to understand
how speech decoding formation performs on the neural decoding tasks, including
pretraining initialization, training \& evaluation set splitting, augmentation,
and scaling law.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploiting Semantic Reconstruction to Mitigate Hallucinations in
  Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16167v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16167v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minchan Kim, Minyeong Kim, Junik Bae, Suhwan Choi, Sungkyung Kim, Buru Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hallucinations in vision-language models pose a significant challenge to
their reliability, particularly in the generation of long captions. Current
methods fall short of accurately identifying and mitigating these
hallucinations. To address this issue, we introduce ESREAL, a novel
unsupervised learning framework designed to suppress the generation of
hallucinations through accurate localization and penalization of hallucinated
tokens. Initially, ESREAL creates a reconstructed image based on the generated
caption and aligns its corresponding regions with those of the original image.
This semantic reconstruction aids in identifying both the presence and type of
token-level hallucinations within the generated caption. Subsequently, ESREAL
computes token-level hallucination scores by assessing the semantic similarity
of aligned regions based on the type of hallucination. Finally, ESREAL employs
a proximal policy optimization algorithm, where it selectively penalizes
hallucinated tokens according to their token-level hallucination scores. Our
framework notably reduces hallucinations in LLaVA, InstructBLIP, and mPLUG-Owl2
by 32.81%, 27.08%, and 7.46% on the CHAIR metric. This improvement is achieved
solely through signals derived from the image itself, without the need for any
image-text pairs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerating Scientific Discovery with Generative Knowledge Extraction,
  Graph-Based Representation, and Multimodal Intelligent Graph Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11996v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11996v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Markus J. Buehler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging generative Artificial Intelligence (AI), we have transformed a
dataset comprising 1,000 scientific papers into an ontological knowledge graph.
Through an in-depth structural analysis, we have calculated node degrees,
identified communities and connectivities, and evaluated clustering
coefficients and betweenness centrality of pivotal nodes, uncovering
fascinating knowledge architectures. The graph has an inherently scale-free
nature, is highly connected, and can be used for graph reasoning by taking
advantage of transitive and isomorphic properties that reveal unprecedented
interdisciplinary relationships that can be used to answer queries, identify
gaps in knowledge, propose never-before-seen material designs, and predict
material behaviors. We compute deep node embeddings for combinatorial node
similarity ranking for use in a path sampling strategy links dissimilar
concepts that have previously not been related. One comparison revealed
structural parallels between biological materials and Beethoven's 9th Symphony,
highlighting shared patterns of complexity through isomorphic mapping. In
another example, the algorithm proposed a hierarchical mycelium-based composite
based on integrating path sampling with principles extracted from Kandinsky's
'Composition VII' painting. The resulting material integrates an innovative set
of concepts that include a balance of chaos/order, adjustable porosity,
mechanical strength, and complex patterned chemical functionalization. We
uncover other isomorphisms across science, technology and art, revealing a
nuanced ontology of immanence that reveal a context-dependent heterarchical
interplay of constituents. Graph-based generative AI achieves a far higher
degree of novelty, explorative capacity, and technical detail, than
conventional approaches and establishes a widely useful framework for
innovation by revealing hidden connections.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling the Pitfalls of Knowledge Editing for Large Language Models <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02129v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02129v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhoubo Li, Ningyu Zhang, Yunzhi Yao, Mengru Wang, Xi Chen, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the cost associated with fine-tuning Large Language Models (LLMs)
continues to rise, recent research efforts have pivoted towards developing
methodologies to edit implicit knowledge embedded within LLMs. Yet, there's
still a dark cloud lingering overhead -- will knowledge editing trigger
butterfly effect? since it is still unclear whether knowledge editing might
introduce side effects that pose potential risks or not. This paper pioneers
the investigation into the potential pitfalls associated with knowledge editing
for LLMs. To achieve this, we introduce new benchmark datasets and propose
innovative evaluation metrics. Our results underline two pivotal concerns: (1)
Knowledge Conflict: Editing groups of facts that logically clash can magnify
the inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)
Knowledge Distortion: Altering parameters with the aim of editing factual
knowledge can irrevocably warp the innate knowledge structure of LLMs.
Experimental results vividly demonstrate that knowledge editing might
inadvertently cast a shadow of unintended consequences on LLMs, which warrant
attention and efforts for future works. Code and data are available at
https://github.com/zjunlp/PitfallsKnowledgeEditing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Science and Game Theory
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalizing Better Response Paths and Weakly Acyclic <span class="highlight-title">Game</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18086v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18086v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bora Yongacoglu, Gürdal Arslan, Lacra Pavel, Serdar Yüksel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weakly acyclic games generalize potential games and are fundamental to the
study of game theoretic control. In this paper, we present a generalization of
weakly acyclic games, and we observe its importance in multi-agent learning
when agents employ experimental strategy updates in periods where they fail to
best respond. While weak acyclicity is defined in terms of path connectivity
properties of a game's better response graph, our generalization is defined
using a generalized better response graph. We provide sufficient conditions for
this notion of generalized weak acyclicity in both two-player games and
$n$-player games. To demonstrate that our generalization is not trivial, we
provide examples of games admitting a pure Nash equilibrium that are not
generalized weakly acyclic. The generalization presented in this work is
closely related to the recent theory of satisficing paths, and the
counterexamples presented here constitute the first negative results in that
theory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Paths to Equilibrium in Normal-Form <span class="highlight-title">Game</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18079v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18079v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bora Yongacoglu, Gürdal Arslan, Lacra Pavel, Serdar Yüksel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In multi-agent reinforcement learning (MARL), agents repeatedly interact
across time and revise their strategies as new data arrives, producing a
sequence of strategy profiles. This paper studies sequences of strategies
satisfying a pairwise constraint inspired by policy updating in reinforcement
learning, where an agent who is best responding in period $t$ does not switch
its strategy in the next period $t+1$. This constraint merely requires that
optimizing agents do not switch strategies, but does not constrain the other
non-optimizing agents in any way, and thus allows for exploration. Sequences
with this property are called satisficing paths, and arise naturally in many
MARL algorithms. A fundamental question about strategic dynamics is such: for a
given game and initial strategy profile, is it always possible to construct a
satisficing path that terminates at an equilibrium strategy? The resolution of
this question has implications about the capabilities or limitations of a class
of MARL algorithms. We answer this question in the affirmative for mixed
extensions of finite normal-form games.%
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prediction-sharing During Training and Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17515v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17515v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yotam Gafni, Ronen Gradwohl, Moshe Tennenholtz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Two firms are engaged in a competitive prediction task. Each firm has two
sources of data -- labeled historical data and unlabeled inference-time data --
and uses the former to derive a prediction model, and the latter to make
predictions on new instances. We study data-sharing contracts between the
firms. The novelty of our study is to introduce and highlight the differences
between contracts that share prediction models only, contracts to share
inference-time predictions only, and contracts to share both. Our analysis
proceeds on three levels. First, we develop a general Bayesian framework that
facilitates our study. Second, we narrow our focus to two natural settings
within this framework: (i) a setting in which the accuracy of each firm's
prediction model is common knowledge, but the correlation between the
respective models is unknown; and (ii) a setting in which two hypotheses exist
regarding the optimal predictor, and one of the firms has a structural
advantage in deducing it. Within these two settings we study optimal contract
choice. More specifically, we find the individually rational and Pareto-optimal
contracts for some notable cases, and describe specific settings where each of
the different sharing contracts emerge as optimal. Finally, in the third level
of our analysis we demonstrate the applicability of our concepts in a synthetic
simulation using real loan data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ So Long Sucker: End<span class="highlight-title">game</span> Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17302v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17302v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean-Lou De Carufel, Marie Rose Jerade
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  So Long Sucker is a strategy board game requiring 4 players, each with $c$
chips of their designated color, and a board made of $k$ empty piles. With a
clear set-up come intricate rules, such as: players taking turns but not in a
fixed order, agreements between some players being made and broken at any time,
and a player winning the game even without any chips in hand.
  One of the main points of interest in studying this game, is finding when a
player has a winning strategy. The game begins with four players that get
eliminated successively until the winner is left. To study winning strategies,
it is of interest to look at endgame situations. We present the following game
set-up: there are two players left in the game, Blue and Red, and only their
respective chip colors. In this paper, we characterize Blue's winning
situations and strategies through inductive reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>49 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Weighted Top-Difference Distance: Axioms, Aggregation, and
  Approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15198v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15198v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Aveni, Ludovico Crippa, Giulio Principi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study a family of distance functions on rankings that allow for asymmetric
treatments of alternatives and consider the distinct relevance of the top and
bottom positions for ordered lists. We provide a full axiomatic
characterization of our distance. In doing so, we retrieve new
characterizations of existing axioms and show how to effectively weaken them
for our purposes. This analysis highlights the generality of our distance as it
embeds many (semi)metrics previously proposed in the literature. Subsequently,
we show that, notwithstanding its level of generality, our distance is still
readily applicable. We apply it to preference aggregation, studying the
features of the associated median voting rule. It is shown how the derived
preference function satisfies many desirable features in the context of voting
rules, ranging from fairness to majority and Pareto-related properties. We show
how to compute consensus rankings exactly, and provide generalized
Diaconis-Graham inequalities that can be leveraged to obtain approximation
algorithms. Finally, we propose some truncation ideas for our distances
inspired by Lu and Boutilier (2010). These can be leveraged to devise a
Polynomial-Time-Approximation Scheme for the corresponding rank aggregation
problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>64 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Mechanism Design with Predictions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02879v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02879v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Balkanski, Vasilis Gkatzelis, Xizhi Tan, Cherlin Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aiming to overcome some of the limitations of worst-case analysis, the
recently proposed framework of "algorithms with predictions" allows algorithms
to be augmented with a (possibly erroneous) machine-learned prediction that
they can use as a guide. In this framework, the goal is to obtain improved
guarantees when the prediction is correct, which is called \emph{consistency},
while simultaneously guaranteeing some worst-case bounds even when the
prediction is arbitrarily wrong, which is called \emph{robustness}. The vast
majority of the work on this framework has focused on a refined analysis of
online algorithms augmented with predictions regarding the future input. A
subsequent line of work has also successfully adapted this framework to
mechanism design, where the prediction is regarding the private information of
strategic agents. In this paper, we initiate the study of online mechanism
design with predictions, which combines the challenges of online algorithms
with predictions and mechanism design with predictions.
  We consider the well-studied problem of designing a revenue-maximizing
auction to sell a single item to strategic bidders who arrive and depart over
time, each with an unknown, private, value for the item. We study the
learning-augmented version of this problem where the auction designer is given
a prediction regarding the maximum value over all agents. Our main result is a
strategyproof mechanism whose revenue guarantees are $\alpha$-consistent with
respect to the highest value and $(1-\alpha^2)/4$-robust with respect to the
second-highest value, for $\alpha \in [0,1]$. We show that this tradeoff is
optimal within a broad and natural family of auctions, meaning that any
$\alpha$-consistent mechanism in that family has robustness at most
$(1-\alpha^2)/4$. Finally, we extend our mechanism to also achieve expected
revenues proportional to the prediction quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SGHormer: An Energy-Saving Graph Transformer Driven by Spikes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17656v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17656v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huizhe Zhang, Jintang Li, Liang Chen, Zibin Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Transformers (GTs) with powerful representation learning ability make a
huge success in wide range of graph tasks. However, the costs behind
outstanding performances of GTs are higher energy consumption and computational
overhead. The complex structure and quadratic complexity during attention
calculation in vanilla transformer seriously hinder its scalability on the
large-scale graph data. Though existing methods have made strides in
simplifying combinations among blocks or attention-learning paradigm to improve
GTs' efficiency, a series of energy-saving solutions originated from
biologically plausible structures are rarely taken into consideration when
constructing GT framework. To this end, we propose a new spiking-based graph
transformer (SGHormer). It turns full-precision embeddings into sparse and
binarized spikes to reduce memory and computational costs. The spiking graph
self-attention and spiking rectify blocks in SGHormer explicitly capture global
structure information and recover the expressive power of spiking embeddings,
respectively. In experiments, SGHormer achieves comparable performances to
other full-precision GTs with extremely low computational energy consumption.
The results show that SGHomer makes a remarkable progress in the field of
low-energy GTs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chain of Compression: A Systematic Approach to Combinationally Compress
  Convolutional Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingtao Shen, Minqing Sun, Jie Zhao, An Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural networks (CNNs) have achieved significant popularity,
but their computational and memory intensity poses challenges for
resource-constrained computing systems, particularly with the prerequisite of
real-time performance. To release this burden, model compression has become an
important research focus. Many approaches like quantization, pruning, early
exit, and knowledge distillation have demonstrated the effect of reducing
redundancy in neural networks. Upon closer examination, it becomes apparent
that each approach capitalizes on its unique features to compress the neural
network, and they can also exhibit complementary behavior when combined. To
explore the interactions and reap the benefits from the complementary features,
we propose the Chain of Compression, which works on the combinational sequence
to apply these common techniques to compress the neural network. Validated on
the image-based regression and classification networks across different data
sets, our proposed Chain of Compression can significantly compress the
computation cost by 100-1000 times with ignorable accuracy loss compared with
the baseline model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Traffic Signal Control via Genetic Programming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17328v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17328v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao-Cheng Liao, Yi Mei, Mengjie Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The control of traffic signals is crucial for improving transportation
efficiency. Recently, learning-based methods, especially Deep Reinforcement
Learning (DRL), garnered substantial success in the quest for more efficient
traffic signal control strategies. However, the design of rewards in DRL highly
demands domain knowledge to converge to an effective policy, and the final
policy also presents difficulties in terms of explainability. In this work, a
new learning-based method for signal control in complex intersections is
proposed. In our approach, we design a concept of phase urgency for each signal
phase. During signal transitions, the traffic light control strategy selects
the next phase to be activated based on the phase urgency. We then proposed to
represent the urgency function as an explainable tree structure. The urgency
function can calculate the phase urgency for a specific phase based on the
current road conditions. Genetic programming is adopted to perform
gradient-free optimization of the urgency function. We test our algorithm on
multiple public traffic signal control datasets. The experimental results
indicate that the tree-shaped urgency function evolved by genetic programming
outperforms the baselines, including a state-of-the-art method in the
transportation field and a well-known DRL-based method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Confidence and second-order errors in cortical circuits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.16046v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.16046v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arno Granier, Mihai A. Petrovici, Walter Senn, Katharina A. Wilmes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Minimization of cortical prediction errors has been considered a key
computational goal of the cerebral cortex underlying perception, action and
learning. However, it is still unclear how the cortex should form and use
information about uncertainty in this process. Here, we formally derive neural
dynamics that minimize prediction errors under the assumption that cortical
areas must not only predict the activity in other areas and sensory streams but
also jointly project their confidence (inverse expected uncertainty) in their
predictions. In the resulting neuronal dynamics, the integration of bottom-up
and top-down cortical streams is dynamically modulated based on confidence in
accordance with the Bayesian principle. Moreover, the theory predicts the
existence of cortical second-order errors, comparing confidence and actual
performance. These errors are propagated through the cortical hierarchy
alongside classical prediction errors and are used to learn the weights of
synapses responsible for formulating confidence. We propose a detailed mapping
of the theory to cortical circuitry, discuss entailed functional
interpretations and provide potential directions for experimental work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Algorithm-Hardware Co-Design of Distribution-Aware Logarithmic-Posit
  Encodings for Efficient DNN Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05465v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05465v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshat Ramachandran, Zishen Wan, Geonhwa Jeong, John Gustafson, Tushar Krishna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional Deep Neural Network (DNN) quantization methods using integer,
fixed-point, or floating-point data types struggle to capture diverse DNN
parameter distributions at low precision, and often require large silicon
overhead and intensive quantization-aware training. In this study, we introduce
Logarithmic Posits (LP), an adaptive, hardware-friendly data type inspired by
posits that dynamically adapts to DNN weight/activation distributions by
parameterizing LP bit fields. We also develop a novel genetic-algorithm based
framework, LP Quantization (LPQ), to find optimal layer-wise LP parameters
while reducing representational divergence between quantized and full-precision
models through a novel global-local contrastive objective. Additionally, we
design a unified mixed-precision LP accelerator (LPA) architecture comprising
of processing elements (PEs) incorporating LP in the computational datapath.
Our algorithm-hardware co-design demonstrates on average <1% drop in top-1
accuracy across various CNN and ViT models. It also achieves ~ 2x improvements
in performance per unit area and 2.2x gains in energy efficiency compared to
state-of-the-art quantization accelerators using different data types.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2024 61st IEEE/ACM Design Automation Conference (DAC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Probabilistically Rewired Message-Passing Neural Networks <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02156v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02156v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chendi Qian, Andrei Manolache, Kareem Ahmed, Zhe Zeng, Guy Van den Broeck, Mathias Niepert, Christopher Morris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Message-passing graph neural networks (MPNNs) emerged as powerful tools for
processing graph-structured input. However, they operate on a fixed input graph
structure, ignoring potential noise and missing information. Furthermore, their
local aggregation mechanism can lead to problems such as over-squashing and
limited expressive power in capturing relevant graph structures. Existing
solutions to these challenges have primarily relied on heuristic methods, often
disregarding the underlying data distribution. Hence, devising principled
approaches for learning to infer graph structures relevant to the given
prediction task remains an open challenge. In this work, leveraging recent
progress in exact and differentiable $k$-subset sampling, we devise
probabilistically rewired MPNNs (PR-MPNNs), which learn to add relevant edges
while omitting less beneficial ones. For the first time, our theoretical
analysis explores how PR-MPNNs enhance expressive power, and we identify
precise conditions under which they outperform purely randomized approaches.
Empirically, we demonstrate that our approach effectively mitigates issues like
over-squashing and under-reaching. In addition, on established real-world
datasets, our method exhibits competitive or superior predictive performance
compared to traditional MPNN models and recent graph transformer architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Nonlinear African Vulture Optimization Algorithm Combining Henon
  Chaotic Mapping Theory and Reverse Learning Competition <span class="highlight-title">Strategy</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15505v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15505v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baiyi Wang, Zipeng Zhang, Patrick Siarry, Xinhua Liu, Grzegorz Królczyk, Dezheng Hua, Frantisek Brumercik, Zhixiong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In order to alleviate the main shortcomings of the AVOA, a nonlinear African
vulture optimization algorithm combining Henon chaotic mapping theory and
reverse learning competition strategy (HWEAVOA) is proposed. Firstly, the Henon
chaotic mapping theory and elite population strategy are proposed to improve
the randomness and diversity of the vulture's initial population; Furthermore,
the nonlinear adaptive incremental inertial weight factor is introduced in the
location update phase to rationally balance the exploration and exploitation
abilities, and avoid individual falling into a local optimum; The reverse
learning competition strategy is designed to expand the discovery fields for
the optimal solution and strengthen the ability to jump out of the local
optimal solution. HWEAVOA and other advanced comparison algorithms are used to
solve classical and CEC2022 test functions. Compared with other algorithms, the
convergence curves of the HWEAVOA drop faster and the line bodies are smoother.
These experimental results show the proposed HWEAVOA is ranked first in all
test functions, which is superior to the comparison algorithms in convergence
speed, optimization ability, and solution stability. Meanwhile, HWEAVOA has
reached the general level in the algorithm complexity, and its overall
performance is competitive in the swarm intelligence algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Two-compartment neuronal spiking model expressing brain-state specific
  apical-amplification, -isolation and -drive regimes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.06074v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.06074v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elena Pastorelli, Alper Yegenoglu, Nicole Kolodziej, Willem Wybo, Francesco Simula, Sandra Diaz, Johan Frederik Storm, Pier Stanislao Paolucci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mounting experimental evidence suggests that brain-state-specific neural
mechanisms, supported by connectomic architectures, play a crucial role in
integrating past and contextual knowledge with the current, incoming flow of
evidence (e.g., from sensory systems). These mechanisms operate across multiple
spatial and temporal scales, necessitating dedicated support at the levels of
individual neurons and synapses. A notable feature within the neocortex is the
structure of large, deep pyramidal neurons, which exhibit a distinctive
separation between an apical dendritic compartment and a basal
dendritic/perisomatic compartment. This separation is characterized by distinct
patterns of incoming connections and brain-state-specific activation
mechanisms, namely, apical amplification, isolation, and drive, which are
associated with wakefulness, deeper NREM sleep stages, and REM sleep,
respectively. The cognitive roles of apical mechanisms have been demonstrated
in behaving animals. In contrast, classical models of learning in spiking
networks are based on single-compartment neurons, lacking the ability to
describe the integration of apical and basal/somatic information. This work
aims to provide the computational community with a two-compartment spiking
neuron model that incorporates features essential for supporting
brain-state-specific learning. This model includes a piece-wise linear transfer
function (ThetaPlanes) at the highest abstraction level, making it suitable for
use in large-scale bio-inspired artificial intelligence systems. A machine
learning evolutionary algorithm, guided by a set of fitness functions, selected
the parameters that define neurons expressing the desired apical mechanisms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 9 figures (29 single images), 4 tables, paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Model for Multi-objective Evolutionary Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12541v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12541v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Liu, Xi Lin, Zhenkun Wang, Shunyu Yao, Xialiang Tong, Mingxuan Yuan, Qingfu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiobjective evolutionary algorithms (MOEAs) are major methods for solving
multiobjective optimization problems (MOPs). Many MOEAs have been proposed in
the past decades, of which the search operators need a carefully handcrafted
design with domain knowledge. Recently, some attempts have been made to replace
the manually designed operators in MOEAs with learning-based operators (e.g.,
neural network models). However, much effort is still required for designing
and training such models, and the learned operators might not generalize well
on new problems. To tackle the above challenges, this work investigates a novel
approach that leverages the powerful large language model (LLM) to design MOEA
operators. With proper prompt engineering, we successfully let a general LLM
serve as a black-box search operator for decomposition-based MOEA (MOEA/D) in a
zero-shot manner. In addition, by learning from the LLM behavior, we further
design an explicit white-box operator with randomness and propose a new version
of decomposition-based MOEA, termed MOEA/D-LO. Experimental studies on
different test benchmarks show that our proposed method can achieve competitive
performance with widely used MOEAs. It is also promising to see the operator
only learned from a few instances can have robust generalization performance on
unseen problems with quite different patterns and settings. The results reveal
the potential benefits of using pre-trained LLMs in the design of MOEAs.To
foster reproducibility and accessibility, the source code is
https://github.com/FeiLiu36/LLM4MOEA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Masked Autoencoders Are Robust Neural Architecture Search Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12086v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12086v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Hu, Xiangxiang Chu, Bo Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Architecture Search (NAS) currently relies heavily on labeled data,
which is both expensive and time-consuming to acquire. In this paper, we
propose a novel NAS framework based on Masked Autoencoders (MAE) that
eliminates the need for labeled data during the search process. By replacing
the supervised learning objective with an image reconstruction task, our
approach enables the robust discovery of network architectures without
compromising performance and generalization ability. Additionally, we address
the problem of performance collapse encountered in the widely-used
Differentiable Architecture Search (DARTS) method in the unsupervised paradigm
by introducing a multi-scale decoder. Through extensive experiments conducted
on various search spaces and datasets, we demonstrate the effectiveness and
robustness of the proposed method, providing empirical evidence of its
superiority over baseline approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion Models Generate Images Like Painters: an Analytical Theory of
  Outline First, Details Later <span class="chip">NeurIPS23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02490v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02490v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binxu Wang, John J. Vastola
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How do diffusion generative models convert pure noise into meaningful images?
In a variety of pretrained diffusion models (including conditional latent space
models like Stable Diffusion), we observe that the reverse diffusion process
that underlies image generation has the following properties: (i) individual
trajectories tend to be low-dimensional and resemble 2D `rotations'; (ii)
high-variance scene features like layout tend to emerge earlier, while
low-variance details tend to emerge later; and (iii) early perturbations tend
to have a greater impact on image content than later perturbations. To
understand these phenomena, we derive and study a closed-form solution to the
probability flow ODE for a Gaussian distribution, which shows that the reverse
diffusion state rotates towards a gradually-specified target on the image
manifold. It also shows that generation involves first committing to an
outline, and then to finer and finer details. We find that this solution
accurately describes the initial phase of image generation for pretrained
models, and can in principle be used to make image generation more efficient by
skipping reverse diffusion steps. Finally, we use our solution to characterize
the image manifold in Stable Diffusion. Our viewpoint reveals an unexpected
similarity between generation by GANs and diffusion and provides a conceptual
link between diffusion and image retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>44 pages, 28 figures. A briefer version was presented at NeurIPS23
  Workshop on Diffusion Models [arXiv:2311.10892]</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spiking Wavelet Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11138v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11138v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuetong Fang, Ziqing Wang, Lingfeng Zhang, Jiahang Cao, Honglei Chen, Renjing Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking neural networks (SNNs) offer an energy-efficient alternative to
conventional deep learning by mimicking the event-driven processing of the
brain. Incorporating the Transformers with SNNs has shown promise for accuracy,
yet it is incompetent to capture high-frequency patterns like moving edge and
pixel-level brightness changes due to their reliance on global self-attention
operations. Porting frequency representations in SNN is challenging yet crucial
for event-driven vision. To address this issue, we propose the Spiking Wavelet
Transformer (SWformer), an attention-free architecture that effectively learns
comprehensive spatial-frequency features in a spike-driven manner by leveraging
the sparse wavelet transform. The critical component is a Frequency-Aware Token
Mixer (FATM) with three branches: 1) spiking wavelet learner for
spatial-frequency domain learning, 2) convolution-based learner for spatial
feature extraction, and 3) spiking pointwise convolution for cross-channel
information aggregation. We also adopt negative spike dynamics to strengthen
the frequency representation further. This enables the SWformer to outperform
vanilla Spiking Transformers in capturing high-frequency visual components, as
evidenced by our empirical results. Experiments on both static and neuromorphic
datasets demonstrate SWformer's effectiveness in capturing spatial-frequency
patterns in a multiplication-free, event-driven fashion, outperforming
state-of-the-art SNNs. SWformer achieves an over 50% reduction in energy
consumption, a 21.1% reduction in parameter count, and a 2.40% performance
improvement on the ImageNet dataset compared to vanilla Spiking Transformers.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Correction of Pseudo Log-Likelihood Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18127v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18127v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shi Feng, Nuoya Xiong, Zhijie Zhang, Wei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pseudo log-likelihood is a type of maximum likelihood estimation (MLE) method
used in various fields including contextual bandits, influence maximization of
social networks, and causal bandits. However, in previous literature
\citep{li2017provably, zhang2022online, xiong2022combinatorial,
feng2023combinatorial1, feng2023combinatorial2}, the log-likelihood function
may not be bounded, which may result in the algorithm they proposed not
well-defined. In this paper, we give a counterexample that the maximum pseudo
log-likelihood estimation fails and then provide a solution to correct the
algorithms in \citep{li2017provably, zhang2022online, xiong2022combinatorial,
feng2023combinatorial1, feng2023combinatorial2}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Goal-Oriented Bayesian Optimal Experimental Design for Nonlinear Models
  using Markov Chain Monte Carlo 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18072v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18072v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijie Zhong, Wanggang Shen, Tommie Catanach, Xun Huan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimal experimental design (OED) provides a systematic approach to quantify
and maximize the value of experimental data. Under a Bayesian approach,
conventional OED maximizes the expected information gain (EIG) on model
parameters. However, we are often interested in not the parameters themselves,
but predictive quantities of interest (QoIs) that depend on the parameters in a
nonlinear manner. We present a computational framework of predictive
goal-oriented OED (GO-OED) suitable for nonlinear observation and prediction
models, which seeks the experimental design providing the greatest EIG on the
QoIs. In particular, we propose a nested Monte Carlo estimator for the QoI EIG,
featuring Markov chain Monte Carlo for posterior sampling and kernel density
estimation for evaluating the posterior-predictive density and its
Kullback-Leibler divergence from the prior-predictive. The GO-OED design is
then found by maximizing the EIG over the design space using Bayesian
optimization. We demonstrate the effectiveness of the overall nonlinear GO-OED
method, and illustrate its differences versus conventional non-GO-OED, through
various test problems and an application of sensor placement for source
inversion in a convection-diffusion field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Unreasonable Ineffectiveness of the Deeper Layers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17887v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17887v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso, Daniel A. Roberts
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We empirically study a simple layer-pruning strategy for popular families of
open-weight pretrained LLMs, finding minimal degradation of performance on
different question-answering benchmarks until after a large fraction (up to
half) of the layers are removed. To prune these models, we identify the optimal
block of layers to prune by considering similarity across layers; then, to
"heal" the damage, we perform a small amount of finetuning. In particular, we
use parameter-efficient finetuning (PEFT) methods, specifically quantization
and Low Rank Adapters (QLoRA), such that each of our experiments can be
performed on a single A100 GPU. From a practical perspective, these results
suggest that layer pruning methods can complement other PEFT strategies to
further reduce computational resources of finetuning on the one hand, and can
improve the memory and latency of inference on the other hand. From a
scientific perspective, the robustness of these LLMs to the deletion of layers
implies either that current pretraining methods are not properly leveraging the
parameters in the deeper layers of the network or that the shallow layers play
a critical role in storing knowledge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 + 10 pages, 5 + 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Counterfactual Fairness through Transforming Data Orthogonal to Bias 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17852v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17852v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuyi Chen, Shixiang Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models have shown exceptional prowess in solving complex
issues across various domains. Nonetheless, these models can sometimes exhibit
biased decision-making, leading to disparities in treatment across different
groups. Despite the extensive research on fairness, the nuanced effects of
multivariate and continuous sensitive variables on decision-making outcomes
remain insufficiently studied. We introduce a novel data pre-processing
algorithm, Orthogonal to Bias (OB), designed to remove the influence of a group
of continuous sensitive variables, thereby facilitating counterfactual fairness
in machine learning applications. Our approach is grounded in the assumption of
a jointly normal distribution within a structural causal model (SCM), proving
that counterfactual fairness can be achieved by ensuring the data is
uncorrelated with sensitive variables. The OB algorithm is model-agnostic,
catering to a wide array of machine learning models and tasks, and includes a
sparse variant to enhance numerical stability through regularization. Through
empirical evaluation on simulated and real-world datasets - including the adult
income and the COMPAS recidivism datasets - our methodology demonstrates its
capacity to enable fairer outcomes without compromising accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Multilevel Modelling of Train Passing Events on the
  Staffordshire Bridge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17820v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17820v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lawrence A. Bull, Chiho Jeon, Mark Girolami, Andrew Duncan, Jennifer Schooling, Miguel Bravo Haro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We suggest a multilevel model, to represent aggregate train-passing events
from the Staffordshire bridge monitoring system. We formulate a combined model
from simple units, representing strain envelopes (of each train passing) for
two types of commuter train. The measurements are treated as a longitudinal
dataset and represented with a (low-rank approximation) hierarchical Gaussian
process. For each unit in the combined model, we encode domain expertise as
boundary condition constraints and work towards a general representation of the
strain response. Looking forward, this should allow for the simulation of train
types that were previously unobserved in the training data. For example, trains
with more passengers or freights with a heavier payload. The strain event
simulations are valuable since they can inform further experiments (including
FEM calibration, fatigue analysis, or design) to test the bridge in
hypothesised scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Benefits of Over-parameterization for Out-of-Distribution
  Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17592v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17592v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Hao, Yong Lin, Difan Zou, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, machine learning models have achieved success based on the
independently and identically distributed assumption. However, this assumption
can be easily violated in real-world applications, leading to the
Out-of-Distribution (OOD) problem. Understanding how modern over-parameterized
DNNs behave under non-trivial natural distributional shifts is essential, as
current theoretical understanding is insufficient. Existing theoretical works
often provide meaningless results for over-parameterized models in OOD
scenarios or even contradict empirical findings. To this end, we are
investigating the performance of the over-parameterized model in terms of OOD
generalization under the general benign overfitting conditions. Our analysis
focuses on a random feature model and examines non-trivial natural
distributional shifts, where the benign overfitting estimators demonstrate a
constant excess OOD loss, despite achieving zero excess in-distribution (ID)
loss. We demonstrate that in this scenario, further increasing the model's
parameterization can significantly reduce the OOD loss. Intuitively, the
variance term of ID loss remains low due to orthogonality of long-tail
features, meaning overfitting noise during training generally doesn't raise
testing loss. However, in OOD cases, distributional shift increases the
variance term. Thankfully, the inherent shift is unrelated to individual x,
maintaining the orthogonality of long-tail features. Expanding the hidden
dimension can additionally improve this orthogonality by mapping the features
into higher-dimensional spaces, thereby reducing the variance term. We further
show that model ensembles also improve OOD loss, akin to increasing model
capacity. These insights explain the empirical phenomenon of enhanced OOD
generalization through model ensembles, supported by consistent simulations
with theoretical results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Test-time Adaptation Meets Image Enhancement: Improving Accuracy via
  Uncertainty-aware Logit Switching <span class="chip">IJCNN2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17423v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17423v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shohei Enomoto, Naoya Hasegawa, Kazuki Adachi, Taku Sasaki, Shin'ya Yamaguchi, Satoshi Suzuki, Takeharu Eda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks have achieved remarkable success in a variety of
computer vision applications. However, there is a problem of degrading accuracy
when the data distribution shifts between training and testing. As a solution
of this problem, Test-time Adaptation~(TTA) has been well studied because of
its practicality. Although TTA methods increase accuracy under distribution
shift by updating the model at test time, using high-uncertainty predictions is
known to degrade accuracy. Since the input image is the root of the
distribution shift, we incorporate a new perspective on enhancing the input
image into TTA methods to reduce the prediction's uncertainty. We hypothesize
that enhancing the input image reduces prediction's uncertainty and increase
the accuracy of TTA methods. On the basis of our hypothesis, we propose a novel
method: Test-time Enhancer and Classifier Adaptation~(TECA). In TECA, the
classification model is combined with the image enhancement model that
transforms input images into recognition-friendly ones, and these models are
updated by existing TTA methods. Furthermore, we found that the prediction from
the enhanced image does not always have lower uncertainty than the prediction
from the original image. Thus, we propose logit switching, which compares the
uncertainty measure of these predictions and outputs the lower one. In our
experiments, we evaluate TECA with various TTA methods and show that TECA
reduces prediction's uncertainty and increases accuracy of TTA methods despite
having no hyperparameters and little parameter overhead.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IJCNN2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On permutation-invariant neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17410v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17410v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masanari Kimura, Ryotaro Shimizu, Yuki Hirakawa, Ryosuke Goto, Yuki Saito
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional machine learning algorithms have traditionally been designed
under the assumption that input data follows a vector-based format, with an
emphasis on vector-centric paradigms. However, as the demand for tasks
involving set-based inputs has grown, there has been a paradigm shift in the
research community towards addressing these challenges. In recent years, the
emergence of neural network architectures such as Deep Sets and Transformers
has presented a significant advancement in the treatment of set-based data.
These architectures are specifically engineered to naturally accommodate sets
as input, enabling more effective representation and processing of set
structures. Consequently, there has been a surge of research endeavors
dedicated to exploring and harnessing the capabilities of these architectures
for various tasks involving the approximation of set functions. This
comprehensive survey aims to provide an overview of the diverse problem
settings and ongoing research efforts pertaining to neural networks that
approximate set functions. By delving into the intricacies of these approaches
and elucidating the associated challenges, the survey aims to equip readers
with a comprehensive understanding of the field. Through this comprehensive
perspective, we hope that researchers can gain valuable insights into the
potential applications, inherent limitations, and future directions of
set-based neural networks. Indeed, from this survey we gain two insights: i)
Deep Sets and its variants can be generalized by differences in the aggregation
function, and ii) the behavior of Deep Sets is sensitive to the choice of the
aggregation function. From these observations, we show that Deep Sets, one of
the well-known permutation-invariant neural networks, can be generalized in the
sense of a quasi-arithmetic mean.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Analysis of Switchback Designs in <span class="highlight-title">Reinforcement</span> Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17285v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17285v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianglin Wen, Chengchun Shi, Ying Yang, Niansheng Tang, Hongtu Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper offers a detailed investigation of switchback designs in A/B
testing, which alternate between baseline and new policies over time. Our aim
is to thoroughly evaluate the effects of these designs on the accuracy of their
resulting average treatment effect (ATE) estimators. We propose a novel "weak
signal analysis" framework, which substantially simplifies the calculations of
the mean squared errors (MSEs) of these ATEs in Markov decision process
environments. Our findings suggest that (i) when the majority of reward errors
are positively correlated, the switchback design is more efficient than the
alternating-day design which switches policies in a daily basis. Additionally,
increasing the frequency of policy switches tends to reduce the MSE of the ATE
estimator. (ii) When the errors are uncorrelated, however, all these designs
become asymptotically equivalent. (iii) In cases where the majority of errors
are negative correlated, the alternating-day design becomes the optimal choice.
These insights are crucial, offering guidelines for practitioners on designing
experiments in A/B testing. Our analysis accommodates a variety of policy value
estimators, including model-based estimators, least squares temporal difference
learning estimators, and double reinforcement learning estimators, thereby
offering a comprehensive understanding of optimal design strategies for policy
evaluation in reinforcement learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Selective inference using randomized group lasso estimators for general
  models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.13829v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.13829v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiling Huang, Sarah Pirenne, Snigdha Panigrahi, Gerda Claeskens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Selective inference methods are developed for group lasso estimators for use
with a wide class of distributions and loss functions. The method includes the
use of exponential family distributions, as well as quasi-likelihood modeling
for overdispersed count data, for example, and allows for categorical or
grouped covariates as well as continuous covariates. A randomized
group-regularized optimization problem is studied. The added randomization
allows us to construct a post-selection likelihood which we show to be adequate
for selective inference when conditioning on the event of the selection of the
grouped covariates. This likelihood also provides a selective point estimator,
accounting for the selection by the group lasso. Confidence regions for the
regression parameters in the selected model take the form of Wald-type regions
and are shown to have bounded volume. The selective inference method for
grouped lasso is illustrated on data from the national health and nutrition
examination survey while simulations showcase its behaviour and favorable
comparison with other methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>64pages, 4 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GPT-4's assessment of its performance in a USMLE-based case study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09654v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09654v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Uttam Dhakal, Aniket Kumar Singh, Suman Devkota, Yogesh Sapkota, Bishal Lamichhane, Suprinsa Paudyal, Chandra Dhakal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates GPT-4's assessment of its performance in healthcare
applications. A simple prompting technique was used to prompt the LLM with
questions taken from the United States Medical Licensing Examination (USMLE)
questionnaire and it was tasked to evaluate its confidence score before posing
the question and after asking the question. The questionnaire was categorized
into two groups-questions with feedback (WF) and questions with no feedback(NF)
post-question. The model was asked to provide absolute and relative confidence
scores before and after each question. The experimental findings were analyzed
using statistical tools to study the variability of confidence in WF and NF
groups. Additionally, a sequential analysis was conducted to observe the
performance variation for the WF and NF groups. Results indicate that feedback
influences relative confidence but doesn't consistently increase or decrease
it. Understanding the performance of LLM is paramount in exploring its utility
in sensitive areas like healthcare. This study contributes to the ongoing
discourse on the reliability of AI, particularly of LLMs like GPT-4, within
healthcare, offering insights into how feedback mechanisms might be optimized
to enhance AI-assisted medical education and decision support.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simulating counterfactuals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.15328v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.15328v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juha Karvanen, Santtu Tikka, Matti Vihola
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counterfactual inference considers a hypothetical intervention in a parallel
world that shares some evidence with the factual world. If the evidence
specifies a conditional distribution on a manifold, counterfactuals may be
analytically intractable. We present an algorithm for simulating values from a
counterfactual distribution where conditions can be set on both discrete and
continuous variables. We show that the proposed algorithm can be presented as a
particle filter leading to asymptotically valid inference. The algorithm is
applied to fairness analysis in credit-scoring.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An optimal control perspective on diffusion-based generative modeling <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.01364v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.01364v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julius Berner, Lorenz Richter, Karen Ullrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We establish a connection between stochastic optimal control and generative
models based on stochastic differential equations (SDEs), such as recently
developed diffusion probabilistic models. In particular, we derive a
Hamilton-Jacobi-Bellman equation that governs the evolution of the
log-densities of the underlying SDE marginals. This perspective allows to
transfer methods from optimal control theory to generative modeling. First, we
show that the evidence lower bound is a direct consequence of the well-known
verification theorem from control theory. Further, we can formulate
diffusion-based generative modeling as a minimization of the Kullback-Leibler
divergence between suitable measures in path space. Finally, we develop a novel
diffusion-based method for sampling from unnormalized densities -- a problem
frequently occurring in statistics and computational sciences. We demonstrate
that our time-reversed diffusion sampler (DIS) can outperform other
diffusion-based sampling approaches on multiple numerical examples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for oral presentation at NeurIPS 2022 Workshop on
  Score-Based Methods</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentially private multivariate medians 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.06459v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.06459v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kelly Ramsay, Aukosh Jagannath, Shoja'eddin Chenouri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Statistical tools which satisfy rigorous privacy guarantees are necessary for
modern data analysis. It is well-known that robustness against contamination is
linked to differential privacy. Despite this fact, using multivariate medians
for differentially private and robust multivariate location estimation has not
been systematically studied. We develop novel finite-sample performance
guarantees for differentially private multivariate depth-based medians, which
are essentially sharp. Our results cover commonly used depth functions, such as
the halfspace (or Tukey) depth, spatial depth, and the integrated dual depth.
We show that under Cauchy marginals, the cost of heavy-tailed location
estimation outweighs the cost of privacy. We demonstrate our results
numerically using a Gaussian contamination model in dimensions up to d = 100,
and compare them to a state-of-the-art private mean estimation algorithm. As a
by-product of our investigation, we prove concentration inequalities for the
output of the exponential mechanism about the maximizer of the population
objective function. This bound applies to objective functions that satisfy a
mild regularity condition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>42 pages, 3 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal Design of Volt/VAR Control Rules of Inverters using Deep
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.09557v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.09557v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sarthak Gupta, Vassilis Kekatos, Spyros Chatzivasileiadis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distribution grids are challenged by rapid voltage fluctuations induced by
variable power injections from distributed energy resources (DERs). To regulate
voltage, the IEEE Standard 1547 recommends each DER inject reactive power
according to piecewise-affine Volt/VAR control rules. Although the standard
suggests a default shape, the rule can be customized per bus. This task of
optimal rule design (ORD) is challenging as Volt/VAR rules introduce nonlinear
dynamics, and lurk trade-offs between stability and steady-state voltage
profiles. ORD is formulated as a mixed-integer nonlinear program (MINLP), but
scales unfavorably with the problem size. Towards a more efficient solution, we
reformulate ORD as a deep learning problem. The idea is to design a DNN that
emulates Volt/VAR dynamics. The DNN takes grid scenarios as inputs, rule
parameters as weights, and outputs equilibrium voltages. Optimal rule
parameters can be found by training the DNN so its output approaches unity for
various scenarios. The DNN is only used to optimize rules and is never employed
in the field. While dealing with ORD, we also review and expand on stability
conditions and convergence rates for Volt/VAR dynamics on single- and
multi-phase feeders. Tests showcase the merit of DNN-based ORD by benchmarking
it against its MINLP counterpart.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in the IEEE Trans. on Smart Grid</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spatiotemporal Besov Priors for Bayesian Inverse Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.16378v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.16378v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiwei Lan, Mirjeta Pasha, Shuyi Li, Weining Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fast development in science and technology has driven the need for proper
statistical tools to capture special data features such as abrupt changes or
sharp contrast. Many inverse problems in data science require spatiotemporal
solutions derived from a sequence of time-dependent objects with these spatial
features, e.g., dynamic reconstruction of computerized tomography (CT) images
with edges. Conventional methods based on Gaussian processes (GP) often fall
short in providing satisfactory solutions since they tend to offer over-smooth
priors. Recently, the Besov process (BP), defined by wavelet expansions with
random coefficients, has emerged as a more suitable prior for Bayesian inverse
problems of this nature. While BP excels in handling spatial inhomogeneity, it
does not automatically incorporate temporal correlation inherited in the
dynamically changing objects. In this paper, we generalize BP to a novel
spatiotemporal Besov process (STBP) by replacing the random coefficients in the
series expansion with stochastic time functions as Q-exponential process (Q-EP)
which governs the temporal correlation structure. We thoroughly investigate the
mathematical and statistical properties of STBP. A white-noise representation
of STBP is also proposed to facilitate the inference. Simulations, two
limited-angle CT reconstruction examples and a highly non-linear inverse
problem involving Navier-Stokes equation are used to demonstrate the advantage
of the proposed STBP in preserving spatial features while accounting for
temporal changes compared with the classic STGP and a time-uncorrelated
approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>47 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transport meets Variational Inference: Controlled Monte Carlo Diffusions <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.01050v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.01050v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francisco Vargas, Shreyas Padhy, Denis Blessing, Nikolas Nüsken
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Connecting optimal transport and variational inference, we present a
principled and systematic framework for sampling and generative modelling
centred around divergences on path space. Our work culminates in the
development of the \emph{Controlled Monte Carlo Diffusion} sampler (CMCD) for
Bayesian computation, a score-based annealing technique that crucially adapts
both forward and backward dynamics in a diffusion model. On the way, we clarify
the relationship between the EM-algorithm and iterative proportional fitting
(IPF) for Schr{\"o}dinger bridges, deriving as well a regularised objective
that bypasses the iterative bottleneck of standard IPF-updates. Finally, we
show that CMCD has a strong foundation in the Jarzinsky and Crooks identities
from statistical physics, and that it convincingly outperforms competing
approaches across a wide array of experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Workshop on New Frontiers in Learning, Control, and Dynamical Systems
  at the International Conference on Machine Learning (ICML), Honolulu, Hawaii,
  USA, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Semismooth Newton Stochastic Proximal Point Algorithm with Variance
  Reduction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.00406v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.00406v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andre Milzarek, Fabian Schaipp, Michael Ulbrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop an implementable stochastic proximal point (SPP) method for a
class of weakly convex, composite optimization problems. The proposed
stochastic proximal point algorithm incorporates a variance reduction mechanism
and the resulting SPP updates are solved using an inexact semismooth Newton
framework. We establish detailed convergence results that take the inexactness
of the SPP steps into account and that are in accordance with existing
convergence guarantees of (proximal) stochastic variance-reduced gradient
methods. Numerical experiments show that the proposed algorithm competes
favorably with other state-of-the-art methods and achieves higher robustness
with respect to the step size selection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bayesian data-driven discovery of partial differential equations with
  variable coefficients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.01432v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.01432v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aoxue Chen, Yifan Du, Liyao Mars Gao, Guang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The discovery of Partial Differential Equations (PDEs) is an essential task
for applied science and engineering. However, data-driven discovery of PDEs is
generally challenging, primarily stemming from the sensitivity of the
discovered equation to noise and the complexities of model selection. In this
work, we propose an advanced Bayesian sparse learning algorithm for PDE
discovery with variable coefficients, predominantly when the coefficients are
spatially or temporally dependent. Specifically, we apply threshold Bayesian
group Lasso regression with a spike-and-slab prior (tBGL-SS) and leverage a
Gibbs sampler for Bayesian posterior estimation of PDE coefficients. This
approach not only enhances the robustness of point estimation with valid
uncertainty quantification but also relaxes the computational burden from
Bayesian inference through the integration of coefficient thresholds as an
approximate MCMC method. Moreover, from the quantified uncertainties, we
propose a Bayesian total error bar criteria for model selection, which
outperforms classic metrics including the root mean square and the Akaike
information criterion. The capability of this method is illustrated by the
discovery of several classical benchmark PDEs with spatially or temporally
varying coefficients from solution data obtained from the reference
simulations. In the experiments, we show that the tBGL-SS method is more robust
than the baseline methods under noisy environments and provides better model
selection criteria along the regularization path.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Riemannian Laplace Approximation with the Fisher Metric <span class="chip">AISTATS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.02766v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.02766v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanlin Yu, Marcelo Hartmann, Bernardo Williams, Mark Girolami, Arto Klami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Laplace's method approximates a target density with a Gaussian distribution
at its mode. It is computationally efficient and asymptotically exact for
Bayesian inference due to the Bernstein-von Mises theorem, but for complex
targets and finite-data posteriors it is often too crude an approximation. A
recent generalization of the Laplace Approximation transforms the Gaussian
approximation according to a chosen Riemannian geometry providing a richer
approximation family, while still retaining computational efficiency. However,
as shown here, its properties depend heavily on the chosen metric, indeed the
metric adopted in previous work results in approximations that are overly
narrow as well as being biased even at the limit of infinite data. We correct
this shortcoming by developing the approximation family further, deriving two
alternative variants that are exact at the limit of infinite data, extending
the theoretical analysis of the method, and demonstrating practical
improvements in a range of experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AISTATS 2024, with additional fixes</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identification and multiply robust estimation in causal mediation
  analysis across principal strata 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.10025v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.10025v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Cheng, Fan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider assessing causal mediation in the presence of a post-treatment
event (examples include noncompliance, a clinical event, or a terminal event).
We identify natural mediation effects for the entire study population and for
each principal stratum characterized by the joint potential values of the
post-treatment event. We derive efficient influence functions for each
mediation estimand, which motivate a set of multiply robust estimators for
inference. The multiply robust estimators are consistent under four types of
misspecifications and are efficient when all nuisance models are correctly
specified. We illustrate our methods via simulations and two real data
examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Prediction Error Estimation in Random Forests 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.00736v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.00736v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ian Krupkin, Johanna Hardin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, error estimates of classification Random Forests are
quantitatively assessed. Based on the initial theoretical framework built by
Bates et al. (2023), the true error rate and expected error rate are
theoretically and empirically investigated in the context of a variety of error
estimation methods common to Random Forests. We show that in the classification
case, Random Forests' estimates of prediction error is closer on average to the
true error rate instead of the average prediction error. This is opposite the
findings of Bates et al. (2023) which are given for logistic regression. We
further show that our result holds across different error estimation strategies
such as cross-validation, bagging, and data splitting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2104.00673 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multivariate Gaussian Approximation for Random Forest via Region-based
  Stabilization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09960v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09960v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoyang Shi, Chinmoy Bhattacharjee, Krishnakumar Balasubramanian, Wolfgang Polonik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We derive Gaussian approximation bounds for random forest predictions based
on a set of training points given by a Poisson process, under fairly mild
regularity assumptions on the data generating process. Our approach is based on
the key observation that the random forest predictions satisfy a certain
geometric property called region-based stabilization. In the process of
developing our results for the random forest, we also establish a probabilistic
result, which might be of independent interest, on multivariate Gaussian
approximation bounds for general functionals of Poisson process that are
region-based stabilizing. This general result makes use of the Malliavin-Stein
method, and is potentially applicable to various related statistical problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PPI++: Efficient Prediction-Powered Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01453v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01453v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anastasios N. Angelopoulos, John C. Duchi, Tijana Zrnic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present PPI++: a computationally lightweight methodology for estimation
and inference based on a small labeled dataset and a typically much larger
dataset of machine-learning predictions. The methods automatically adapt to the
quality of available predictions, yielding easy-to-compute confidence sets --
for parameters of any dimensionality -- that always improve on classical
intervals using only the labeled data. PPI++ builds on prediction-powered
inference (PPI), which targets the same problem setting, improving its
computational and statistical efficiency. Real and synthetic experiments
demonstrate the benefits of the proposed adaptations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at https://github.com/aangelopoulos/ppi_py</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Omega: Optimistic EMA Gradients <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.07905v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.07905v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan Ramirez, Rohan Sukumaran, Quentin Bertrand, Gauthier Gidel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stochastic min-max optimization has gained interest in the machine learning
community with the advancements in GANs and adversarial training. Although game
optimization is fairly well understood in the deterministic setting, some
issues persist in the stochastic regime. Recent work has shown that stochastic
gradient descent-ascent methods such as the optimistic gradient are highly
sensitive to noise or can fail to converge. Although alternative strategies
exist, they can be prohibitively expensive. We introduce Omega, a method with
optimistic-like updates that mitigates the impact of noise by incorporating an
EMA of historic gradients in its update rule. We also explore a variation of
this algorithm that incorporates momentum. Although we do not provide
convergence guarantees, our experiments on stochastic games show that Omega
outperforms the optimistic gradient method when applied to linear players.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Oral at the LatinX in AI workshop @ ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-25T00:00:00Z">2024-03-25</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computer Science and Game Theory
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Economic DAO Governance: A Contestable Control Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16980v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16980v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeff Strnad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this article, we propose a new form of DAO governance that uses a
sequential auction mechanism to overcome entrenched control issues that have
emerged for DAOs by creating a regime of temporary contestable control. The
mechanism avoids potential public choice problems inherent in voting approaches
but at the same time provides a vehicle that can enhance and secure value than
inheres to DAO voting and other DAO non-market governance procedures. It is
robust to empty voting and is code feasible. It facilitates the ability of DAOs
to meet their normative and operational goals in the face of diverse regulatory
approaches. Designed to shift control to the party with the most promising
business plan, at the same time it distributes surplus in a way that tends to
promote investment by other parties.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>84 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do LLM <span class="highlight-title">Agent</span>s Have Regret? A Case Study in Online Learning and <span class="highlight-title">Game</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16843v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16843v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chanwoo Park, Xiangyu Liu, Asuman Ozdaglar, Kaiqing Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have been increasingly employed for
(interactive) decision-making, via the development of LLM-based autonomous
agents. Despite their emerging successes, the performance of LLM agents in
decision-making has not been fully investigated through quantitative metrics,
especially in the multi-agent setting when they interact with each other, a
typical scenario in real-world LLM-agent applications. To better understand the
limits of LLM agents in these interactive environments, we propose to study
their interactions in benchmark decision-making settings in online learning and
game theory, through the performance metric of \emph{regret}. We first
empirically study the {no-regret} behaviors of LLMs in canonical
(non-stationary) online learning problems, as well as the emergence of
equilibria when LLM agents interact through playing repeated games. We then
provide some theoretical insights into the no-regret behaviors of LLM agents,
under certain assumptions on the supervised pre-training and the rationality
model of human decision-makers who generate the data. Notably, we also identify
(simple) cases where advanced LLMs such as GPT-4 fail to be no-regret. To
promote the no-regret behaviors, we propose a novel \emph{unsupervised}
training loss of \emph{regret-loss}, which, in contrast to the supervised
pre-training loss, does not require the labels of (optimal) actions. We then
establish the statistical guarantee of generalization bound for regret-loss
minimization, followed by the optimization guarantee that minimizing such a
loss may automatically lead to known no-regret learning algorithms. Our further
experiments demonstrate the effectiveness of our regret-loss, especially in
addressing the above ``regrettable'' cases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Method for Finding Optimal Strategies in Chopstick Auctions
  with Uniform Objects Values <span class="chip">AAMAS-24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16799v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16799v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stanisław Kaźmierowski, Marcin Dziubiński
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an algorithm for computing Nash equilibria (NE) in a class of
conflicts with multiple battlefields with uniform battlefield values and a
non-linear aggregation function. By expanding the symmetrization idea of Hart
[9], proposed for the Colonel Blotto game, to the wider class of symmetric
conflicts with multiple battlefields, we reduce the number of strategies of the
players by an exponential factor. We propose a clash matrix algorithm which
allows for computing the payoffs in the symmetrized model in polynomial time.
Combining symmetrization and clash matrix algorithm with the double oracle
algorithm we obtain an algorithm for computing NE in the models in question
that achieves a significant speed-up as compared to the standard, LP-based,
approach. We also introduce a heuristic to further speed up the process.
Overall, our approach offers an efficient and novel method for computing NE in
a specific class of conflicts, with potential practical applications in various
fields.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for AAMAS-24 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A new social welfare function with a number of desirable properties 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16373v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16373v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fujun Hou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  By relaxing the dominating set in three ways (e.g., from "each member beats
every non-member" to "each member beats or ties every non-member, with an
additional requirement that at least one member beat every non-member"), we
propose a new social welfare function, which satisfies a number of desirable
properties including Condorcet winner principle, Condorcet loser principle,
strong Gehrlein-stability (hence Smith set principle), anonymity, neutrality,
weak Pareto, strong Pareto, non-dictatorship, and [independence of irrelevant
alternatives (IIA) when the pairwise majority relation is an ordering on the
alternative set]. If the pairwise majority relation is complete and transitive,
the proposed method yields a collective preference relation that coincides with
the input majority relation. It thus shares the same collective preference
function on the dichotomous domain with the approval voting and the majority
voting. It runs in polynomial time and thus possesses a competitive advantage
over a number of computationally intractable voting rules such as the Dodgson's
rule, the Kemeny's rule, the Slater's rule, the Banks rule, and the Schwartz's
tournament equilibrium set (TEQ) rule. When it is used in tournaments, its
winner belongs to the uncovered set, the top cycle set, the Smith set, and the
Schwartz set. In addition, in a tournament where the number of alternatives is
not more than 4, its winner set is a subset, sometimes proper, of the Copeland
winner set. Whether this attractive argument is still valid in
four-more-alternative tournaments remains an open question.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A new social choice function (and a corresponding social welfare
  function) is proposed. It has a number of desirable properties. An open
  question is also posed</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multilinear formulations for computing Nash equilibrium of multi-player
  matrix <span class="highlight-title">game</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.03406v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.03406v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miriam Fischer, Akshay Gupte
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present multilinear and mixed-integer multilinear programs to find a Nash
equilibrium in multi-player noncooperative games. We compare the formulations
to common algorithms in Gambit, and conclude that a multilinear feasibility
program finds a Nash equilibrium faster than any of the methods we compare it
to, including the quantal response equilibrium method, which is recommended for
large games. Hence, the multilinear feasibility program is an alternative
method to find a Nash equilibrium in multi-player games, and outperforms many
common algorithms. The mixed-integer formulations are generalisations of known
mixed-integer programs for two-player games, however unlike two-player games,
these mixed-integer programs do not give better performance than existing
algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 page conference paper accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Project-Fair and Truthful Mechanisms for Budget Aggregation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.02613v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.02613v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rupert Freeman, Ulrike Schmidt-Kraepelin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the budget aggregation problem in which a set of strategic voters
must split a finite divisible resource (such as money or time) among a set of
competing projects. Our goal is twofold: We seek truthful mechanisms that
provide fairness guarantees to the projects. For the first objective, we focus
on the class of moving phantom mechanisms [Freeman et al., 2021], which are --
to this day -- essentially the only known truthful mechanisms in this setting.
For project fairness, we consider the mean division as a fair baseline, and
bound the maximum difference between the funding received by any project and
this baseline. We propose a novel and simple moving phantom mechanism that
provides optimal project fairness guarantees. As a corollary of our results, we
show that our new mechanism minimizes the $\ell_1$ distance to the mean (a
measure suggested by Caragiannis et al. [2022]) for three projects and gives
the first non-trivial bounds on this quantity for more than three projects.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Objective Quality-Diversity for Crystal Structure Prediction <span class="chip">GECCO 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17164v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17164v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hannah Janmohamed, Marta Wolinska, Shikha Surana, Thomas Pierrot, Aron Walsh, Antoine Cully
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crystal structures are indispensable across various domains, from batteries
to solar cells, and extensive research has been dedicated to predicting their
properties based on their atomic configurations. However, prevailing Crystal
Structure Prediction methods focus on identifying the most stable solutions
that lie at the global minimum of the energy function. This approach overlooks
other potentially interesting materials that lie in neighbouring local minima
and have different material properties such as conductivity or resistance to
deformation. By contrast, Quality-Diversity algorithms provide a promising
avenue for Crystal Structure Prediction as they aim to find a collection of
high-performing solutions that have diverse characteristics. However, it may
also be valuable to optimise for the stability of crystal structures alongside
other objectives such as magnetism or thermoelectric efficiency. Therefore, in
this work, we harness the power of Multi-Objective Quality-Diversity algorithms
in order to find crystal structures which have diverse features and achieve
different trade-offs of objectives. We analyse our approach on 5 crystal
systems and demonstrate that it is not only able to re-discover known real-life
structures, but also find promising new ones. Moreover, we propose a method for
illuminating the objective space to gain an understanding of what trade-offs
can be achieved.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted GECCO 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Backpropagation through space, time, and the brain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16933v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16933v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Ellenberger, Paul Haider, Jakob Jordan, Kevin Max, Ismael Jaras, Laura Kriener, Federico Benitez, Mihai A. Petrovici
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective learning in neuronal networks requires the adaptation of individual
synapses given their relative contribution to solving a task. However, physical
neuronal systems -- whether biological or artificial -- are constrained by
spatio-temporal locality. How such networks can perform efficient credit
assignment, remains, to a large extent, an open question. In Machine Learning,
the answer is almost universally given by the error backpropagation algorithm,
through both space (BP) and time (BPTT). However, BP(TT) is well-known to rely
on biologically implausible assumptions, in particular with respect to
spatiotemporal (non-)locality, while forward-propagation models such as
real-time recurrent learning (RTRL) suffer from prohibitive memory constraints.
We introduce Generalized Latent Equilibrium (GLE), a computational framework
for fully local spatio-temporal credit assignment in physical, dynamical
networks of neurons. We start by defining an energy based on neuron-local
mismatches, from which we derive both neuronal dynamics via stationarity and
parameter dynamics via gradient descent. The resulting dynamics can be
interpreted as a real-time, biologically plausible approximation of BPTT in
deep cortical networks with continuous-time neuronal dynamics and continuously
active, local synaptic plasticity. In particular, GLE exploits the ability of
biological neurons to phase-shift their output rate with respect to their
membrane potential, which is essential in both directions of information
propagation. For the forward computation, it enables the mapping of
time-continuous inputs to neuronal space, performing an effective
spatiotemporal convolution. For the backward computation, it permits the
temporal inversion of feedback signals, which consequently approximate the
adjoint states necessary for useful parameter updates.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cluster-Based Normalization Layer for Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16798v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16798v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bilal Faye, Hanane Azzag, Mustapha Lebbah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning faces significant challenges during the training of neural
networks, including internal covariate shift, label shift, vanishing/exploding
gradients, overfitting, and computational complexity. While conventional
normalization methods, such as Batch Normalization, aim to tackle some of these
issues, they often depend on assumptions that constrain their adaptability.
Mixture Normalization faces computational hurdles in its pursuit of handling
multiple Gaussian distributions. This paper introduces Cluster-Based
Normalization (CB-Norm) in two variants - Supervised Cluster-Based
Normalization (SCB-Norm) and Unsupervised Cluster-Based Normalization
(UCB-Norm) - proposing a groundbreaking one-step normalization approach.
CB-Norm leverages a Gaussian mixture model to specifically address challenges
related to gradient stability and learning acceleration. For SCB-Norm, a
supervised variant, the novel mechanism involves introducing predefined data
partitioning, termed clusters, to normalize activations based on the assigned
cluster. This cluster-driven approach creates a space that conforms to a
Gaussian mixture model. On the other hand, UCB-Norm, an unsupervised
counterpart, dynamically clusters neuron activations during training, adapting
to task-specific challenges without relying on predefined data partitions
(clusters). This dual approach ensures flexibility in addressing diverse
learning scenarios. CB-Norm innovatively uses a one-step normalization
approach, where parameters of each mixture component (cluster in activation
space) serve as weights for deep neural networks. This adaptive clustering
process tackles both clustering and resolution of deep neural network tasks
concurrently during training, signifying a notable advancement in the field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Graph Representation Learning with Attention-Driven Spiking
  Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17040v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17040v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huifeng Yin, Mingkun Xu, Jing Pei, Lei Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph representation learning has become a crucial task in machine learning
and data mining due to its potential for modeling complex structures such as
social networks, chemical compounds, and biological systems. Spiking neural
networks (SNNs) have recently emerged as a promising alternative to traditional
neural networks for graph learning tasks, benefiting from their ability to
efficiently encode and process temporal and spatial information. In this paper,
we propose a novel approach that integrates attention mechanisms with SNNs to
improve graph representation learning. Specifically, we introduce an attention
mechanism for SNN that can selectively focus on important nodes and
corresponding features in a graph during the learning process. We evaluate our
proposed method on several benchmark datasets and show that it achieves
comparable performance compared to existing graph learning techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding the Functional Roles of Modelling Components in Spiking
  Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huifeng Yin, Hanle Zheng, Jiayi Mao, Siyuan Ding, Xing Liu, Mingkun Xu, Yifan Hu, Jing Pei, Lei Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking neural networks (SNNs), inspired by the neural circuits of the brain,
are promising in achieving high computational efficiency with biological
fidelity. Nevertheless, it is quite difficult to optimize SNNs because the
functional roles of their modelling components remain unclear. By designing and
evaluating several variants of the classic model, we systematically investigate
the functional roles of key modelling components, leakage, reset, and
recurrence, in leaky integrate-and-fire (LIF) based SNNs. Through extensive
experiments, we demonstrate how these components influence the accuracy,
generalization, and robustness of SNNs. Specifically, we find that the leakage
plays a crucial role in balancing memory retention and robustness, the reset
mechanism is essential for uninterrupted temporal processing and computational
efficiency, and the recurrence enriches the capability to model complex
dynamics at a cost of robustness degradation. With these interesting
observations, we provide optimization suggestions for enhancing the performance
of SNNs in different scenarios. This work deepens the understanding of how SNNs
work, which offers valuable guidance for the development of more effective and
robust neuromorphic models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QKFormer: Hierarchical Spiking Transformer using Q-K Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenlin Zhou, Han Zhang, Zhaokun Zhou, Liutao Yu, Liwei Huang, Xiaopeng Fan, Li Yuan, Zhengyu Ma, Huihui Zhou, Yonghong Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking Transformers, which integrate Spiking Neural Networks (SNNs) with
Transformer architectures, have attracted significant attention due to their
potential for energy efficiency and high performance. However, existing models
in this domain still suffer from suboptimal performance. We introduce several
innovations to improve the performance: i) We propose a novel spike-form Q-K
attention mechanism, tailored for SNNs, which efficiently models the importance
of token or channel dimensions through binary vectors with linear complexity.
ii) We incorporate the hierarchical structure, which significantly benefits the
performance of both the brain and artificial neural networks, into spiking
transformers to obtain multi-scale spiking representation. iii) We design a
versatile and powerful patch embedding module with a deformed shortcut
specifically for spiking transformers. Together, we develop QKFormer, a
hierarchical spiking transformer based on Q-K attention with direct training.
QKFormer shows significantly superior performance over existing
state-of-the-art SNN models on various mainstream datasets. Notably, with
comparable size to Spikformer (66.34 M, 74.81%), QKFormer (64.96 M) achieves a
groundbreaking top-1 accuracy of 85.65% on ImageNet-1k, substantially
outperforming Spikformer by 10.84%. To our best knowledge, this is the first
time that directly training SNNs have exceeded 85% accuracy on ImageNet-1K. The
code and models are publicly available at
https://github.com/zhouchenlin2096/QKFormer
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, code: https://github.com/zhouchenlin2096/QKFormer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Large Language Model to Generate a Novel Metaheuristic
  Algorithm with CRISPE Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16417v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16417v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Zhong, Yuefeng Xu, Chao Zhang, Jun Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we borrow the large language model (LLM) ChatGPT-3.5 to
automatically and quickly design a new metaheuristic algorithm (MA) with only a
small amount of input. The novel animal-inspired MA named zoological search
optimization (ZSO) draws inspiration from the collective behaviors of animals
for solving continuous optimization problems. Specifically, the basic ZSO
algorithm involves two search operators: the prey-predator interaction operator
and the social flocking operator to balance exploration and exploitation well.
Besides, the standard prompt engineering framework CRISPE (i.e., Capacity and
Role, Insight, Statement, Personality, and Experiment) is responsible for the
specific prompt design. Furthermore, we designed four variants of the ZSO
algorithm with slight human-interacted adjustment. In numerical experiments, we
comprehensively investigate the performance of ZSO-derived algorithms on
CEC2014 benchmark functions, CEC2022 benchmark functions, and six engineering
optimization problems. 20 popular and state-of-the-art MAs are employed as
competitors. The experimental results and statistical analysis confirm the
efficiency and effectiveness of ZSO-derived algorithms. At the end of this
paper, we explore the prospects for the development of the metaheuristics
community under the LLM era.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards White Box Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09863v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09863v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maciej Satkiewicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces semantic features as a candidate conceptual framework
for building inherently interpretable neural networks. A proof of concept model
for informative subproblem of MNIST consists of 4 such layers with the total of
5K learnable parameters. The model is well-motivated, inherently interpretable,
requires little hyperparameter tuning and achieves human-level adversarial test
accuracy - with no form of adversarial training! These results and the general
nature of the approach warrant further research on semantic features. The code
is available at https://github.com/314-Foundation/white-box-nn
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 9 figures, independent research, v2 changes: more adequate
  title; added: related research in Introduction, Ablation Study, Discussion,
  examples in Further Research, Appendix C; minor wording changes (including
  abstract)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discovering modular solutions that generalize compositionally <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.15001v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.15001v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Schug, Seijin Kobayashi, Yassir Akram, Maciej Wołczyk, Alexandra Proca, Johannes von Oswald, Razvan Pascanu, João Sacramento, Angelika Steger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many complex tasks can be decomposed into simpler, independent parts.
Discovering such underlying compositional structure has the potential to enable
compositional generalization. Despite progress, our most powerful systems
struggle to compose flexibly. It therefore seems natural to make models more
modular to help capture the compositional nature of many tasks. However, it is
unclear under which circumstances modular systems can discover hidden
compositional structure. To shed light on this question, we study a
teacher-student setting with a modular teacher where we have full control over
the composition of ground truth modules. This allows us to relate the problem
of compositional generalization to that of identification of the underlying
modules. In particular we study modularity in hypernetworks representing a
general class of multiplicative interactions. We show theoretically that
identification up to linear transformation purely from demonstrations is
possible without having to learn an exponential number of module combinations.
We further demonstrate empirically that under the theoretically identified
conditions, meta-learning from finite data can discover modular policies that
generalize compositionally in a number of complex environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2024; Code available at
  https://github.com/smonsays/modular-hyperteacher</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tight Convergence Rate Bounds for Optimization Under Power Law Spectral
  Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.00992v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.00992v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maksim Velikanov, Dmitry Yarotsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Performance of optimization on quadratic problems sensitively depends on the
low-lying part of the spectrum. For large (effectively infinite-dimensional)
problems, this part of the spectrum can often be naturally represented or
approximated by power law distributions, resulting in power law convergence
rates for iterative solutions of these problems by gradient-based algorithms.
In this paper, we propose a new spectral condition providing tighter upper
bounds for problems with power law optimization trajectories. We use this
condition to build a complete picture of upper and lower bounds for a wide
range of optimization algorithms -- Gradient Descent, Steepest Descent, Heavy
Ball, and Conjugate Gradients -- with an emphasis on the underlying schedules
of learning rate and momentum. In particular, we demonstrate how an optimally
accelerated method, its schedule, and convergence upper bound can be obtained
in a unified manner for a given shape of the spectrum. Also, we provide first
proofs of tight lower bounds for convergence rates of Steepest Descent and
Conjugate Gradients under spectral power laws with general exponents. Our
experiments show that the obtained convergence bounds and acceleration
strategies are not only relevant for exactly quadratic optimization problems,
but also fairly accurate when applied to the training of neural networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Design-Space <span class="highlight-title">Exploration</span> of SNN Models using Application-Specific
  Multi-Core Architectures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12061v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12061v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Sanaullah, Shamini Koravuna, Ulrich Rückert, Thorsten Jungeblut
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the motivation and the difficulties that currently exist in
comprehending and utilizing the promising features of SNNs, we proposed a novel
run-time multi-core architecture-based simulator called "RAVSim" (Runtime
Analysis and Visualization Simulator), a cutting-edge SNN simulator, developed
using LabVIEW and it is publicly available on their website as an official
module. RAVSim is a runtime virtual simulation environment tool that enables
the user to interact with the model, observe its behavior of output
concentration, and modify the set of parametric values at any time while the
simulation is in execution. Recently some popular tools have been presented,
but we believe that none of the tools allow users to interact with the model
simulation in run time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Abstract Presentation in 2023 Neuro-Inspired Computing Elements
  (NICE) Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EVOTER: Evolution of Transparent Explainable Rule-sets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.10438v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.10438v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hormoz Shahrzad, Babak Hodjat, Risto Miikkulainen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most AI systems are black boxes generating reasonable outputs for given
inputs. Some domains, however, have explainability and trustworthiness
requirements that cannot be directly met by these approaches. Various methods
have therefore been developed to interpret black-box models after training.
This paper advocates an alternative approach where the models are transparent
and explainable to begin with. This approach, EVOTER, evolves rule-sets based
on simple logical expressions. The approach is evaluated in several
prediction/classification and prescription/policy search domains with and
without a surrogate. It is shown to discover meaningful rule sets that perform
similarly to black-box models. The rules can provide insight into the domain,
and make biases hidden in the data explicit. It may also be possible to edit
them directly to remove biases and add constraints. EVOTER thus forms a
promising foundation for building trustworthy AI systems for real-world
applications in the future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Convolutional Spiking Neural Networks for Detecting Anticipatory Brain
  Potentials Using Electroencephalogram 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.06900v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.06900v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Lutes, Venkata Sriram Siddhardh Nadendla, K. Krishnamurthy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking neural networks (SNNs) are receiving increased attention because they
mimic synaptic connections in biological systems and produce spike trains,
which can be approximated by binary values for computational efficiency.
Recently, the addition of convolutional layers to combine the feature
extraction power of convolutional networks with the computational efficiency of
SNNs has been introduced. This paper studies the feasibility of using a
convolutional spiking neural network (CSNN) to detect anticipatory slow
cortical potentials (SCPs) related to braking intention in human participants
using an electroencephalogram (EEG). Data was collected during an experiment
wherein participants operated a remote-controlled vehicle on a testbed designed
to simulate an urban environment. Participants were alerted to an incoming
braking event via an audio countdown to elicit anticipatory potentials that
were measured using an EEG. The CSNN's performance was compared to a standard
CNN, EEGNet and three graph neural networks via 10-fold cross-validation. The
CSNN outperformed all the other neural networks, and had a predictive accuracy
of 99.06 percent with a true positive rate of 98.50 percent, a true negative
rate of 99.20 percent and an F1-score of 0.98. Performance of the CSNN was
comparable to the CNN in an ablation study using a subset of EEG channels that
localized SCPs. Classification performance of the CSNN degraded only slightly
when the floating-point EEG data were converted into spike trains via delta
modulation to mimic synaptic connections.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 6 figures, Scientific Reports submission</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DASA: Delay-Adaptive Multi-<span class="highlight-title">Agent</span> Stochastic Approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17247v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17247v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolo Dal Fabbro, Arman Adibi, H. Vincent Poor, Sanjeev R. Kulkarni, Aritra Mitra, George J. Pappas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a setting in which $N$ agents aim to speedup a common Stochastic
Approximation (SA) problem by acting in parallel and communicating with a
central server. We assume that the up-link transmissions to the server are
subject to asynchronous and potentially unbounded time-varying delays. To
mitigate the effect of delays and stragglers while reaping the benefits of
distributed computation, we propose \texttt{DASA}, a Delay-Adaptive algorithm
for multi-agent Stochastic Approximation. We provide a finite-time analysis of
\texttt{DASA} assuming that the agents' stochastic observation processes are
independent Markov chains. Significantly advancing existing results,
\texttt{DASA} is the first algorithm whose convergence rate depends only on the
mixing time $\tmix$ and on the average delay $\tau_{avg}$ while jointly
achieving an $N$-fold convergence speedup under Markovian sampling. Our work is
relevant for various SA applications, including multi-agent and distributed
temporal difference (TD) learning, Q-learning and stochastic optimization with
correlated data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Offline <span class="highlight-title">Reinforcement</span> Learning: Role of State Aggregation and Trajectory
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17091v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17091v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyu Jia, Alexander Rakhlin, Ayush Sekhari, Chen-Yu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We revisit the problem of offline reinforcement learning with value function
realizability but without Bellman completeness. Previous work by Xie and Jiang
(2021) and Foster et al. (2022) left open the question whether a bounded
concentrability coefficient along with trajectory-based offline data admits a
polynomial sample complexity. In this work, we provide a negative answer to
this question for the task of offline policy evaluation. In addition to
addressing this question, we provide a rather complete picture for offline
policy evaluation with only value function realizability. Our primary findings
are threefold: 1) The sample complexity of offline policy evaluation is
governed by the concentrability coefficient in an aggregated Markov Transition
Model jointly determined by the function class and the offline data
distribution, rather than that in the original MDP. This unifies and
generalizes the ideas of Xie and Jiang (2021) and Foster et al. (2022), 2) The
concentrability coefficient in the aggregated Markov Transition Model may grow
exponentially with the horizon length, even when the concentrability
coefficient in the original MDP is small and the offline data is admissible
(i.e., the data distribution equals the occupancy measure of some policy), 3)
Under value function realizability, there is a generic reduction that can
convert any hard instance with admissible data to a hard instance with
trajectory data, implying that trajectory data offers no extra benefits over
admissible data. These three pieces jointly resolve the open problem, though
each of them could be of independent interest.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Rectified Flow: Advancing Diffusion Language Generation with
  Probabilistic Flows <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16995v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16995v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shujian Zhang, Lemeng Wu, Chengyue Gong, Xingchao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works have demonstrated success in controlling sentence attributes
($e.g.$, sentiment) and structure ($e.g.$, syntactic structure) based on the
diffusion language model. A key component that drives theimpressive performance
for generating high-quality samples from noise is iteratively denoise for
thousands of steps. While beneficial, the complexity of starting from the noise
and the learning steps has limited its implementation to many NLP real-world
applications. This paper proposes Language Rectified Flow ({\ours}). Our method
is based on the reformulation of the standard probabilistic flow models.
Language rectified flow learns (neural) ordinary differential equation models
to transport between the source distribution and the target distribution, hence
providing a unified and effective solution to generative modeling and domain
transfer. From the source distribution, our language rectified flow yields fast
simulation and effectively decreases the inference time. Experiments on three
challenging fine-grained control tasks and multiple high-quality text editing
show that our method consistently outperforms its baselines. Extensive
experiments and ablation studies demonstrate that our method can be general,
effective, and beneficial for many NLP tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The <span class="highlight-title">Sample</span> Complexity of Simple Binary Hypothesis Testing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ankit Pensia, Varun Jog, Po-Ling Loh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The sample complexity of simple binary hypothesis testing is the smallest
number of i.i.d. samples required to distinguish between two distributions $p$
and $q$ in either: (i) the prior-free setting, with type-I error at most
$\alpha$ and type-II error at most $\beta$; or (ii) the Bayesian setting, with
Bayes error at most $\delta$ and prior distribution $(\alpha, 1-\alpha)$. This
problem has only been studied when $\alpha = \beta$ (prior-free) or $\alpha =
1/2$ (Bayesian), and the sample complexity is known to be characterized by the
Hellinger divergence between $p$ and $q$, up to multiplicative constants. In
this paper, we derive a formula that characterizes the sample complexity (up to
multiplicative constants that are independent of $p$, $q$, and all error
parameters) for: (i) all $0 \le \alpha, \beta \le 1/8$ in the prior-free
setting; and (ii) all $\delta \le \alpha/4$ in the Bayesian setting. In
particular, the formula admits equivalent expressions in terms of certain
divergences from the Jensen--Shannon and Hellinger families. The main technical
result concerns an $f$-divergence inequality between members of the
Jensen--Shannon and Hellinger families, which is proved by a combination of
information-theoretic tools and case-by-case analyses. We explore applications
of our results to robust and distributed (locally-private and
communication-constrained) hypothesis testing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Comments welcome</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SCOD: From Heuristics to Theory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vojtech Franc, Jakub Paplham, Daniel Prusa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the problem of designing reliable prediction models that
abstain from predictions when faced with uncertain or out-of-distribution
samples - a recently proposed problem known as Selective Classification in the
presence of Out-of-Distribution data (SCOD). We make three key contributions to
SCOD. Firstly, we demonstrate that the optimal SCOD strategy involves a Bayes
classifier for in-distribution (ID) data and a selector represented as a
stochastic linear classifier in a 2D space, using i) the conditional risk of
the ID classifier, and ii) the likelihood ratio of ID and out-of-distribution
(OOD) data as input. This contrasts with suboptimal strategies from current OOD
detection methods and the Softmax Information Retaining Combination (SIRC),
specifically developed for SCOD. Secondly, we establish that in a
distribution-free setting, the SCOD problem is not Probably Approximately
Correct learnable when relying solely on an ID data sample. Third, we introduce
POSCOD, a simple method for learning a plugin estimate of the optimal SCOD
strategy from both an ID data sample and an unlabeled mixture of ID and OOD
data. Our empirical results confirm the theoretical findings and demonstrate
that our proposed method, POSCOD, out performs existing OOD methods in
effectively addressing the SCOD problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Provably Robust Score-Based Diffusion Posterior Sampling for
  Plug-and-Play Image Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17042v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17042v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyu Xu, Yuejie Chi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a great number of tasks in science and engineering, the goal is to infer
an unknown image from a small number of measurements collected from a known
forward model describing certain sensing or imaging modality. Due to resource
constraints, this task is often extremely ill-posed, which necessitates the
adoption of expressive prior information to regularize the solution space.
Score-based diffusion models, due to its impressive empirical success, have
emerged as an appealing candidate of an expressive prior in image
reconstruction. In order to accommodate diverse tasks at once, it is of great
interest to develop efficient, consistent and robust algorithms that
incorporate {\em unconditional} score functions of an image prior distribution
in conjunction with flexible choices of forward models.
  This work develops an algorithmic framework for employing score-based
diffusion models as an expressive data prior in general nonlinear inverse
problems. Motivated by the plug-and-play framework in the imaging community, we
introduce a diffusion plug-and-play method (\textsf{DPnP}) that alternatively
calls two samplers, a proximal consistency sampler based solely on the
likelihood function of the forward model, and a denoising diffusion sampler
based solely on the score functions of the image prior. The key insight is that
denoising under white Gaussian noise can be solved {\em rigorously} via both
stochastic (i.e., DDPM-type) and deterministic (i.e., DDIM-type) samplers using
the unconditional score functions. We establish both asymptotic and
non-asymptotic performance guarantees of \textsf{DPnP}, and provide numerical
experiments to illustrate its promise in solving both linear and nonlinear
image reconstruction tasks. To the best of our knowledge, \textsf{DPnP} is the
first provably-robust posterior sampling method for nonlinear inverse problems
using unconditional diffusion priors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Discrete Latent Graph Generative Modeling with Diffusion Bridges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16883v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16883v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Van Khoa Nguyen, Yoann Boget, Frantzeska Lavda, Alexandros Kalousis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning graph generative models over latent spaces has received less
attention compared to models that operate on the original data space and has so
far demonstrated lacklustre performance. We present GLAD a latent space graph
generative model. Unlike most previous latent space graph generative models,
GLAD operates on a discrete latent space that preserves to a significant extent
the discrete nature of the graph structures making no unnatural assumptions
such as latent space continuity. We learn the prior of our discrete latent
space by adapting diffusion bridges to its structure. By operating over an
appropriately constructed latent space we avoid relying on decompositions that
are often used in models that operate in the original data space. We present
experiments on a series of graph benchmark datasets which clearly show the
superiority of the discrete latent space and obtain state of the art graph
generative performance, making GLAD the first latent space graph generative
model with competitive performance. Our source code is published at:
\url{https://github.com/v18nguye/GLAD}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conformal Off-<span class="highlight-title">Policy</span> Prediction for Multi-<span class="highlight-title">Agent</span> Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16871v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16871v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Kuipers, Renukanandan Tumu, Shuo Yang, Milad Kazemi, Rahul Mangharam, Nicola Paoletti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Off-Policy Prediction (OPP), i.e., predicting the outcomes of a target policy
using only data collected under a nominal (behavioural) policy, is a paramount
problem in data-driven analysis of safety-critical systems where the deployment
of a new policy may be unsafe. To achieve dependable off-policy predictions,
recent work on Conformal Off-Policy Prediction (COPP) leverage the conformal
prediction framework to derive prediction regions with probabilistic guarantees
under the target process. Existing COPP methods can account for the
distribution shifts induced by policy switching, but are limited to
single-agent systems and scalar outcomes (e.g., rewards). In this work, we
introduce MA-COPP, the first conformal prediction method to solve OPP problems
involving multi-agent systems, deriving joint prediction regions for all
agents' trajectories when one or more "ego" agents change their policies.
Unlike the single-agent scenario, this setting introduces higher complexity as
the distribution shifts affect predictions for all agents, not just the ego
agents, and the prediction task involves full multi-dimensional trajectories,
not just reward values. A key contribution of MA-COPP is to avoid enumeration
or exhaustive search of the output space of agent trajectories, which is
instead required by existing COPP methods to construct the prediction region.
We achieve this by showing that an over-approximation of the true JPR can be
constructed, without enumeration, from the maximum density ratio of the JPR
trajectories. We evaluate the effectiveness of MA-COPP in multi-agent systems
from the PettingZoo library and the F1TENTH autonomous racing environment,
achieving nominal coverage in higher dimensions and various shift settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the 63rd IEEE Conference on Decision and Control (CDC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Weak Convergence Analysis of Online Neural Actor-Critic Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16825v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16825v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Chun-Hei Lam, Justin Sirignano, Ziheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We prove that a single-layer neural network trained with the online actor
critic algorithm converges in distribution to a random ordinary differential
equation (ODE) as the number of hidden units and the number of training steps
$\rightarrow \infty$. In the online actor-critic algorithm, the distribution of
the data samples dynamically changes as the model is updated, which is a key
challenge for any convergence analysis. We establish the geometric ergodicity
of the data samples under a fixed actor policy. Then, using a Poisson equation,
we prove that the fluctuations of the model updates around the limit
distribution due to the randomly-arriving data samples vanish as the number of
parameter updates $\rightarrow \infty$. Using the Poisson equation and weak
convergence techniques, we prove that the actor neural network and critic
neural network converge to the solutions of a system of ODEs with random
initial conditions. Analysis of the limit ODE shows that the limit critic
network will converge to the true value function, which will provide the actor
an asymptotically unbiased estimate of the policy gradient. We then prove that
the limit actor network will converge to a stationary point.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal convex $M$-estimation via score matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16688v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16688v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oliver Y. Feng, Yu-Chun Kao, Min Xu, Richard J. Samworth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the context of linear regression, we construct a data-driven convex loss
function with respect to which empirical risk minimisation yields optimal
asymptotic variance in the downstream estimation of the regression
coefficients. Our semiparametric approach targets the best decreasing
approximation of the derivative of the log-density of the noise distribution.
At the population level, this fitting process is a nonparametric extension of
score matching, corresponding to a log-concave projection of the noise
distribution with respect to the Fisher divergence. The procedure is
computationally efficient, and we prove that our procedure attains the minimal
asymptotic covariance among all convex $M$-estimators. As an example of a
non-log-concave setting, for Cauchy errors, the optimal convex loss function is
Huber-like, and our procedure yields an asymptotic efficiency greater than 0.87
relative to the oracle maximum likelihood estimator of the regression
coefficients that uses knowledge of this error distribution; in this sense, we
obtain robustness without sacrificing much efficiency. Numerical experiments
confirm the practical merits of our proposal.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>69 pages, 12 figures and 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A note on generalization bounds for losses with finite moments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Borja Rodríguez-Gálvez, Omar Rivasplata, Ragnar Thobaben, Mikael Skoglund
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the truncation method from Alquier [1] to derive
high-probability PAC-Bayes bounds for unbounded losses with heavy tails.
Assuming that the $p$-th moment is bounded, the resulting bounds interpolate
between a slow rate $1 / \sqrt{n}$ when $p=2$, and a fast rate $1 / n$ when $p
\to \infty$ and the loss is essentially bounded. Moreover, the paper derives a
high-probability PAC-Bayes bound for losses with a bounded variance. This bound
has an exponentially better dependence on the confidence parameter and the
dependency measure than previous bounds in the literature. Finally, the paper
extends all results to guarantees in expectation and single-draw PAC-Bayes. In
order to so, it obtains analogues of the PAC-Bayes fast rate bound for bounded
losses from [2] in these settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages: 5 of main text, 1 of references, and 3 of appendices</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal Discovery from Poisson Branching Structural Causal Model Using
  High-Order Cumulant with Path Analysis <span class="chip">AAAI-2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16523v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16523v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Qiao, Yu Xiang, Zhengming Chen, Ruichu Cai, Zhifeng Hao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Count data naturally arise in many fields, such as finance, neuroscience, and
epidemiology, and discovering causal structure among count data is a crucial
task in various scientific and industrial scenarios. One of the most common
characteristics of count data is the inherent branching structure described by
a binomial thinning operator and an independent Poisson distribution that
captures both branching and noise. For instance, in a population count
scenario, mortality and immigration contribute to the count, where survival
follows a Bernoulli distribution, and immigration follows a Poisson
distribution. However, causal discovery from such data is challenging due to
the non-identifiability issue: a single causal pair is Markov equivalent, i.e.,
$X\rightarrow Y$ and $Y\rightarrow X$ are distributed equivalent. Fortunately,
in this work, we found that the causal order from $X$ to its child $Y$ is
identifiable if $X$ is a root vertex and has at least two directed paths to
$Y$, or the ancestor of $X$ with the most directed path to $X$ has a directed
path to $Y$ without passing $X$. Specifically, we propose a Poisson Branching
Structure Causal Model (PB-SCM) and perform a path analysis on PB-SCM using
high-order cumulants. Theoretical results establish the connection between the
path and cumulant and demonstrate that the path information can be obtained
from the cumulant. With the path information, causal order is identifiable
under some graphical conditions. A practical algorithm for learning causal
structure under PB-SCM is proposed and the experiments demonstrate and verify
the effectiveness of the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI-2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the rates of convergence for learning with convolutional neural
  networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16459v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16459v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunfei Yang, Han Feng, Ding-Xuan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the approximation and learning capacities of convolutional neural
networks (CNNs). Our first result proves a new approximation bound for CNNs
with certain constraint on the weights. Our second result gives a new analysis
on the covering number of feed-forward neural networks, which include CNNs as
special cases. The analysis carefully takes into account the size of the
weights and hence gives better bounds than existing literature in some
situations. Using these two results, we are able to derive rates of convergence
for estimators based on CNNs in many learning problems. In particular, we
establish minimax optimal convergence rates of the least squares based on CNNs
for learning smooth functions in the nonparametric regression setting. For
binary classification, we derive convergence rates for CNN classifiers with
hinge loss and logistic loss. It is also shown that the obtained rates are
minimax optimal in several settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-time Adaptation for Condition Monitoring Signal Prediction using
  Label-aware Neural Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16377v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16377v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seokhyun Chung, Raed Al Kontar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building a predictive model that rapidly adapts to real-time condition
monitoring (CM) signals is critical for engineering systems/units.
Unfortunately, many current methods suffer from a trade-off between
representation power and agility in online settings. For instance, parametric
methods that assume an underlying functional form for CM signals facilitate
efficient online prediction updates. However, this simplification leads to
vulnerability to model specifications and an inability to capture complex
signals. On the other hand, approaches based on over-parameterized or
non-parametric models can excel at explaining complex nonlinear signals, but
real-time updates for such models pose a challenging task. In this paper, we
propose a neural process-based approach that addresses this trade-off. It
encodes available observations within a CM signal into a representation space
and then reconstructs the signal's history and evolution for prediction. Once
trained, the model can encode an arbitrary number of observations without
requiring retraining, enabling on-the-spot real-time predictions along with
quantified uncertainty and can be readily updated as more online data is
gathered. Furthermore, our model is designed to incorporate qualitative
information (i.e., labels) from individual units. This integration not only
enhances individualized predictions for each unit but also enables joint
inference for both signals and their associated labels. Numerical studies on
both synthetic and real-world data in reliability engineering highlight the
advantageous features of our model in real-time adaptation, enhanced signal
prediction with uncertainty quantification, and joint prediction for labels and
signals.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Action-based Representations Using Invariance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16369v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16369v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Rudolph, Caleb Chuck, Kevin Black, Misha Lvovsky, Scott Niekum, Amy Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust reinforcement learning agents using high-dimensional observations must
be able to identify relevant state features amidst many exogeneous distractors.
A representation that captures controllability identifies these state elements
by determining what affects agent control. While methods such as inverse
dynamics and mutual information capture controllability for a limited number of
timesteps, capturing long-horizon elements remains a challenging problem.
Myopic controllability can capture the moment right before an agent crashes
into a wall, but not the control-relevance of the wall while the agent is still
some distance away. To address this we introduce action-bisimulation encoding,
a method inspired by the bisimulation invariance pseudometric, that extends
single-step controllability with a recursive invariance constraint. By doing
this, action-bisimulation learns a multi-step controllability metric that
smoothly discounts distant state features that are relevant for control. We
demonstrate that action-bisimulation pretraining on reward-free, uniformly
random data improves sample efficiency in several environments, including a
photorealistic 3D simulation domain, Habitat. Additionally, we provide
theoretical analysis and qualitative results demonstrating the information
captured by action-bisimulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predictive Inference in Multi-environment Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16336v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16336v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John C. Duchi, Suyash Gupta, Kuanhao Jiang, Pragya Sur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the challenge of constructing valid confidence intervals and sets
in problems of prediction across multiple environments. We investigate two
types of coverage suitable for these problems, extending the jackknife and
split-conformal methods to show how to obtain distribution-free coverage in
such non-traditional, hierarchical data-generating scenarios. Our contributions
also include extensions for settings with non-real-valued responses and a
theory of consistency for predictive inference in these general problems. We
demonstrate a novel resizing method to adapt to problem difficulty, which
applies both to existing approaches for predictive inference with hierarchical
data and the methods we develop; this reduces prediction set sizes using
limited information from the test environment, a key to the methods' practical
performance, which we evaluate through neurochemical sensing and species
classification datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Applying statistical learning theory to deep learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.15404v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.15404v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cédric Gerbelot, Avetik Karagulyan, Stefani Karp, Kavya Ravichandran, Menachem Stern, Nathan Srebro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although statistical learning theory provides a robust framework to
understand supervised learning, many theoretical aspects of deep learning
remain unclear, in particular how different architectures may lead to inductive
bias when trained using gradient based methods. The goal of these lectures is
to provide an overview of some of the main questions that arise when attempting
to understand deep learning from a learning theory perspective. After a brief
reminder on statistical learning theory and stochastic optimization, we discuss
implicit bias in the context of benign overfitting. We then move to a general
description of the mirror descent algorithm, showing how we may go back and
forth between a parameter space and the corresponding function space for a
given learning problem, as well as how the geometry of the learning problem may
be represented by a metric tensor. Building on this framework, we provide a
detailed study of the implicit bias of gradient descent on linear diagonal
networks for various regression tasks, showing how the loss function, scale of
parameters at initialization and depth of the network may lead to various forms
of implicit bias, in particular transitioning between kernel or feature
learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>66 pages, 20 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A distribution-free mixed-integer optimization approach to hierarchical
  modelling of clustered and longitudinal data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.03157v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.03157v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Madhav Sankaranarayanan, Intekhab Hossain, Tom Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Mixed Integer Optimization (MIO) algorithms, paired
with hardware enhancements, have led to significant speedups in resolving MIO
problems. These strategies have been utilized for optimal subset selection,
specifically for choosing $k$ features out of $p$ in linear regression given
$n$ observations. In this paper, we broaden this method to facilitate
cluster-aware regression, where selection aims to choose $\lambda$ out of $K$
clusters in a linear mixed effects (LMM) model with $n_k$ observations for each
cluster. Through comprehensive testing on a multitude of synthetic and real
datasets, we exhibit that our method efficiently solves problems within
minutes. Through numerical experiments, we also show that the MIO approach
outperforms both Gaussian- and Laplace-distributed LMMs in terms of generating
sparse solutions with high predictive power. Traditional LMMs typically assume
that clustering effects are independent of individual features. However, we
introduce an innovative algorithm that evaluates cluster effects for new data
points, thereby increasing the robustness and precision of this model. The
inferential and predictive efficacy of this approach is further illustrated
through its application in student scoring and protein expression.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Entropy-MCMC: Sampling from Flat Basins with Ease 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05401v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05401v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bolian Li, Ruqi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian deep learning counts on the quality of posterior distribution
estimation. However, the posterior of deep neural networks is highly
multi-modal in nature, with local modes exhibiting varying generalization
performance. Given a practical budget, targeting at the original posterior can
lead to suboptimal performance, as some samples may become trapped in "bad"
modes and suffer from overfitting. Leveraging the observation that "good" modes
with low generalization error often reside in flat basins of the energy
landscape, we propose to bias sampling on the posterior toward these flat
regions. Specifically, we introduce an auxiliary guiding variable, the
stationary distribution of which resembles a smoothed posterior free from sharp
modes, to lead the MCMC sampler to flat basins. By integrating this guiding
variable with the model parameter, we create a simple joint distribution that
enables efficient sampling with minimal computational overhead. We prove the
convergence of our method and further show that it converges faster than
several existing flatness-aware methods in the strongly convex setting.
Empirical results demonstrate that our method can successfully sample from flat
basins of the posterior, and outperforms all compared baselines on multiple
benchmarks including classification, calibration, and out-of-distribution
detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distributional Robustness Bounds Generalization Errors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.09962v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.09962v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shixiong Wang, Haowei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian methods, distributionally robust optimization methods, and
regularization methods are three pillars of trustworthy machine learning
combating distributional uncertainty, e.g., the uncertainty of an empirical
distribution compared to the true underlying distribution. This paper
investigates the connections among the three frameworks and, in particular,
explores why these frameworks tend to have smaller generalization errors.
Specifically, first, we suggest a quantitative definition for "distributional
robustness", propose the concept of "robustness measure", and formalize several
philosophical concepts in distributionally robust optimization. Second, we show
that Bayesian methods are distributionally robust in the probably approximately
correct (PAC) sense; in addition, by constructing a Dirichlet-process-like
prior in Bayesian nonparametrics, it can be proven that any regularized
empirical risk minimization method is equivalent to a Bayesian method. Third,
we show that generalization errors of machine learning models can be
characterized using the distributional uncertainty of the nominal distribution
and the robustness measures of these machine learning models, which is a new
perspective to bound generalization errors, and therefore, explain the reason
why distributionally robust machine learning models, Bayesian models, and
regularization models tend to have smaller generalization errors in a unified
manner.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated Version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Poincaré Inequality and Consistency Results for Signal Sampling on
  Large Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10610v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10610v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thien Le, Luana Ruiz, Stefanie Jegelka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale graph machine learning is challenging as the complexity of
learning models scales with the graph size. Subsampling the graph is a viable
alternative, but sampling on graphs is nontrivial as graphs are non-Euclidean.
Existing graph sampling techniques require not only computing the spectra of
large matrices but also repeating these computations when the graph changes,
e.g., grows. In this paper, we introduce a signal sampling theory for a type of
graph limit -- the graphon. We prove a Poincar\'e inequality for graphon
signals and show that complements of node subsets satisfying this inequality
are unique sampling sets for Paley-Wiener spaces of graphon signals. Exploiting
connections with spectral clustering and Gaussian elimination, we prove that
such sampling sets are consistent in the sense that unique sampling sets on a
convergent graph sequence converge to unique sampling sets on the graphon. We
then propose a related graphon signal sampling algorithm for large graphs, and
demonstrate its good empirical performance on graph machine learning tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TACTiS-2: Better, Faster, Simpler Attentional Copulas for Multivariate
  Time Series <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01327v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01327v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arjun Ashok, Étienne Marcotte, Valentina Zantedeschi, Nicolas Chapados, Alexandre Drouin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new model for multivariate probabilistic time series
prediction, designed to flexibly address a range of tasks including
forecasting, interpolation, and their combinations. Building on copula theory,
we propose a simplified objective for the recently-introduced transformer-based
attentional copulas (TACTiS), wherein the number of distributional parameters
now scales linearly with the number of variables instead of factorially. The
new objective requires the introduction of a training curriculum, which goes
hand-in-hand with necessary changes to the original architecture. We show that
the resulting model has significantly better training dynamics and achieves
state-of-the-art performance across diverse real-world forecasting tasks, while
maintaining the flexibility of prior work, such as seamless handling of
unaligned and unevenly-sampled time series. Code is made available at
https://github.com/ServiceNow/TACTiS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 15 figures, The Twelfth International Conference on
  Learning Representations (ICLR 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The autoregressive neural network architecture of the Boltzmann
  distribution of pairwise interacting spins systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.08347v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.08347v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Indaco Biazzo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Autoregressive Neural Networks (ARNNs) have recently demonstrated
exceptional results in image and language generation tasks, contributing to the
growing popularity of generative models in both scientific and commercial
applications. This work presents an exact mapping of the Boltzmann distribution
of binary pairwise interacting systems into autoregressive form. The resulting
ARNN architecture has weights and biases of its first layer corresponding to
the Hamiltonian's couplings and external fields, featuring widely used
structures such as the residual connections and a recurrent architecture with
clear physical meanings. Moreover, its architecture's explicit formulation
enables the use of statistical physics techniques to derive new ARNNs for
specific systems. As examples, new effective ARNN architectures are derived
from two well-known mean-field systems, the Curie-Weiss and
Sherrington-Kirkpatrick models, showing superior performance in approximating
the Boltzmann distributions of the corresponding physics model compared to
other commonly used architectures. The connection established between the
physics of the system and the neural network architecture provides a means to
derive new architectures for different interacting systems and interpret
existing ones from a physical perspective.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 10 figure plus the Supplementary Information</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Sample</span>t basis pursuit: Multiresolution scattered data approximation with
  sparsity constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.10180v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.10180v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davide Baroli, Helmut Harbrecht, Michael Multerer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider scattered data approximation in samplet coordinates with
$\ell_1$-regularization. The application of an $\ell_1$-regularization term
enforces sparsity of the coefficients with respect to the samplet basis.
Samplets are wavelet-type signed measures, which are tailored to scattered
data. They provide similar properties as wavelets in terms of localization,
multiresolution analysis, and data compression. By using the Riesz isometry, we
embed samplets into reproducing kernel Hilbert spaces and discuss the
properties of the resulting functions. We argue that the class of signals that
are sparse with respect to the embedded samplet basis is considerably larger
than the class of signals that are sparse with respect to the basis of kernel
translates. Vice versa, every signal that is a linear combination of only a few
kernel translates is sparse in samplet coordinates. Therefore, samplets enable
the use of well-established multiresolution techniques on general scattered
data sets.
  We propose the rapid solution of the problem under consideration by combining
soft-shrinkage with the semi-smooth Newton method. Leveraging on the sparse
representation of kernel matrices in samplet coordinates, this approach
converges faster than the fast iterative shrinkage thresholding algorithm and
is feasible for large-scale data. Numerical benchmarks are presented and
demonstrate the superiority of the multiresolution approach over the
single-scale approach. As large-scale applications, the surface reconstruction
from scattered data and the reconstruction of scattered temperature data using
a dictionary of multiple kernels are considered.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Variational Bayes image restoration with compressive autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17744v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17744v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maud Biquard, Marie Chabert, Thomas Oberlin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Regularization of inverse problems is of paramount importance in
computational imaging. The ability of neural networks to learn efficient image
representations has been recently exploited to design powerful data-driven
regularizers. While state-of-the-art plug-and-play methods rely on an implicit
regularization provided by neural denoisers, alternative Bayesian approaches
consider Maximum A Posteriori (MAP) estimation in the latent space of a
generative model, thus with an explicit regularization. However,
state-of-the-art deep generative models require a huge amount of training data
compared to denoisers. Besides, their complexity hampers the optimization
involved in latent MAP derivation. In this work, we first propose to use
compressive autoencoders instead. These networks, which can be seen as
variational autoencoders with a flexible latent prior, are smaller and easier
to train than state-of-the-art generative models. As a second contribution, we
introduce the Variational Bayes Latent Estimation (VBLE) algorithm, which
performs latent estimation within the framework of variational inference.
Thanks to a simple yet efficient parameterization of the variational posterior,
VBLE allows for fast and easy (approximate) posterior sampling. Experimental
results on image datasets BSD and FFHQ demonstrate that VBLE reaches similar
performance than state-of-the-art plug-and-play methods, while being able to
quantify uncertainties faster than other existing posterior sampling
techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CrossQ: Batch Normalization in Deep <span class="highlight-title">Reinforcement</span> Learning for Greater
  <span class="highlight-title">Sample</span> Efficiency and Simplicity <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1902.05605v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1902.05605v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Bhatt, Daniel Palenicek, Boris Belousov, Max Argus, Artemij Amiranashvili, Thomas Brox, Jan Peters
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sample efficiency is a crucial problem in deep reinforcement learning. Recent
algorithms, such as REDQ and DroQ, found a way to improve the sample efficiency
by increasing the update-to-data (UTD) ratio to 20 gradient update steps on the
critic per environment sample. However, this comes at the expense of a greatly
increased computational cost. To reduce this computational burden, we introduce
CrossQ: A lightweight algorithm for continuous control tasks that makes careful
use of Batch Normalization and removes target networks to surpass the current
state-of-the-art in sample efficiency while maintaining a low UTD ratio of 1.
Notably, CrossQ does not rely on advanced bias-reduction schemes used in
current methods. CrossQ's contributions are threefold: (1) it matches or
surpasses current state-of-the-art methods in terms of sample efficiency, (2)
it substantially reduces the computational cost compared to REDQ and DroQ, (3)
it is easy to implement, requiring just a few lines of code on top of SAC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICLR 2024. Project page at
  http://aditya.bhatts.org/CrossQ and code release at
  https://github.com/adityab/CrossQ</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spectral clustering in the Gaussian mixture block model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.00979v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.00979v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuangping Li, Tselil Schramm
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gaussian mixture block models are distributions over graphs that strive to
model modern networks: to generate a graph from such a model, we associate each
vertex $i$ with a latent feature vector $u_i \in \mathbb{R}^d$ sampled from a
mixture of Gaussians, and we add edge $(i,j)$ if and only if the feature
vectors are sufficiently similar, in that $\langle u_i,u_j \rangle \ge \tau$
for a pre-specified threshold $\tau$. The different components of the Gaussian
mixture represent the fact that there may be different types of nodes with
different distributions over features -- for example, in a social network each
component represents the different attributes of a distinct community. Natural
algorithmic tasks associated with these networks are embedding (recovering the
latent feature vectors) and clustering (grouping nodes by their mixture
component).
  In this paper we initiate the study of clustering and embedding graphs
sampled from high-dimensional Gaussian mixture block models, where the
dimension of the latent feature vectors $d\to \infty$ as the size of the
network $n \to \infty$. This high-dimensional setting is most appropriate in
the context of modern networks, in which we think of the latent feature space
as being high-dimensional. We analyze the performance of canonical spectral
clustering and embedding algorithms for such graphs in the case of 2-component
spherical Gaussian mixtures, and begin to sketch out the
information-computation landscape for clustering and embedding in these models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>48 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> PhyloGFN: Phylogenetic inference with generative flow networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08774v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08774v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyang Zhou, Zichao Yan, Elliot Layne, Nikolay Malkin, Dinghuai Zhang, Moksh Jain, Mathieu Blanchette, <span class="highlight-author">Yoshua Bengio</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Phylogenetics is a branch of computational biology that studies the
evolutionary relationships among biological entities. Its long history and
numerous applications notwithstanding, inference of phylogenetic trees from
sequence data remains challenging: the high complexity of tree space poses a
significant obstacle for the current combinatorial and probabilistic
techniques. In this paper, we adopt the framework of generative flow networks
(GFlowNets) to tackle two core problems in phylogenetics: parsimony-based and
Bayesian phylogenetic inference. Because GFlowNets are well-suited for sampling
complex combinatorial structures, they are a natural choice for exploring and
sampling from the multimodal posterior distribution over tree topologies and
evolutionary distances. We demonstrate that our amortized posterior sampler,
PhyloGFN, produces diverse and high-quality evolutionary hypotheses on real
benchmark datasets. PhyloGFN is competitive with prior works in marginal
likelihood estimation and achieves a closer fit to the target distribution than
state-of-the-art variational inference methods. Our code is available at
https://github.com/zmy1116/phylogfn.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-24T00:00:00Z">2024-03-24</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computer Science and Game Theory
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Coupled Optimization Framework for Correlated Equilibria in
  Normal-Form <span class="highlight-title">Game</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sarah H. Q. Li, Yue Yu, Florian Dörfler, John Lygeros
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In competitive multi-player interactions, simultaneous optimality is a key
requirement for establishing strategic equilibria. This property is explicit
when the game-theoretic equilibrium is the simultaneously optimal solution of
coupled optimization problems. However, no such optimization problems exist for
the correlated equilibrium, a strategic equilibrium where the players can
correlate their actions. We address the lack of a coupled optimization
framework for the correlated equilibrium by introducing an {unnormalized game}
-- an extension of normal-form games in which the player strategies are lifted
to unnormalized measures over the joint actions. We show that the set of fully
mixed generalized Nash equilibria of this unnormalized game is a subset of the
correlated equilibrium of the normal-form game. Furthermore, we introduce an
entropy regularization to the unnormalized game and prove that the
entropy-regularized generalized Nash equilibrium is a sub-optimal correlated
equilibrium of the normal form game where the degree of sub-optimality depends
on the magnitude of regularization. We prove that the entropy-regularized
unnormalized game has a closed-form solution, and empirically verify its
computational efficacy at approximating the correlated equilibrium of
normal-form games.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Artificial Neural Microcircuits as Building Blocks: Concept and
  Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16327v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16327v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Walter, Shimeng Wu, Andy M. Tyrrell, Liam McDaid, Malachy McElholm, Nidhin Thandassery Sumithran, Jim Harkin, Martin A. Trefzer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial Neural Networks (ANNs) are one of the most widely employed forms
of bio-inspired computation. However the current trend is for ANNs to be
structurally homogeneous. Furthermore, this structural homogeneity requires the
application of complex training and learning tools that produce application
specific ANNs, susceptible to pitfalls such as overfitting. In this paper, an
new approach is explored, inspired by the role played in biology by Neural
Microcircuits, the so called ``fundamental processing elements'' of organic
nervous systems. How large neural networks, particularly Spiking Neural
Networks (SNNs) can be assembled using Artificial Neural Microcircuits (ANMs),
intended as off-the-shelf components, is articulated; the results of initial
work to produce a catalogue of such Microcircuits though the use of Novelty
Search is shown; followed by efforts to expand upon this initial work,
including a discussion of challenges uncovered during these efforts and
explorations of methods by which they might be overcome.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 31 figures, 3 tables, submitted to A-Life Journal for
  review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Sequence-to-Sequence Models for Abstractive Text Summarization
  Using Meta Heuristic Approaches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16247v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16247v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Saxena, Ashutosh Ranjan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As human society transitions into the information age, reduction in our
attention span is a contingency, and people who spend time reading lengthy news
articles are decreasing rapidly and the need for succinct information is higher
than ever before. Therefore, it is essential to provide a quick overview of
important news by concisely summarizing the top news article and the most
intuitive headline. When humans try to make summaries, they extract the
essential information from the source and add useful phrases and grammatical
annotations from the original extract. Humans have a unique ability to create
abstractions. However, automatic summarization is a complicated problem to
solve. The use of sequence-to-sequence (seq2seq) models for neural abstractive
text summarization has been ascending as far as prevalence. Numerous innovative
strategies have been proposed to develop the current seq2seq models further,
permitting them to handle different issues like saliency, familiarity, and
human lucidness and create excellent synopses. In this article, we aimed toward
enhancing the present architectures and models for abstractive text
summarization. The modifications have been aimed at fine-tuning
hyper-parameters, attempting specific encoder-decoder combinations. We examined
many experiments on an extensively used CNN/DailyMail dataset to check the
effectiveness of various models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CBGT-Net: A Neuromimetic Architecture for Robust Classification of
  Streaming Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shreya Sharma, Dana Hughes, Katia Sycara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes CBGT-Net, a neural network model inspired by the
cortico-basal ganglia-thalamic (CBGT) circuits found in mammalian brains.
Unlike traditional neural network models, which either generate an output for
each provided input, or an output after a fixed sequence of inputs, the
CBGT-Net learns to produce an output after a sufficient criteria for evidence
is achieved from a stream of observed data. For each observation, the CBGT-Net
generates a vector that explicitly represents the amount of evidence the
observation provides for each potential decision, accumulates the evidence over
time, and generates a decision when the accumulated evidence exceeds a
pre-defined threshold. We evaluate the proposed model on two image
classification tasks, where models need to predict image categories based on a
stream of small patches extracted from the image. We show that the CBGT-Net
provides improved accuracy and robustness compared to models trained to
classify from a single patch, and models leveraging an LSTM layer to classify
from a fixed sequence length of patches.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Out-of-Distribution Detection via Deep Multi-Comprehension Ensemble 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16260v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16260v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenhui Xu, Fuxun Yu, Zirui Xu, Nathan Inkawhich, Xiang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research underscores the pivotal role of the Out-of-Distribution (OOD)
feature representation field scale in determining the efficacy of models in OOD
detection. Consequently, the adoption of model ensembles has emerged as a
prominent strategy to augment this feature representation field, capitalizing
on anticipated model diversity.
  However, our introduction of novel qualitative and quantitative model
ensemble evaluation methods, specifically Loss Basin/Barrier Visualization and
the Self-Coupling Index, reveals a critical drawback in existing ensemble
methods. We find that these methods incorporate weights that are
affine-transformable, exhibiting limited variability and thus failing to
achieve the desired diversity in feature representation.
  To address this limitation, we elevate the dimensions of traditional model
ensembles, incorporating various factors such as different weight
initializations, data holdout, etc., into distinct supervision tasks. This
innovative approach, termed Multi-Comprehension (MC) Ensemble, leverages
diverse training tasks to generate distinct comprehensions of the data and
labels, thereby extending the feature representation field.
  Our experimental results demonstrate the superior performance of the MC
Ensemble strategy in OOD detection compared to both the naive Deep Ensemble
method and a standalone model of comparable size. This underscores the
effectiveness of our proposed approach in enhancing the model's capability to
detect instances outside its training distribution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Analytic Solution to Covariance Propagation in Neural Networks <span class="chip">AISTATS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16163v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16163v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oren Wright, Yorie Nakahira, José M. F. Moura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uncertainty quantification of neural networks is critical to measuring the
reliability and robustness of deep learning systems. However, this often
involves costly or inaccurate sampling methods and approximations. This paper
presents a sample-free moment propagation technique that propagates mean
vectors and covariance matrices across a network to accurately characterize the
input-output distributions of neural networks. A key enabler of our technique
is an analytic solution for the covariance of random variables passed through
nonlinear activation functions, such as Heaviside, ReLU, and GELU. The wide
applicability and merits of the proposed technique are shown in experiments
analyzing the input-output distributions of trained neural networks and
training Bayesian neural networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AISTATS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Manifold Regularization Classification Model Based On Improved Diffusion
  Map 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16059v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16059v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongfu Guo, Wencheng Zou, Zeyu Zhang, Shuishan Zhang, Ruitong Wang, Jintao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Manifold regularization model is a semi-supervised learning model that
leverages the geometric structure of a dataset, comprising a small number of
labeled samples and a large number of unlabeled samples, to generate
classifiers. However, the original manifold norm limits the performance of
models to local regions. To address this limitation, this paper proposes an
approach to improve manifold regularization based on a label propagation model.
We initially enhance the probability transition matrix of the diffusion map
algorithm, which can be used to estimate the Neumann heat kernel, enabling it
to accurately depict the label propagation process on the manifold. Using this
matrix, we establish a label propagation function on the dataset to describe
the distribution of labels at different time steps. Subsequently, we extend the
label propagation function to the entire data manifold. We prove that the
extended label propagation function converges to a stable distribution after a
sufficiently long time and can be considered as a classifier. Building upon
this concept, we propose a viable improvement to the manifold regularization
model and validate its superiority through experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 24figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Directed Acyclic Graphs from Partial Orderings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16031v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16031v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Shojaie, Wenyu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Directed acyclic graphs (DAGs) are commonly used to model causal
relationships among random variables. In general, learning the DAG structure is
both computationally and statistically challenging. Moreover, without
additional information, the direction of edges may not be estimable from
observational data. In contrast, given a complete causal ordering of the
variables, the problem can be solved efficiently, even in high dimensions. In
this paper, we consider the intermediate problem of learning DAGs when a
partial causal ordering of variables is available. We propose a general
estimation framework for leveraging the partial ordering and present efficient
estimation algorithms for low- and high-dimensional problems. The advantages of
the proposed framework are illustrated via numerical studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Near-Optimal differentially private low-rank trace regression with
  guaranteed private initialization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15999v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15999v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengyue Zha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study differentially private (DP) estimation of a rank-$r$ matrix $M \in
\mathbb{R}^{d_1\times d_2}$ under the trace regression model with Gaussian
measurement matrices. Theoretically, the sensitivity of non-private spectral
initialization is precisely characterized, and the
differential-privacy-constrained minimax lower bound for estimating $M$ under
the Schatten-$q$ norm is established. Methodologically, the paper introduces a
computationally efficient algorithm for DP-initialization with a sample size of
$n \geq \widetilde O (r^2 (d_1\vee d_2))$. Under certain regularity conditions,
the DP-initialization falls within a local ball surrounding $M$. We also
propose a differentially private algorithm for estimating $M$ based on
Riemannian optimization (DP-RGrad), which achieves a near-optimal convergence
rate with the DP-initialization and sample size of $n \geq \widetilde O(r (d_1
+ d_2))$. Finally, the paper discusses the non-trivial gap between the minimax
lower bound and the upper bound of low-rank matrix estimation under the trace
regression model. It is shown that the estimator given by DP-RGrad attains the
optimal convergence rate in a weaker notion of differential privacy. Our
powerful technique for analyzing the sensitivity of initialization requires no
eigengap condition between $r$ non-zero singular values.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stochastic Hessian Fittings on Lie Groups 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11858v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11858v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi-Lin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the fitting of Hessian or its inverse for stochastic
optimizations using a Hessian fitting criterion from the preconditioned
stochastic gradient descent (PSGD) method, which is intimately related to many
commonly used second order and adaptive gradient optimizers, e.g., BFGS,
Gaussian-Newton and natural gradient descent, AdaGrad, etc. Our analyses reveal
the efficiency and reliability differences among a wide range of preconditioner
fitting methods, from closed-form to iterative solutions, using Hessian-vector
products or stochastic gradients only, with Hessian fittings in the Euclidean
space, the manifold of symmetric positive definite (SPL) matrices, or a variety
of Lie groups. The most intriguing discovery is that the Hessian fitting itself
as an optimization problem is strongly convex under mild conditions on a
specific yet general enough Lie group. This discovery turns Hessian fitting
into a well behaved optimization problem, and facilitates the designs of highly
efficient and elegant Lie group sparse preconditioner fitting methods for large
scale stochastic optimizations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uncertainty Quantification of MLE for Entity Ranking with Covariates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.09961v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.09961v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianqing Fan, Jikai Hou, Mengxin Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper concerns with statistical estimation and inference for the ranking
problems based on pairwise comparisons with additional covariate information
such as the attributes of the compared items. Despite extensive studies, few
prior literatures investigate this problem under the more realistic setting
where covariate information exists. To tackle this issue, we propose a novel
model, Covariate-Assisted Ranking Estimation (CARE) model, that extends the
well-known Bradley-Terry-Luce (BTL) model, by incorporating the covariate
information. Specifically, instead of assuming every compared item has a fixed
latent score $\{\theta_i^*\}_{i=1}^n$, we assume the underlying scores are
given by $\{\alpha_i^*+{x}_i^\top\beta^*\}_{i=1}^n$, where $\alpha_i^*$ and
${x}_i^\top\beta^*$ represent latent baseline and covariate score of the $i$-th
item, respectively. We impose natural identifiability conditions and derive the
$\ell_{\infty}$- and $\ell_2$-optimal rates for the maximum likelihood
estimator of $\{\alpha_i^*\}_{i=1}^{n}$ and $\beta^*$ under a sparse comparison
graph, using a novel `leave-one-out' technique (Chen et al., 2019) . To conduct
statistical inferences, we further derive asymptotic distributions for the MLE
of $\{\alpha_i^*\}_{i=1}^n$ and $\beta^*$ with minimal sample complexity. This
allows us to answer the question whether some covariates have any explanation
power for latent scores and to threshold some sparse parameters to improve the
ranking performance. We improve the approximation method used in (Gao et al.,
2021) for the BLT model and generalize it to the CARE model. Moreover, we
validate our theoretical results through large-scale numerical studies and an
application to the mutual fund stock holding dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>81 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sparse joint shift in multinomial classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.16971v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.16971v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dirk Tasche
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse joint shift (SJS) was recently proposed as a tractable model for
general dataset shift which may cause changes to the marginal distributions of
features and labels as well as the posterior probabilities and the
class-conditional feature distributions. Fitting SJS for a target dataset
without label observations may produce valid predictions of labels and
estimates of class prior probabilities. We present new results on the
transmission of SJS from sets of features to larger sets of features, a
conditional correction formula for the class posterior probabilities under the
target distribution, identifiability of SJS, and the relationship between SJS
and covariate shift. In addition, we point out inconsistencies in the
algorithms which were proposed for estimating the characteristics of SJS, as
they could hamper the search for optimal solutions, and suggest potential
improvements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Featurizing Koopman Mode Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.09146v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.09146v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Aristoff, Jeremy Copperman, Nathan Mankovich, Alexander Davies
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article introduces an advanced Koopman mode decomposition (KMD)
technique -- coined Featurized Koopman Mode Decomposition (FKMD) -- that uses
time embedding and Mahalanobis scaling to enhance analysis and prediction of
high dimensional dynamical systems. The time embedding expands the observation
space to better capture underlying manifold structure, while the Mahalanobis
scaling, applied to kernel or random Fourier features, adjusts observations
based on the system's dynamics. This aids in featurizing KMD in cases where
good features are not a priori known. We find that the Mahalanobis scaling from
FKMD can be used for effective dimensionality reduction of alanine dipeptide
data. We also show that FKMD improves predictions for a high-dimensional Lorenz
attractor and a cell signaling problem from cancer research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conformal link prediction for false discovery rate control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.14693v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.14693v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ariane Marandon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most link prediction methods return estimates of the connection probability
of missing edges in a graph. Such output can be used to rank the missing edges
from most to least likely to be a true edge, but does not directly provide a
classification into true and non-existent. In this work, we consider the
problem of identifying a set of true edges with a control of the false
discovery rate (FDR). We propose a novel method based on high-level ideas from
the literature on conformal inference. The graph structure induces intricate
dependence in the data, which we carefully take into account, as this makes the
setup different from the usual setup in conformal inference, where data
exchangeability is assumed. The FDR control is empirically demonstrated for
both simulated and real data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Inference for Regression with Variables Generated from Unstructured Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15585v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15585v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laura Battaglia, Timothy Christensen, Stephen Hansen, Szymon Sacher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The leading strategy for analyzing unstructured data uses two steps. First,
latent variables of economic interest are estimated with an upstream
information retrieval model. Second, the estimates are treated as "data" in a
downstream econometric model. We establish theoretical arguments for why this
two-step strategy leads to biased inference in empirically plausible settings.
More constructively, we propose a one-step strategy for valid inference that
uses the upstream and downstream models jointly. The one-step strategy (i)
substantially reduces bias in simulations; (ii) has quantitatively important
effects in a leading application using CEO time-use data; and (iii) can be
readily adapted by applied researchers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A tutorial on learning from preferences and choices with Gaussian
  Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11782v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11782v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessio Benavoli, Dario Azzimonti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Preference modelling lies at the intersection of economics, decision theory,
machine learning and statistics. By understanding individuals' preferences and
how they make choices, we can build products that closely match their
expectations, paving the way for more efficient and personalised applications
across a wide range of domains. The objective of this tutorial is to present a
cohesive and comprehensive framework for preference learning with Gaussian
Processes (GPs), demonstrating how to seamlessly incorporate rationality
principles (from economics and decision theory) into the learning process. By
suitably tailoring the likelihood function, this framework enables the
construction of preference learning models that encompass random utility
models, limits of discernment, and scenarios with multiple conflicting
utilities for both object- and label-preference. This tutorial builds upon
established research while simultaneously introducing some novel GP-based
models to address specific gaps in the existing literature.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exponential Concentration in Stochastic Approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.07243v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.07243v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kody Law, Neil Walton, Shangda Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We analyze the behavior of stochastic approximation algorithms where
iterates, in expectation, progress towards an objective at each step. When
progress is proportional to the step size of the algorithm, we prove
exponential concentration bounds. These tail-bounds contrast asymptotic
normality results, which are more frequently associated with stochastic
approximation. The methods that we develop rely on a geometric ergodicity
proof. This extends a result on Markov chains due to Hajek (1982) to the area
of stochastic approximation algorithms. We apply our results to several
different Stochastic Approximation algorithms, specifically Projected
Stochastic Gradient Descent, Kiefer-Wolfowitz and Stochastic Frank-Wolfe
algorithms. When applicable, our results prove faster $O(1/t)$ and linear
convergence rates for Projected Stochastic Gradient Descent with a
non-vanishing gradient.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 11 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Energy-Based Models by Cooperative Diffusion Recovery
  Likelihood 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.05153v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.05153v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaxuan Zhu, Jianwen Xie, Yingnian Wu, Ruiqi Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training energy-based models (EBMs) on high-dimensional data can be both
challenging and time-consuming, and there exists a noticeable gap in sample
quality between EBMs and other generative frameworks like GANs and diffusion
models. To close this gap, inspired by the recent efforts of learning EBMs by
maximizing diffusion recovery likelihood (DRL), we propose cooperative
diffusion recovery likelihood (CDRL), an effective approach to tractably learn
and sample from a series of EBMs defined on increasingly noisy versions of a
dataset, paired with an initializer model for each EBM. At each noise level,
the two models are jointly estimated within a cooperative training framework:
samples from the initializer serve as starting points that are refined by a few
MCMC sampling steps from the EBM. The EBM is then optimized by maximizing
recovery likelihood, while the initializer model is optimized by learning from
the difference between the refined samples and the initial samples. In
addition, we made several practical designs for EBM training to further improve
the sample quality. Combining these advances, our approach significantly boost
the generation performance compared to existing EBM methods on CIFAR-10 and
ImageNet datasets. We also demonstrate the effectiveness of our models for
several downstream tasks, including classifier-free guided generation,
compositional generation, image inpainting and out-of-distribution detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Confidence on the Focal: Conformal Prediction with <span class="highlight-title">Selection</span>-Conditional
  Coverage 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03868v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03868v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying Jin, Zhimei Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conformal prediction builds marginally valid prediction intervals that cover
the unknown outcome of a randomly drawn new test point with a prescribed
probability. However, a common scenario in practice is that, after seeing the
data, practitioners decide which test unit(s) to focus on in a data-driven
manner and seek for uncertainty quantification of the focal unit(s). In such
cases, marginally valid conformal prediction intervals may not provide valid
coverage for the focal unit(s) due to selection bias. This paper presents a
general framework for constructing a prediction set with finite-sample exact
coverage conditional on the unit being selected by a given procedure. The
general form of our method works for arbitrary selection rules that are
invariant to the permutation of the calibration units, and generalizes Mondrian
Conformal Prediction to multiple test units and non-equivariant classifiers. We
then work out the computationally efficient implementation of our framework for
a number of realistic selection rules, including top-K selection,
optimization-based selection, selection based on conformal p-values, and
selection based on properties of preliminary conformal prediction sets. The
performance of our methods is demonstrated via applications in drug discovery
and health risk prediction.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-23T00:00:00Z">2024-03-23</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computer Science and Game Theory
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Stability of Learning in Network <span class="highlight-title">Game</span>s with Many Players <span class="chip">AAMAS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15848v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15848v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aamal Hussain, Dan Leonte, Francesco Belardinelli, Georgios Piliouras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-agent learning algorithms have been shown to display complex, unstable
behaviours in a wide array of games. In fact, previous works indicate that
convergent behaviours are less likely to occur as the total number of agents
increases. This seemingly prohibits convergence to stable strategies, such as
Nash Equilibria, in games with many players.
  To make progress towards addressing this challenge we study the Q-Learning
Dynamics, a classical model for exploration and exploitation in multi-agent
learning. In particular, we study the behaviour of Q-Learning on games where
interactions between agents are constrained by a network. We determine a number
of sufficient conditions, depending on the game and network structure, which
guarantee that agent strategies converge to a unique stable strategy, called
the Quantal Response Equilibrium (QRE). Crucially, these sufficient conditions
are independent of the total number of agents, allowing for provable
convergence in arbitrarily large games.
  Next, we compare the learned QRE to the underlying NE of the game, by showing
that any QRE is an $\epsilon$-approximate Nash Equilibrium. We first provide
tight bounds on $\epsilon$ and show how these bounds lead naturally to a
centralised scheme for choosing exploration rates, which enables independent
learners to learn stable approximate Nash Equilibrium strategies. We validate
the method through experiments and demonstrate its effectiveness even in the
presence of numerous agents and actions. Through these results, we show that
independent learning dynamics may converge to approximate Nash Equilibria, even
in the presence of many agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAMAS 2024. arXiv admin note: text overlap with arXiv:2307.13922</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Non-convex potential <span class="highlight-title">game</span>s for finding global solutions to sensor
  network localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.03326v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.03326v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gehui Xu, Guanpu Chen, Yiguang Hong, Baris Fidan, Thomas Parisini, Karl H. Johansson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sensor network localization (SNL) problems require determining the physical
coordinates of all sensors in a network. This process relies on the global
coordinates of anchors and the available measurements between non-anchor and
anchor nodes. Attributed to the intrinsic non-convexity, obtaining a globally
optimal solution to SNL is challenging, as well as implementing corresponding
algorithms. In this paper, we formulate a non-convex multi-player potential
game for a generic SNL problem to investigate the identification condition of
the global Nash equilibrium (NE) therein, where the global NE represents the
global solution of SNL. We employ canonical duality theory to transform the
non-convex game into a complementary dual problem. Then we develop a
conjugation-based algorithm to compute the stationary points of the
complementary dual problem. On this basis, we show an identification condition
of the global NE: the stationary point of the proposed algorithm satisfies a
duality relation. Finally, simulation results are provided to validate the
effectiveness of the theoretical results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cooperative networks and f-Shapley value 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.06860v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.06860v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tongseok Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lloyd Shapley's cooperative value allocation theory stands as a central
concept in game theory, extensively utilized across various domains to
distribute resources, evaluate individual contributions, and ensure fairness.
The Shapley value formula and his four axioms that characterize it form the
foundation of the theory.
  Traditionally, the Shapley value is assigned under the assumption that all
players in a cooperative game will ultimately form the grand coalition. In this
paper, we reinterpret the Shapley value as an expectation of a certain
stochastic path integral, with each path representing a general coalition
formation process. As a result, the value allocation is naturally extended to
all partial coalition states. In addition, we provide a set of five properties
that extend the Shapley axioms and characterize the stochastic path integral.
Finally, by integrating Hodge calculus, stochastic processes, and path
integration of edge flows on graphs, we expand the cooperative value allocation
theory beyond the standard coalition game structure to encompass a broader
range of cooperative network configurations.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Restricted Bayesian Neural Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04810v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04810v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sourav Ganguly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern deep learning tools are remarkably effective in addressing intricate
problems. However, their operation as black-box models introduces increased
uncertainty in predictions. Additionally, they contend with various challenges,
including the need for substantial storage space in large networks, issues of
overfitting, underfitting, vanishing gradients, and more. This study explores
the concept of Bayesian Neural Networks, presenting a novel architecture
designed to significantly alleviate the storage space complexity of a network.
Furthermore, we introduce an algorithm adept at efficiently handling
uncertainties, ensuring robust convergence values without becoming trapped in
local optima, particularly when the objective function lacks perfect convexity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spiking-LEAF: A Learnable Auditory front-end for Spiking Neural Networks <span class="chip">ICASSP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.09469v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.09469v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyang Song, Jibin Wu, Malu Zhang, Mike Zheng Shou, Haizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Brain-inspired spiking neural networks (SNNs) have demonstrated great
potential for temporal signal processing. However, their performance in speech
processing remains limited due to the lack of an effective auditory front-end.
To address this limitation, we introduce Spiking-LEAF, a learnable auditory
front-end meticulously designed for SNN-based speech processing. Spiking-LEAF
combines a learnable filter bank with a novel two-compartment spiking neuron
model called IHC-LIF. The IHC-LIF neurons draw inspiration from the structure
of inner hair cells (IHC) and they leverage segregated dendritic and somatic
compartments to effectively capture multi-scale temporal dynamics of speech
signals. Additionally, the IHC-LIF neurons incorporate the lateral feedback
mechanism along with spike regularization loss to enhance spike encoding
efficiency. On keyword spotting and speaker identification tasks, the proposed
Spiking-LEAF outperforms both SOTA spiking auditory front-ends and conventional
real-valued acoustic features in terms of classification accuracy, noise
robustness, and encoding efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Gaussian Covariance Network with Trajectory Sampling for
  Data-Efficient <span class="highlight-title">Policy</span> Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15908v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15908v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Can Bogoclu, Robert Vosshall, Kevin Cremanns, Dirk Roos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Probabilistic world models increase data efficiency of model-based
reinforcement learning (MBRL) by guiding the policy with their epistemic
uncertainty to improve exploration and acquire new samples. Moreover, the
uncertainty-aware learning procedures in probabilistic approaches lead to
robust policies that are less sensitive to noisy observations compared to
uncertainty unaware solutions. We propose to combine trajectory sampling and
deep Gaussian covariance network (DGCN) for a data-efficient solution to MBRL
problems in an optimal control setting. We compare trajectory sampling with
density-based approximation for uncertainty propagation using three different
probabilistic world models; Gaussian processes, Bayesian neural networks, and
DGCNs. We provide empirical evidence using four different well-known test
environments, that our method improves the sample-efficiency over other
combinations of uncertainty propagation methods and probabilistic models.
During our tests, we place particular emphasis on the robustness of the learned
policies with respect to noisy initial states.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast and Unified Path Gradient Estimators for Normalizing Flows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15881v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15881v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenz Vaitl, Ludwig Winkler, Lorenz Richter, Pan Kessel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work shows that path gradient estimators for normalizing flows have
lower variance compared to standard estimators for variational inference,
resulting in improved training. However, they are often prohibitively more
expensive from a computational point of view and cannot be applied to maximum
likelihood training in a scalable manner, which severely hinders their
widespread adoption. In this work, we overcome these crucial limitations.
Specifically, we propose a fast path gradient estimator which improves
computational efficiency significantly and works for all normalizing flow
architectures of practical relevance. We then show that this estimator can also
be applied to maximum likelihood training for which it has a regularizing
effect as it can take the form of a given target energy function into account.
We empirically establish its superior performance and reduced variance for
several natural sciences applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrated path stability <span class="highlight-title">selection</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15877v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15877v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omar Melikechi, Jeffrey W. Miller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stability selection is a widely used method for improving the performance of
feature selection algorithms. However, stability selection has been found to be
highly conservative, resulting in low sensitivity. Further, the theoretical
bound on the expected number of false positives, E(FP), is relatively loose,
making it difficult to know how many false positives to expect in practice. In
this paper, we introduce a novel method for stability selection based on
integrating the stability paths rather than maximizing over them. This yields a
tighter bound on E(FP), resulting in a feature selection criterion that has
higher sensitivity in practice and is better calibrated in terms of matching
the target E(FP). Our proposed method requires the same amount of computation
as the original stability selection algorithm, and only requires the user to
specify one input parameter, a target value for E(FP). We provide theoretical
bounds on performance, and demonstrate the method on simulations and real data
from cancer gene expression studies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Holographic Global Convolutional Networks for Long-Range Prediction
  Tasks in Malware Detection <span class="chip">AISTATS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17978v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17978v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Mahmudul Alam, Edward Raff, Stella Biderman, Tim Oates, James Holt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Malware detection is an interesting and valuable domain to work in because it
has significant real-world impact and unique machine-learning challenges. We
investigate existing long-range techniques and benchmarks and find that they're
not very suitable in this problem area. In this paper, we introduce Holographic
Global Convolutional Networks (HGConv) that utilize the properties of
Holographic Reduced Representations (HRR) to encode and decode features from
sequence elements. Unlike other global convolutional methods, our method does
not require any intricate kernel computation or crafted kernel design. HGConv
kernels are defined as simple parameters learned through backpropagation. The
proposed method has achieved new SOTA results on Microsoft Malware
Classification Challenge, Drebin, and EMBER malware benchmarks. With log-linear
complexity in sequence length, the empirical results demonstrate substantially
faster run-time by HGConv compared to other methods achieving far more
efficient scaling even with sequence length $\geq 100,000$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in Proceedings of the 27th International Conference on
  Artificial Intelligence and Statistics (AISTATS) 2024, Valencia, Spain</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Computational Sentence-level Metrics Predicting Human Sentence
  Comprehension 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15822v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15822v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Sun, Rong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The majority of research in computational psycholinguistics has concentrated
on the processing of words. This study introduces innovative methods for
computing sentence-level metrics using multilingual large language models. The
metrics developed sentence surprisal and sentence relevance and then are tested
and compared to validate whether they can predict how humans comprehend
sentences as a whole across languages. These metrics offer significant
interpretability and achieve high accuracy in predicting human sentence reading
speeds. Our results indicate that these computational sentence-level metrics
are exceptionally effective at predicting and elucidating the processing
difficulties encountered by readers in comprehending sentences as a whole
across a variety of languages. Their impressive performance and generalization
capabilities provide a promising avenue for future research in integrating LLMs
and cognitive science.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boarding for ISS: Imbalanced Self-Supervised: Discovery of a Scaled
  Autoencoder for Mixed Tabular <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15790v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15790v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Stocksieker, Denys Pommeret, Arthur Charpentier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of imbalanced self-supervised learning, especially in the context
of tabular data, has not been extensively studied. Existing research has
predominantly focused on image datasets. This paper aims to fill this gap by
examining the specific challenges posed by data imbalance in self-supervised
learning in the domain of tabular data, with a primary focus on autoencoders.
Autoencoders are widely employed for learning and constructing a new
representation of a dataset, particularly for dimensionality reduction. They
are also often used for generative model learning, as seen in variational
autoencoders. When dealing with mixed tabular data, qualitative variables are
often encoded using a one-hot encoder with a standard loss function (MSE or
Cross Entropy). In this paper, we analyze the drawbacks of this approach,
especially when categorical variables are imbalanced. We propose a novel metric
to balance learning: a Multi-Supervised Balanced MSE. This approach reduces the
reconstruction error by balancing the influence of variables. Finally, we
empirically demonstrate that this new metric, compared to the standard MSE: i)
outperforms when the dataset is imbalanced, especially when the learning
process is insufficient, and ii) provides similar results in the opposite case.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Supervised Learning via Ensembles of Diverse Functional Representations:
  the Functional Voting Classifier 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15778v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15778v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donato Riccio, Fabrizio Maturo, Elvira Romano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many conventional statistical and machine learning methods face challenges
when applied directly to high dimensional temporal observations. In recent
decades, Functional Data Analysis (FDA) has gained widespread popularity as a
framework for modeling and analyzing data that are, by their nature, functions
in the domain of time. Although supervised classification has been extensively
explored in recent decades within the FDA literature, ensemble learning of
functional classifiers has only recently emerged as a topic of significant
interest. Thus, the latter subject presents unexplored facets and challenges
from various statistical perspectives. The focal point of this paper lies in
the realm of ensemble learning for functional data and aims to show how
different functional data representations can be used to train ensemble members
and how base model predictions can be combined through majority voting. The
so-called Functional Voting Classifier (FVC) is proposed to demonstrate how
different functional representations leading to augmented diversity can
increase predictive accuracy. Many real-world datasets from several domains are
used to display that the FVC can significantly enhance performance compared to
individual models. The framework presented provides a foundation for voting
ensembles with functional data and can stimulate a highly encouraging line of
research in the FDA context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 20 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identifiable Latent Neural Causal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15711v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15711v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhang Liu, Zhen Zhang, Dong Gong, Mingming Gong, Biwei Huang, Anton van den Hengel, Kun Zhang, Javen Qinfeng Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal representation learning seeks to uncover latent, high-level causal
representations from low-level observed data. It is particularly good at
predictions under unseen distribution shifts, because these shifts can
generally be interpreted as consequences of interventions. Hence leveraging
{seen} distribution shifts becomes a natural strategy to help identifying
causal representations, which in turn benefits predictions where distributions
are previously {unseen}. Determining the types (or conditions) of such
distribution shifts that do contribute to the identifiability of causal
representations is critical. This work establishes a {sufficient} and
{necessary} condition characterizing the types of distribution shifts for
identifiability in the context of latent additive noise models. Furthermore, we
present partial identifiability results when only a portion of distribution
shifts meets the condition. In addition, we extend our findings to latent
post-nonlinear causal models. We translate our findings into a practical
algorithm, allowing for the acquisition of reliable latent causal
representations. Our algorithm, guided by our underlying theory, has
demonstrated outstanding performance across a diverse range of synthetic and
real-world datasets. The empirical observations align closely with the
theoretical findings, affirming the robustness and effectiveness of our
approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Role of Locality and Weight Sharing in Image-Based Tasks: A <span class="highlight-title">Sample</span>
  Complexity Separation between CNNs, LCNs, and FCNs <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aakash Lahoti, Stefani Karp, Ezra Winston, Aarti Singh, Yuanzhi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision tasks are characterized by the properties of locality and translation
invariance. The superior performance of convolutional neural networks (CNNs) on
these tasks is widely attributed to the inductive bias of locality and weight
sharing baked into their architecture. Existing attempts to quantify the
statistical benefits of these biases in CNNs over locally connected
convolutional neural networks (LCNs) and fully connected neural networks (FCNs)
fall into one of the following categories: either they disregard the optimizer
and only provide uniform convergence upper bounds with no separating lower
bounds, or they consider simplistic tasks that do not truly mirror the locality
and translation invariance as found in real-world vision tasks. To address
these deficiencies, we introduce the Dynamic Signal Distribution (DSD)
classification task that models an image as consisting of $k$ patches, each of
dimension $d$, and the label is determined by a $d$-sparse signal vector that
can freely appear in any one of the $k$ patches. On this task, for any
orthogonally equivariant algorithm like gradient descent, we prove that CNNs
require $\tilde{O}(k+d)$ samples, whereas LCNs require $\Omega(kd)$ samples,
establishing the statistical advantages of weight sharing in translation
invariant tasks. Furthermore, LCNs need $\tilde{O}(k(k+d))$ samples, compared
to $\Omega(k^2d)$ samples for FCNs, showcasing the benefits of locality in
local tasks. Additionally, we develop information theoretic tools for analyzing
randomized algorithms, which may be of interest for statistical research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages, 4 figures, Accepted to ICLR 2024, Spotlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Action Learning in High Dimensions: A Conservative Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2009.13961v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2009.13961v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Claudio Cardoso Flores, Marcelo Cunha Medeiros
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential learning problems are common in several fields of research and
practical applications. Examples include dynamic pricing and assortment, design
of auctions and incentives and permeate a large number of sequential treatment
experiments. In this paper, we extend one of the most popular learning
solutions, the $\epsilon_t$-greedy heuristics, to high-dimensional contexts
considering a conservative directive. We do this by allocating part of the time
the original rule uses to adopt completely new actions to a more focused search
in a restrictive set of promising actions. The resulting rule might be useful
for practical applications that still values surprises, although at a
decreasing rate, while also has restrictions on the adoption of unusual
actions. With high probability, we find reasonable bounds for the cumulative
regret of a conservative high-dimensional decaying $\epsilon_t$-greedy rule.
Also, we provide a lower bound for the cardinality of the set of viable actions
that implies in an improved regret bound for the conservative version when
compared to its non-conservative counterpart. Additionally, we show that
end-users have sufficient flexibility when establishing how much safety they
want, since it can be tuned without impacting theoretical properties. We
illustrate our proposal both in a simulation exercise and using a real dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We found an error in the proof of the main theorem which cannot be
  fixed without completely changing the results in the paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Convergence Analysis of Stochastic Gradient Descent with MCMC Estimators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10599v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10599v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyou Li, Fan Chen, Huajie Chen, Zaiwen Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding stochastic gradient descent (SGD) and its variants is essential
for machine learning. However, most of the preceding analyses are conducted
under amenable conditions such as unbiased gradient estimator and bounded
objective functions, which does not encompass many sophisticated applications,
such as variational Monte Carlo, entropy-regularized reinforcement learning and
variational inference. In this paper, we consider the SGD algorithm that employ
the Markov Chain Monte Carlo (MCMC) estimator to compute the gradient, called
MCMC-SGD. Since MCMC reduces the sampling complexity significantly, it is an
asymptotically convergent biased estimator in practice. Moreover, by
incorporating a general class of unbounded functions, it is much more difficult
to analyze the MCMC sampling error. Therefore, we assume that the function is
sub-exponential and use the Bernstein inequality for non-stationary Markov
chains to derive error bounds of the MCMC estimator. Consequently, MCMC-SGD is
proven to have a first order convergence rate $O(\log K/\sqrt{n K})$ with $K$
iterations and a sample size $n$. It partially explains how MCMC influences the
behavior of SGD. Furthermore, we verify the correlated negative curvature
condition under reasonable assumptions. It is shown that MCMC-SGD escapes from
saddle points and reaches $(\epsilon,\epsilon^{1/4})$ approximate second order
stationary points or $\epsilon^{1/2}$-variance points at least
$O(\epsilon^{-11/2}\log^{2}(1/\epsilon) )$ steps with high probability. Our
analysis unveils the convergence pattern of MCMC-SGD across a broad class of
stochastic optimization problems, and interprets the convergence phenomena
observed in practical applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Effect of Ambient-Intrinsic Dimension Gap on Adversarial Vulnerability <span class="chip">AISTATS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03967v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03967v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rajdeep Haldar, Yue Xing, Qifan Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The existence of adversarial attacks on machine learning models imperceptible
to a human is still quite a mystery from a theoretical perspective. In this
work, we introduce two notions of adversarial attacks: natural or on-manifold
attacks, which are perceptible by a human/oracle, and unnatural or off-manifold
attacks, which are not. We argue that the existence of the off-manifold attacks
is a natural consequence of the dimension gap between the intrinsic and ambient
dimensions of the data. For 2-layer ReLU networks, we prove that even though
the dimension gap does not affect generalization performance on samples drawn
from the observed data space, it makes the clean-trained model more vulnerable
to adversarial perturbations in the off-manifold direction of the data space.
Our main results provide an explicit relationship between the
$\ell_2,\ell_{\infty}$ attack strength of the on/off-manifold attack and the
dimension gap.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AISTATS 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-22T00:00:00Z">2024-03-22</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computer Science and Game Theory
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Variational Interpretation of Mirror Play in Monotone <span class="highlight-title">Game</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15636v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15636v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunian Pan, Tao Li, Quanyan Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mirror play (MP) is a well-accepted primal-dual multi-agent learning
algorithm where all agents simultaneously implement mirror descent in a
distributed fashion. The advantage of MP over vanilla gradient play lies in its
usage of mirror maps that better exploit the geometry of decision domains.
Despite extensive literature dedicated to the asymptotic convergence of MP to
equilibrium, the understanding of the finite-time behavior of MP before
reaching equilibrium is still rudimentary. To facilitate the study of MP's
non-equilibrium performance, this work establishes an equivalence between MP's
finite-time primal-dual path (mirror path) in monotone games and the
closed-loop Nash equilibrium path of a finite-horizon differential game,
referred to as mirror differential game (MDG). Our construction of MDG rests on
the Brezis-Ekeland variational principle, and the stage cost functional for MDG
is Fenchel coupling between MP's iterates and associated gradient updates. The
variational interpretation of mirror path in static games as the equilibrium
path in MDG holds in deterministic and stochastic cases. Such a variational
interpretation translates the non-equilibrium studies of learning dynamics into
a more tractable equilibrium analysis of dynamic games, as demonstrated in a
case study on the Cournot game, where MP dynamics corresponds to a linear
quadratic game.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Balancing Fairness and Efficiency in Energy Resource Allocations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15616v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15616v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayi Li, Matthew Motoki, Baosen Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bringing fairness to energy resource allocation remains a challenge, due to
the complexity of system structures and economic interdependencies among users
and system operators' decision-making. The rise of distributed energy resources
has introduced more diverse heterogeneous user groups, surpassing the
capabilities of traditional efficiency-oriented allocation schemes. Without
explicitly bringing fairness to user-system interaction, this disparity often
leads to disproportionate payments for certain user groups due to their utility
formats or group sizes.
  Our paper addresses this challenge by formalizing the problem of fair energy
resource allocation and introducing the framework for aggregators. This
framework enables optimal fairness-efficiency trade-offs by selecting
appropriate objectives in a principled way. By jointly optimizing over the
total resources to allocate and individual allocations, our approach reveals
optimized allocation schemes that lie on the Pareto front, balancing fairness
and efficiency in resource allocation strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Strategic Network Creation for Enabling Greedy Routing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Berger, Tobias Friedrich, Pascal Lenzner, Paraskevi Machaira, Janosch Ruff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present the first game-theoretic network creation model
that incorporates greedy routing, i.e., the agents in our model are embedded in
some metric space and strive for creating a network where all-pairs greedy
routing is enabled. In contrast to graph-theoretic shortest paths, our agents
route their traffic along greedy paths, which are sequences of nodes where the
distance in the metric space to the respective target node gets strictly
smaller by each hop. Besides enabling greedy routing, the agents also optimize
their connection quality within the created network by constructing greedy
paths with low stretch. This ensures that greedy routing is always possible in
equilibrium networks, while realistically modeling the agents' incentives for
local structural changes to the network. With this we augment the elegant
network creation model by Moscibroda, Schmidt, and Wattenhofer (PODC'06) with
the feature of greedy routing.
  For our model, we analyze the existence of (approximate)-equilibria and the
computational hardness in different underlying metric spaces. E.g., we
characterize the set of equilibria in 1-2-metrics and tree metrics, we show
that in both metrics Nash equilibria always exist, and we prove that the
well-known $\Theta$-graph construction yields constant-approximate Nash
equilibria in Euclidean space. The latter justifies distributed network
construction via $\Theta$-graphs from a new point-of-view, since it shows that
this powerful technique not only guarantees networks having a low stretch but
also networks that are almost stable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human behaviour through a LENS: How Linguistic content triggers Emotions
  and Norms and determines <span class="highlight-title">Strategy</span> choices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15293v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15293v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valerio Capraro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the last two decades, a growing body of experimental research has
provided evidence that linguistic frames influence human behaviour in economic
games, beyond the economic consequences of the available actions. This article
proposes a novel framework that transcends the traditional confines of
outcome-based preference models. According to the LENS model, the Linguistic
description of the decision problem triggers Emotional responses and suggests
potential Norms of behaviour, which then interact to shape an individual's
Strategic choice. The article reviews experimental evidence that supports each
path of the LENS model. Furthermore, it identifies and discusses several
critical research questions that arise from this model, pointing towards
avenues for future inquiry.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PPA-<span class="highlight-title">Game</span>: Characterizing and Learning Competitive Dynamics Among Online
  Content Creators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15524v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15524v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renzhe Xu, Haotian Wang, Xingxuan Zhang, Bo Li, Peng Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the Proportional Payoff Allocation Game (PPA-Game) to model how
agents, akin to content creators on platforms like YouTube and TikTok, compete
for divisible resources and consumers' attention. Payoffs are allocated to
agents based on heterogeneous weights, reflecting the diversity in content
quality among creators. Our analysis reveals that although a pure Nash
equilibrium (PNE) is not guaranteed in every scenario, it is commonly observed,
with its absence being rare in our simulations. Beyond analyzing static
payoffs, we further discuss the agents' online learning about resource payoffs
by integrating a multi-player multi-armed bandit framework. We propose an
online algorithm facilitating each agent's maximization of cumulative payoffs
over $T$ rounds. Theoretically, we establish that the regret of any agent is
bounded by $O(\log^{1 + \eta} T)$ for any $\eta > 0$. Empirical results further
validate the effectiveness of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A-PSRO: A Unified <span class="highlight-title">Strategy</span> Learning Method with Advantage Function for
  Normal-form <span class="highlight-title">Game</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12520v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12520v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yudong Hu, Haoran Li, Congying Han, Tiande Guo, Mingqiang Li, Bonan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Solving Nash equilibrium is the key challenge in normal-form games with large
strategy spaces, where open-ended learning frameworks offer an efficient
approach. In this work, we propose an innovative unified open-ended learning
framework A-PSRO, i.e., Advantage Policy Space Response Oracle, as a
comprehensive framework for both zero-sum and general-sum games. In particular,
we introduce the advantage function as an enhanced evaluation metric for
strategies, enabling a unified learning objective for agents engaged in
normal-form games. We prove that the advantage function exhibits favorable
properties and is connected with the Nash equilibrium, which can be used as an
objective to guide agents to learn strategies efficiently. Our experiments
reveal that A-PSRO achieves a considerable decrease in exploitability in
zero-sum games and an escalation in rewards in general-sum games, significantly
outperforming previous PSRO algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Latent Neural Cellular Automata for Resource-Efficient Image Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Menta, Alberto Archetti, Matteo Matteucci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural cellular automata represent an evolution of the traditional cellular
automata model, enhanced by the integration of a deep learning-based transition
function. This shift from a manual to a data-driven approach significantly
increases the adaptability of these models, enabling their application in
diverse domains, including content generation and artificial life. However,
their widespread application has been hampered by significant computational
requirements. In this work, we introduce the Latent Neural Cellular Automata
(LNCA) model, a novel architecture designed to address the resource limitations
of neural cellular automata. Our approach shifts the computation from the
conventional input space to a specially designed latent space, relying on a
pre-trained autoencoder. We apply our model in the context of image
restoration, which aims to reconstruct high-quality images from their degraded
versions. This modification not only reduces the model's resource consumption
but also maintains a flexible framework suitable for various applications. Our
model achieves a significant reduction in computational requirements while
maintaining high reconstruction fidelity. This increase in efficiency allows
for inputs up to 16 times larger than current state-of-the-art neural cellular
automata models, using the same resources.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Magic for the Age of Quantized DNNs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14999v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14999v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoshihide Sawada, Ryuji Saiin, Kazuma Suetake
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the number of parameters in DNNs has explosively increased, as
exemplified by LLMs (Large Language Models), making inference on small-scale
computers more difficult. Model compression technology is, therefore, essential
for integration into products. In this paper, we propose a method of
quantization-aware training. We introduce a novel normalization (Layer-Batch
Normalization) that is independent of the mini-batch size and does not require
any additional computation cost during inference. Then, we quantize the weights
by the scaled round-clip function with the weight standardization. We also
quantize activation functions using the same function and apply surrogate
gradients to train the model with both quantized weights and the quantized
activation functions. We call this method Magic for the age of Quantised DNNs
(MaQD). Experimental results show that our quantization method can be achieved
with minimal accuracy degradation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimisation of photodetectors design: comparison between Montecarlo and
  Genetic Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14913v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14913v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patricia M. E. Vázquez, Ligia Ciocci Brazzano, Francisco E. Veiras, Patricio A. Sorichetti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Montecarlo and Genetic Algorithm optimisations applied to the
design of photodetectors based on a transimpedance amplifier and a photodiode.
The circuit performance is evaluated with a merit function and the systematic
search method is used as a reference. The design parameters are the feedback
network components and the photodiode bias voltage. To evaluate the
optimisations, we define the relative difference between its merit and the
optimum merit obtained by the systematic search. In both algorithms, the
relative difference decreases with the number of evaluations, following a power
law. The power-law exponent for the Genetic Algorithm is larger than that of
Montecarlo (0.74 vs. 0.50). We conclude that both algorithms are advantageous
compared to the systematic search method, and that the Genetic Algorithm shows
a better performance than Montecarlo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Workload-Balanced Pruning for Sparse Spiking Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.06746v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.06746v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruokai Yin, Youngeun Kim, Yuhang Li, Abhishek Moitra, Nitin Satpute, Anna Hambitzer, Priyadarshini Panda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pruning for Spiking Neural Networks (SNNs) has emerged as a fundamental
methodology for deploying deep SNNs on resource-constrained edge devices.
Though the existing pruning methods can provide extremely high weight sparsity
for deep SNNs, the high weight sparsity brings a workload imbalance problem.
Specifically, the workload imbalance happens when a different number of
non-zero weights are assigned to hardware units running in parallel. This
results in low hardware utilization and thus imposes longer latency and higher
energy costs. In preliminary experiments, we show that sparse SNNs (~98% weight
sparsity) can suffer as low as ~59% utilization. To alleviate the workload
imbalance problem, we propose u-Ticket, where we monitor and adjust the weight
connections of the SNN during Lottery Ticket Hypothesis (LTH) based pruning,
thus guaranteeing the final ticket gets optimal utilization when deployed onto
the hardware. Experiments indicate that our u-Ticket can guarantee up to 100%
hardware utilization, thus reducing up to 76.9% latency and 63.8% energy cost
compared to the non-utilization-aware LTH method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages. Accepted to IEEE Transactions on Emerging Topics in
  Computational Intelligence (2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Complexity to Clarity: Analytical Expressions of Deep Neural
  Network Weights via Clifford's Geometric Algebra and Convexity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.16512v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.16512v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mert Pilanci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a novel analysis of neural networks based on
geometric (Clifford) algebra and convex optimization. We show that optimal
weights of deep ReLU neural networks are given by the wedge product of training
samples when trained with standard regularized loss. Furthermore, the training
problem reduces to convex optimization over wedge product features, which
encode the geometric structure of the training dataset. This structure is given
in terms of signed volumes of triangles and parallelotopes generated by data
vectors. The convex problem finds a small subset of samples via $\ell_1$
regularization to discover only relevant wedge product features. Our analysis
provides a novel perspective on the inner workings of deep neural networks and
sheds light on the role of the hidden layers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model Uncertainty in Evolutionary Optimization and Bayesian
  Optimization: A Comparative Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14413v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14413v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Hao, Xiaoqun Zhang, Aimin Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Black-box optimization problems, which are common in many real-world
applications, require optimization through input-output interactions without
access to internal workings. This often leads to significant computational
resources being consumed for simulations. Bayesian Optimization (BO) and
Surrogate-Assisted Evolutionary Algorithm (SAEA) are two widely used
gradient-free optimization techniques employed to address such challenges. Both
approaches follow a similar iterative procedure that relies on surrogate models
to guide the search process. This paper aims to elucidate the similarities and
differences in the utilization of model uncertainty between these two methods,
as well as the impact of model inaccuracies on algorithmic performance. A novel
model-assisted strategy is introduced, which utilizes unevaluated solutions to
generate offspring, leveraging the population-based search capabilities of
evolutionary algorithm to enhance the effectiveness of model-assisted
optimization. Experimental results demonstrate that the proposed approach
outperforms mainstream Bayesian optimization algorithms in terms of accuracy
and efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RFI Detection with Spiking Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.14303v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.14303v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas J. Pritchard, Andreas Wicenec, Mohammed Bennamoun, Richard Dodson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting and mitigating Radio Frequency Interference (RFI) is critical for
enabling and maximising the scientific output of radio telescopes. The
emergence of machine learning methods has led to their application in radio
astronomy, and in RFI detection. Spiking Neural Networks (SNNs), inspired by
biological systems, are well-suited for processing spatio-temporal data. This
study introduces the first exploratory application of SNNs to an astronomical
data-processing task, specifically RFI detection. We adapt the
nearest-latent-neighbours (NLN) algorithm and auto-encoder architecture
proposed by previous authors to SNN execution by direct ANN2SNN conversion,
enabling simplified downstream RFI detection by sampling the naturally varying
latent space from the internal spiking neurons. Our subsequent evaluation aims
to determine whether SNNs are viable for future RFI detection schemes. We
evaluate detection performance with the simulated HERA telescope and
hand-labelled LOFAR observation dataset the original authors provided. We
additionally evaluate detection performance with a new MeerKAT-inspired
simulation dataset that provides a technical challenge for machine-learnt RFI
detection methods. This dataset focuses on satellite-based RFI, an increasingly
important class of RFI and is an additional contribution. Our approach remains
competitive with existing methods in AUROC, AUPRC and F1 scores for the HERA
dataset but exhibits difficulty in the LOFAR and Tabascal datasets. Our method
maintains this accuracy while completely removing the compute and
memory-intense latent sampling step found in NLN. This work demonstrates the
viability of SNNs as a promising avenue for machine-learning-based RFI
detection in radio telescopes by establishing a minimal performance baseline on
traditional and nascent satellite-based RFI sources and is the first work to
our knowledge to apply SNNs in astronomy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures, 5 tables. Accepted for publication in PASA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training Fully Connected Neural Networks is $\exists\mathbb{R}$-Complete 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.01368v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.01368v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Bertschinger, Christoph Hertrich, Paul Jungeblut, Tillmann Miltzow, Simon Weber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of finding weights and biases for a two-layer fully
connected neural network to fit a given set of data points as well as possible,
also known as EmpiricalRiskMinimization. Our main result is that the associated
decision problem is $\exists\mathbb{R}$-complete, that is, polynomial-time
equivalent to determining whether a multivariate polynomial with integer
coefficients has any real roots. Furthermore, we prove that algebraic numbers
of arbitrarily large degree are required as weights to be able to train some
instances to optimality, even if all data points are rational. Our result
already applies to fully connected instances with two inputs, two outputs, and
one hidden layer of ReLU neurons. Thereby, we strengthen a result by
Abrahamsen, Kleist and Miltzow [NeurIPS 2021]. A consequence of this is that a
combinatorial search algorithm like the one by Arora, Basu, Mianjy and
Mukherjee [ICLR 2018] is impossible for networks with more than one output
dimension, unless $\mathsf{NP}=\exists\mathbb{R}$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages, 17 figures. Changes in version 2: Added algebraic
  universality result, improved interpretation of results Changes in version 3:
  Improved exposition by formalizing properties of gadgets</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Wasserstein perspective of Vanilla GANs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15312v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15312v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lea Kunkel, Mathias Trabs
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The empirical success of Generative Adversarial Networks (GANs) caused an
increasing interest in theoretical research. The statistical literature is
mainly focused on Wasserstein GANs and generalizations thereof, which
especially allow for good dimension reduction properties. Statistical results
for Vanilla GANs, the original optimization problem, are still rather limited
and require assumptions such as smooth activation functions and equal
dimensions of the latent space and the ambient space. To bridge this gap, we
draw a connection from Vanilla GANs to the Wasserstein distance. By doing so,
existing results for Wasserstein GANs can be extended to Vanilla GANs. In
particular, we obtain an oracle inequality for Vanilla GANs in Wasserstein
distance. The assumptions of this oracle inequality are designed to be
satisfied by network architectures commonly used in practice, such as
feedforward ReLU networks. By providing a quantitative result for the
approximation of a Lipschitz function by a feedforward ReLU network with
bounded H\"older norm, we conclude a rate of convergence for Vanilla GANs as
well as Wasserstein GANs as estimators of the unknown probability distribution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conformal online model aggregation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15527v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15527v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Gasparin, Aaditya Ramdas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conformal prediction equips machine learning models with a reasonable notion
of uncertainty quantification without making strong distributional assumptions.
It wraps around any black-box prediction model and converts point predictions
into set predictions that have a predefined marginal coverage guarantee.
However, conformal prediction only works if we fix the underlying machine
learning model in advance. A relatively unaddressed issue in conformal
prediction is that of model selection and/or aggregation: for a given problem,
which of the plethora of prediction methods (random forests, neural nets,
regularized linear models, etc.) should we conformalize? This paper proposes a
new approach towards conformal model aggregation in online settings that is
based on combining the prediction sets from several algorithms by voting, where
weights on the models are adapted over time based on past performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 8 figures. arXiv admin note: substantial text overlap with
  arXiv:2401.09379</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Bayesian Deep Learning: The Application of Statistical
  Aggregation Methods to Bayesian Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15263v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15263v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Fischer, Marko Orescanin, Justin Loomis, Patrick McClure
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) is an approach to training machine learning models
that takes advantage of multiple distributed datasets while maintaining data
privacy and reducing communication costs associated with sharing local
datasets. Aggregation strategies have been developed to pool or fuse the
weights and biases of distributed deterministic models; however, modern
deterministic deep learning (DL) models are often poorly calibrated and lack
the ability to communicate a measure of epistemic uncertainty in prediction,
which is desirable for remote sensing platforms and safety-critical
applications. Conversely, Bayesian DL models are often well calibrated and
capable of quantifying and communicating a measure of epistemic uncertainty
along with a competitive prediction accuracy. Unfortunately, because the
weights and biases in Bayesian DL models are defined by a probability
distribution, simple application of the aggregation methods associated with FL
schemes for deterministic models is either impossible or results in sub-optimal
performance. In this work, we use independent and identically distributed (IID)
and non-IID partitions of the CIFAR-10 dataset and a fully variational
ResNet-20 architecture to analyze six different aggregation strategies for
Bayesian DL models. Additionally, we analyze the traditional federated
averaging approach applied to an approximate Bayesian Monte Carlo dropout model
as a lightweight alternative to more complex variational inference methods in
FL. We show that aggregation strategy is a key hyperparameter in the design of
a Bayesian FL system with downstream effects on accuracy, calibration,
uncertainty quantification, training stability, and client compute
requirements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Double Cross-fit Doubly Robust Estimators: Beyond Series Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15175v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15175v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alec McClean, Sivaraman Balakrishnan, Edward H. Kennedy, Larry Wasserman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Doubly robust estimators with cross-fitting have gained popularity in causal
inference due to their favorable structure-agnostic error guarantees. However,
when additional structure, such as H\"{o}lder smoothness, is available then
more accurate "double cross-fit doubly robust" (DCDR) estimators can be
constructed by splitting the training data and undersmoothing nuisance function
estimators on independent samples. We study a DCDR estimator of the Expected
Conditional Covariance, a functional of interest in causal inference and
conditional independence testing, and derive a series of increasingly powerful
results with progressively stronger assumptions. We first provide a
structure-agnostic error analysis for the DCDR estimator with no assumptions on
the nuisance functions or their estimators. Then, assuming the nuisance
functions are H\"{o}lder smooth, but without assuming knowledge of the true
smoothness level or the covariate density, we establish that DCDR estimators
with several linear smoothers are semiparametric efficient under minimal
conditions and achieve fast convergence rates in the non-$\sqrt{n}$ regime.
When the covariate density and smoothnesses are known, we propose a minimax
rate-optimal DCDR estimator based on undersmoothed kernel regression. Moreover,
we show an undersmoothed DCDR estimator satisfies a slower-than-$\sqrt{n}$
central limit theorem, and that inference is possible even in the
non-$\sqrt{n}$ regime. Finally, we support our theoretical results with
simulations, providing intuition for double cross-fitting and undersmoothing,
demonstrating where our estimator achieves semiparametric efficiency while the
usual "single cross-fit" estimator fails, and illustrating asymptotic normality
for the undersmoothed DCDR estimator.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantification using Permutation-Invariant Networks based on Histograms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15123v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15123v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olaya Pérez-Mon, Alejandro Moreo, Juan José del Coz, Pablo González
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantification, also known as class prevalence estimation, is the supervised
learning task in which a model is trained to predict the prevalence of each
class in a given bag of examples. This paper investigates the application of
deep neural networks to tasks of quantification in scenarios where it is
possible to apply a symmetric supervised approach that eliminates the need for
classification as an intermediary step, directly addressing the quantification
problem. Additionally, it discusses existing permutation-invariant layers
designed for set processing and assesses their suitability for quantification.
In light of our analysis, we propose HistNetQ, a novel neural architecture that
relies on a permutation-invariant representation based on histograms that is
specially suited for quantification problems. Our experiments carried out in
the only quantification competition held to date, show that HistNetQ
outperforms other deep neural architectures devised for set processing, as well
as the state-of-the-art quantification methods. Furthermore, HistNetQ offers
two significant advantages over traditional quantification methods: i) it does
not require the labels of the training examples but only the prevalence values
of a collection of training bags, making it applicable to new scenarios; and
ii) it is able to optimize any custom quantification-oriented loss function.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Active Learning for Regression based on Wasserstein distance and
  GroupSort Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15108v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15108v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Bobbia, Matthias Picard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses a new active learning strategy for regression problems.
The presented Wasserstein active regression model is based on the principles of
distribution-matching to measure the representativeness of the labeled dataset.
The Wasserstein distance is computed using GroupSort Neural Networks. The use
of such networks provides theoretical foundations giving a way to quantify
errors with explicit bounds for their size and depth. This solution is combined
with another uncertainty-based approach that is more outlier-tolerant to
complete the query strategy. Finally, this method is compared with other
classical and recent solutions. The study empirically shows the pertinence of
such a representativity-uncertainty approach, which provides good estimation
all along the query procedure. Moreover, the Wasserstein active regression
often achieves more precise estimations and tends to improve accuracy faster
than other models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Estimation of multiple mean vectors in high dimension 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15038v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15038v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gilles Blanchard, Jean-Baptiste Fermanian, Hannah Marienwald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We endeavour to estimate numerous multi-dimensional means of various
probability distributions on a common space based on independent samples. Our
approach involves forming estimators through convex combinations of empirical
means derived from these samples. We introduce two strategies to find
appropriate data-dependent convex combination weights: a first one employing a
testing procedure to identify neighbouring means with low variance, which
results in a closed-form plug-in formula for the weights, and a second one
determining weights via minimization of an upper confidence bound on the
quadratic risk.Through theoretical analysis, we evaluate the improvement in
quadratic risk offered by our methods compared to the empirical means. Our
analysis focuses on a dimensional asymptotics perspective, showing that our
methods asymptotically approach an oracle (minimax) improvement as the
effective dimension of the data increases.We demonstrate the efficacy of our
methods in estimating multiple kernel mean embeddings through experiments on
both simulated and real-world datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Conformal Prediction under Distribution Shift via
  Physics-Informed Structural Causal Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15025v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15025v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Xu, Yue Sun, Chao Chen, Parv Venkitasubramaniam, Sihong Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uncertainty is critical to reliable decision-making with machine learning.
Conformal prediction (CP) handles uncertainty by predicting a set on a test
input, hoping the set to cover the true label with at least $(1-\alpha)$
confidence. This coverage can be guaranteed on test data even if the marginal
distributions $P_X$ differ between calibration and test datasets. However, as
it is common in practice, when the conditional distribution $P_{Y|X}$ is
different on calibration and test data, the coverage is not guaranteed and it
is essential to measure and minimize the coverage loss under distributional
shift at \textit{all} possible confidence levels. To address these issues, we
upper bound the coverage difference at all levels using the cumulative density
functions of calibration and test conformal scores and Wasserstein distance.
Inspired by the invariance of physics across data distributions, we propose a
physics-informed structural causal model (PI-SCM) to reduce the upper bound. We
validated that PI-SCM can improve coverage robustness along confidence level
and test domain on a traffic speed prediction task and an epidemic spread task
with multiple real-world datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Empirical investigation of multi-source cross-validation in clinical
  machine learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15012v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15012v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuija Leinonen, David Wong, Ali Wahab, Ramesh Nadarajah, Matti Kaisti, Antti Airola
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditionally, machine learning-based clinical prediction models have been
trained and evaluated on patient data from a single source, such as a hospital.
Cross-validation methods can be used to estimate the accuracy of such models on
new patients originating from the same source, by repeated random splitting of
the data. However, such estimates tend to be highly overoptimistic when
compared to accuracy obtained from deploying models to sources not represented
in the dataset, such as a new hospital. The increasing availability of
multi-source medical datasets provides new opportunities for obtaining more
comprehensive and realistic evaluations of expected accuracy through
source-level cross-validation designs.
  In this study, we present a systematic empirical evaluation of standard
K-fold cross-validation and leave-source-out cross-validation methods in a
multi-source setting. We consider the task of electrocardiogram based
cardiovascular disease classification, combining and harmonizing the openly
available PhysioNet CinC Challenge 2021 and the Shandong Provincial Hospital
datasets for our study.
  Our results show that K-fold cross-validation, both on single-source and
multi-source data, systemically overestimates prediction performance when the
end goal is to generalize to new sources. Leave-source-out cross-validation
provides more reliable performance estimates, having close to zero bias though
larger variability. The evaluation highlights the dangers of obtaining
misleading cross-validation results on medical data and demonstrates how these
issues can be mitigated when having access to multi-source data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contrastive Learning on Multimodal Analysis of Electronic Health Records 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14926v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14926v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianxi Cai, Feiqing Huang, Ryumei Nakada, Linjun Zhang, Doudou Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electronic health record (EHR) systems contain a wealth of multimodal
clinical data including structured data like clinical codes and unstructured
data such as clinical notes. However, many existing EHR-focused studies has
traditionally either concentrated on an individual modality or merged different
modalities in a rather rudimentary fashion. This approach often results in the
perception of structured and unstructured data as separate entities, neglecting
the inherent synergy between them. Specifically, the two important modalities
contain clinically relevant, inextricably linked and complementary health
information. A more complete picture of a patient's medical history is captured
by the joint analysis of the two modalities of data. Despite the great success
of multimodal contrastive learning on vision-language, its potential remains
under-explored in the realm of multimodal EHR, particularly in terms of its
theoretical understanding. To accommodate the statistical analysis of
multimodal EHR data, in this paper, we propose a novel multimodal feature
embedding generative model and design a multimodal contrastive loss to obtain
the multimodal EHR feature representation. Our theoretical analysis
demonstrates the effectiveness of multimodal learning compared to
single-modality learning and connects the solution of the loss function to the
singular value decomposition of a pointwise mutual information matrix. This
connection paves the way for a privacy-preserving algorithm tailored for
multimodal EHR feature representation learning. Simulation studies show that
the proposed algorithm performs well under a variety of configurations. We
further validate the clinical utility of the proposed algorithm in real-world
EHR data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mean-field Analysis on Two-layer Neural Networks from a Kernel
  Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14917v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14917v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shokichi Takakura, Taiji Suzuki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study the feature learning ability of two-layer neural
networks in the mean-field regime through the lens of kernel methods. To focus
on the dynamics of the kernel induced by the first layer, we utilize a
two-timescale limit, where the second layer moves much faster than the first
layer. In this limit, the learning problem is reduced to the minimization
problem over the intrinsic kernel. Then, we show the global convergence of the
mean-field Langevin dynamics and derive time and particle discretization error.
We also demonstrate that two-layer neural networks can learn a union of
multiple reproducing kernel Hilbert spaces more efficiently than any kernel
methods, and neural networks acquire data-dependent kernel which aligns with
the target function. In addition, we develop a label noise procedure, which
converges to the global optimum and show that the degrees of freedom appears as
an implicit regularization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Statistical Inference For Noisy Matrix Completion Incorporating
  Auxiliary Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14899v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14899v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shujie Ma, Po-Yao Niu, Yichong Zhang, Yinchu Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates statistical inference for noisy matrix completion in
a semi-supervised model when auxiliary covariates are available. The model
consists of two parts. One part is a low-rank matrix induced by unobserved
latent factors; the other part models the effects of the observed covariates
through a coefficient matrix which is composed of high-dimensional column
vectors. We model the observational pattern of the responses through a logistic
regression of the covariates, and allow its probability to go to zero as the
sample size increases. We apply an iterative least squares (LS) estimation
approach in our considered context. The iterative LS methods in general enjoy a
low computational cost, but deriving the statistical properties of the
resulting estimators is a challenging task. We show that our method only needs
a few iterations, and the resulting entry-wise estimators of the low-rank
matrix and the coefficient matrix are guaranteed to have asymptotic normal
distributions. As a result, individual inference can be conducted for each
entry of the unknown matrices. We also propose a simultaneous testing procedure
with multiplier bootstrap for the high-dimensional coefficient matrix. This
simultaneous inferential tool can help us further investigate the effects of
covariates for the prediction of missing entries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Thompson Sampling for Stochastic Bandits with Noisy Contexts: An
  Information-Theoretic Regret Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11565v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11565v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sharu Theresa Jose, Shana Moothedath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore a stochastic contextual linear bandit problem where the agent
observes a noisy, corrupted version of the true context through a noise channel
with an unknown noise parameter. Our objective is to design an action policy
that can approximate" that of an oracle, which has access to the reward model,
the channel parameter, and the predictive distribution of the true context from
the observed noisy context. In a Bayesian framework, we introduce a Thompson
sampling algorithm for Gaussian bandits with Gaussian context noise. Adopting
an information-theoretic analysis, we demonstrate the Bayesian regret of our
algorithm concerning the oracle's action policy. We also extend this problem to
a scenario where the agent observes the true context with some delay after
receiving the reward and show that delayed true contexts lead to lower Bayesian
regret. Finally, we empirically demonstrate the performance of the proposed
algorithms against baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentially Private Conditional Independence Testing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06721v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06721v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Iden Kalemaj, Shiva Prasad Kasiviswanathan, Aaditya Ramdas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conditional independence (CI) tests are widely used in statistical data
analysis, e.g., they are the building block of many algorithms for causal graph
discovery. The goal of a CI test is to accept or reject the null hypothesis
that $X \perp \!\!\! \perp Y \mid Z$, where $X \in \mathbb{R}, Y \in
\mathbb{R}, Z \in \mathbb{R}^d$. In this work, we investigate conditional
independence testing under the constraint of differential privacy. We design
two private CI testing procedures: one based on the generalized covariance
measure of Shah and Peters (2020) and another based on the conditional
randomization test of Cand\`es et al. (2016) (under the model-X assumption). We
provide theoretical guarantees on the performance of our tests and validate
them empirically. These are the first private CI tests with rigorous
theoretical guarantees that work for the general case when $Z$ is continuous.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Local Search GFlowNets <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02710v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02710v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minsu Kim, Taeyoung Yun, Emmanuel Bengio, Dinghuai Zhang, <span class="highlight-author">Yoshua Bengio</span>, Sungsoo Ahn, Jinkyoo Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Flow Networks (GFlowNets) are amortized sampling methods that
learn a distribution over discrete objects proportional to their rewards.
GFlowNets exhibit a remarkable ability to generate diverse samples, yet
occasionally struggle to consistently produce samples with high rewards due to
over-exploration on wide sample space. This paper proposes to train GFlowNets
with local search, which focuses on exploiting high-rewarded sample space to
resolve this issue. Our main idea is to explore the local neighborhood via
backtracking and reconstruction guided by backward and forward policies,
respectively. This allows biasing the samples toward high-reward solutions,
which is not possible for a typical GFlowNet solution generation scheme, which
uses the forward policy to generate the solution from scratch. Extensive
experiments demonstrate a remarkable performance improvement in several
biochemical tasks. Source code is available:
\url{https://github.com/dbsxodud-11/ls_gfn}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024 (Spotlight paper), 18 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bootstrapped Training of Score-Conditioned Generator for Offline Design
  of Biological Sequences <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03111v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03111v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minsu Kim, Federico Berto, Sungsoo Ahn, Jinkyoo Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of optimizing biological sequences, e.g., proteins, DNA,
and RNA, to maximize a black-box score function that is only evaluated in an
offline dataset. We propose a novel solution, bootstrapped training of
score-conditioned generator (BootGen) algorithm. Our algorithm repeats a
two-stage process. In the first stage, our algorithm trains the biological
sequence generator with rank-based weights to enhance the accuracy of sequence
generation based on high scores. The subsequent stage involves bootstrapping,
which augments the training dataset with self-generated data labeled by a proxy
score function. Our key idea is to align the score-based generation with a
proxy score function, which distills the knowledge of the proxy score function
to the generator. After training, we aggregate samples from multiple
bootstrapped generators and proxies to produce a diverse design. Extensive
experiments show that our method outperforms competitive baselines on
biological sequential design tasks. We provide reproducible source code:
\href{https://github.com/kaist-silab/bootgen}{https://github.com/kaist-silab/bootgen}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023, 19 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Complexity to Clarity: Analytical Expressions of Deep Neural
  Network Weights via Clifford's Geometric Algebra and Convexity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.16512v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.16512v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mert Pilanci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a novel analysis of neural networks based on
geometric (Clifford) algebra and convex optimization. We show that optimal
weights of deep ReLU neural networks are given by the wedge product of training
samples when trained with standard regularized loss. Furthermore, the training
problem reduces to convex optimization over wedge product features, which
encode the geometric structure of the training dataset. This structure is given
in terms of signed volumes of triangles and parallelotopes generated by data
vectors. The convex problem finds a small subset of samples via $\ell_1$
regularization to discover only relevant wedge product features. Our analysis
provides a novel perspective on the inner workings of deep neural networks and
sheds light on the role of the hidden layers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identifying Linearly-Mixed Causal Representations from Multi-Node
  Interventions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.02695v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.02695v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Bing, Urmi Ninad, Jonas Wahl, Jakob Runge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of inferring high-level causal variables from low-level
observations, commonly referred to as causal representation learning, is
fundamentally underconstrained. As such, recent works to address this problem
focus on various assumptions that lead to identifiability of the underlying
latent causal variables. A large corpus of these preceding approaches consider
multi-environment data collected under different interventions on the causal
model. What is common to virtually all of these works is the restrictive
assumption that in each environment, only a single variable is intervened on.
In this work, we relax this assumption and provide the first identifiability
result for causal representation learning that allows for multiple variables to
be targeted by an intervention within one environment. Our approach hinges on a
general assumption on the coverage and diversity of interventions across
environments, which also includes the shared assumption of single-node
interventions of previous works. The main idea behind our approach is to
exploit the trace that interventions leave on the variance of the ground truth
causal variables and regularizing for a specific notion of sparsity with
respect to this trace. In addition to and inspired by our theoretical
contributions, we present a practical algorithm to learn causal representations
from multi-node interventional data and provide empirical evidence that
validates our identifiability results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at CLeaR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Agnostic View on the Cost of Overfitting in (Kernel) Ridge Regression <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.13185v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.13185v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lijia Zhou, James B. Simon, Gal Vardi, Nathan Srebro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the cost of overfitting in noisy kernel ridge regression (KRR),
which we define as the ratio between the test error of the interpolating
ridgeless model and the test error of the optimally-tuned model. We take an
"agnostic" view in the following sense: we consider the cost as a function of
sample size for any target function, even if the sample size is not large
enough for consistency or the target is outside the RKHS. We analyze the cost
of overfitting under a Gaussian universality ansatz using recently derived
(non-rigorous) risk estimates in terms of the task eigenstructure. Our analysis
provides a more refined characterization of benign, tempered and catastrophic
overfitting (cf. Mallinar et al. 2022).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is the ICLR CR version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Embed Time Series Patches Independently <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.16427v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.16427v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seunghan Lee, Taeyoung Park, Kibok Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Masked time series modeling has recently gained much attention as a
self-supervised representation learning strategy for time series. Inspired by
masked image modeling in computer vision, recent works first patchify and
partially mask out time series, and then train Transformers to capture the
dependencies between patches by predicting masked patches from unmasked
patches. However, we argue that capturing such patch dependencies might not be
an optimal strategy for time series representation learning; rather, learning
to embed patches independently results in better time series representations.
Specifically, we propose to use 1) the simple patch reconstruction task, which
autoencode each patch without looking at other patches, and 2) the simple
patch-wise MLP that embeds each patch independently. In addition, we introduce
complementary contrastive learning to hierarchically capture adjacent time
series information efficiently. Our proposed method improves time series
forecasting and classification performance compared to state-of-the-art
Transformer-based models, while it is more efficient in terms of the number of
parameters and training/inference time. Code is available at this repository:
https://github.com/seunghan96/pits.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Soft Contrastive Learning for Time Series <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.16424v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.16424v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seunghan Lee, Taeyoung Park, Kibok Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning has shown to be effective to learn representations from
time series in a self-supervised way. However, contrasting similar time series
instances or values from adjacent timestamps within a time series leads to
ignore their inherent correlations, which results in deteriorating the quality
of learned representations. To address this issue, we propose SoftCLT, a simple
yet effective soft contrastive learning strategy for time series. This is
achieved by introducing instance-wise and temporal contrastive loss with soft
assignments ranging from zero to one. Specifically, we define soft assignments
for 1) instance-wise contrastive loss by the distance between time series on
the data space, and 2) temporal contrastive loss by the difference of
timestamps. SoftCLT is a plug-and-play method for time series contrastive
learning that improves the quality of learned representations without bells and
whistles. In experiments, we demonstrate that SoftCLT consistently improves the
performance in various downstream tasks including classification,
semi-supervised learning, transfer learning, and anomaly detection, showing
state-of-the-art performance. Code is available at this repository:
https://github.com/seunghan96/softclt.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024 Spotlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EC-NAS: Energy Consumption Aware Tabular Benchmarks for Neural
  Architecture Search <span class="chip">ICASSP-2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.06015v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.06015v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedram Bakhtiarifard, Christian Igel, Raghavendra Selvan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Energy consumption from the selection, training, and deployment of deep
learning models has seen a significant uptick recently. This work aims to
facilitate the design of energy-efficient deep learning models that require
less computational resources and prioritize environmental sustainability by
focusing on the energy consumption. Neural architecture search (NAS) benefits
from tabular benchmarks, which evaluate NAS strategies cost-effectively through
precomputed performance statistics. We advocate for including energy efficiency
as an additional performance criterion in NAS. To this end, we introduce an
enhanced tabular benchmark encompassing data on energy consumption for varied
architectures. The benchmark, designated as EC-NAS, has been made available in
an open-source format to advance research in energy-conscious NAS. EC-NAS
incorporates a surrogate model to predict energy consumption, aiding in
diminishing the energy expenditure of the dataset creation. Our findings
emphasize the potential of EC-NAS by leveraging multi-objective optimization
algorithms, revealing a balance between energy usage and accuracy. This
suggests the feasibility of identifying energy-lean architectures with little
or no compromise in performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to be presented at the International Conference on
  Acoustics, Speech and Signal Processing (ICASSP-2024). Source code at
  https://github.com/saintslab/EC-NAS-Bench</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Benchmark Study on Calibration <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.11838v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.11838v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linwei Tao, Younan Zhu, Haolan Guo, Minjing Dong, Chang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks are increasingly utilized in various machine learning
tasks. However, as these models grow in complexity, they often face calibration
issues, despite enhanced prediction accuracy. Many studies have endeavored to
improve calibration performance through the use of specific loss functions,
data preprocessing and training frameworks. Yet, investigations into
calibration properties have been somewhat overlooked. Our study leverages the
Neural Architecture Search (NAS) search space, offering an exhaustive model
architecture space for thorough calibration properties exploration. We
specifically create a model calibration dataset. This dataset evaluates 90
bin-based and 12 additional calibration measurements across 117,702 unique
neural networks within the widely employed NATS-Bench search space. Our
analysis aims to answer several longstanding questions in the field, using our
proposed dataset: (i) Can model calibration be generalized across different
datasets? (ii) Can robustness be used as a calibration measurement? (iii) How
reliable are calibration metrics? (iv) Does a post-hoc calibration method
affect all models uniformly? (v) How does calibration interact with accuracy?
(vi) What is the impact of bin size on calibration measurement? (vii) Which
architectural designs are beneficial for calibration? Additionally, our study
bridges an existing gap by exploring calibration within NAS. By providing this
dataset, we enable further research into NAS calibration. As far as we are
aware, our research represents the first large-scale investigation into
calibration properties and the premier study of calibration issues within NAS.
The project page can be found at https://www.taolinwei.com/calibration-study
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024 poster</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Importance <span class="highlight-title">Sample</span> in Primary <span class="highlight-title">Sample</span> Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1808.07840v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1808.07840v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quan Zheng, Matthias Zwicker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Importance sampling is one of the most widely used variance reduction
strategies in Monte Carlo rendering. In this paper, we propose a novel
importance sampling technique that uses a neural network to learn how to sample
from a desired density represented by a set of samples. Our approach considers
an existing Monte Carlo rendering algorithm as a black box. During a
scene-dependent training phase, we learn to generate samples with a desired
density in the primary sample space of the rendering algorithm using maximum
likelihood estimation. We leverage a recent neural network architecture that
was designed to represent real-valued non-volume preserving ('Real NVP')
transformations in high dimensional spaces. We use Real NVP to non-linearly
warp primary sample space and obtain desired densities. In addition, Real NVP
efficiently computes the determinant of the Jacobian of the warp, which is
required to implement the change of integration variables implied by the warp.
A main advantage of our approach is that it is agnostic of underlying light
transport effects, and can be combined with many existing rendering techniques
by treating them as a black box. We show that our approach leads to effective
variance reduction in several practical scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 14 figure; authors' version, the definitive version of
  record is available at https://onlinelibrary.wiley.com/doi/10.1111/cgf.13628</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Statistical Agnostic Regression: a machine learning method to validate
  regression models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15213v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15213v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan M Gorriz, J. Ramirez, F. Segovia, F. J. Martinez-Murcia, C. Jiménez-Mesa, J. Suckling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Regression analysis is a central topic in statistical modeling, aiming to
estimate the relationships between a dependent variable, commonly referred to
as the response variable, and one or more independent variables, i.e.,
explanatory variables. Linear regression is by far the most popular method for
performing this task in several fields of research, such as prediction,
forecasting, or causal inference. Beyond various classical methods to solve
linear regression problems, such as Ordinary Least Squares, Ridge, or Lasso
regressions - which are often the foundation for more advanced machine learning
(ML) techniques - the latter have been successfully applied in this scenario
without a formal definition of statistical significance. At most, permutation
or classical analyses based on empirical measures (e.g., residuals or accuracy)
have been conducted to reflect the greater ability of ML estimations for
detection. In this paper, we introduce a method, named Statistical Agnostic
Regression (SAR), for evaluating the statistical significance of an ML-based
linear regression based on concentration inequalities of the actual risk using
the analysis of the worst case. To achieve this goal, similar to the
classification problem, we define a threshold to establish that there is
sufficient evidence with a probability of at least 1-eta to conclude that there
is a linear relationship in the population between the explanatory (feature)
and the response (label) variables. Simulations in only two dimensions
demonstrate the ability of the proposed agnostic test to provide a similar
analysis of variance given by the classical $F$ test for the slope parameter.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CARE: Large Precision Matrix Estimation for Compositional Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.06985v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.06985v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shucong Zhang, Huiyuan Wang, Wei Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-dimensional compositional data are prevalent in many applications. The
simplex constraint poses intrinsic challenges to inferring the conditional
dependence relationships among the components forming a composition, as encoded
by a large precision matrix. We introduce a precise specification of the
compositional precision matrix and relate it to its basis counterpart, which is
shown to be asymptotically identifiable under suitable sparsity assumptions. By
exploiting this connection, we propose a composition adaptive regularized
estimation (CARE) method for estimating the sparse basis precision matrix. We
derive rates of convergence for the estimator and provide theoretical
guarantees on support recovery and data-driven parameter tuning. Our theory
reveals an intriguing trade-off between identification and estimation, thereby
highlighting the blessing of dimensionality in compositional data analysis. In
particular, in sufficiently high dimensions, the CARE estimator achieves
minimax optimality and performs as well as if the basis were observed. We
further discuss how our framework can be extended to handle data containing
zeros, including sampling zeros and structural zeros. The advantages of CARE
over existing methods are illustrated by simulation studies and an application
to inferring microbial ecological networks in the human gut.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>67 pages, 7 figures, to appear in Journal of the American Statistical
  Association (http://www.tandfonline.com/r/JASA)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Symmetry Breaking and Equivariant Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.09016v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.09016v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sékou-Oumar Kaba, Siamak Ravanbakhsh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Using symmetry as an inductive bias in deep learning has been proven to be a
principled approach for sample-efficient model design. However, the
relationship between symmetry and the imperative for equivariance in neural
networks is not always obvious. Here, we analyze a key limitation that arises
in equivariant functions: their incapacity to break symmetry at the level of
individual data samples. In response, we introduce a novel notion of 'relaxed
equivariance' that circumvents this limitation. We further demonstrate how to
incorporate this relaxation into equivariant multilayer perceptrons (E-MLPs),
offering an alternative to the noise-injection method. The relevance of
symmetry breaking is then discussed in various application domains: physics,
graph representation learning, combinatorial optimization and equivariant
decoding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 2 figures, Symmetry and Geometry in Neural Representations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dual Accuracy-Quality-Driven Neural Network for Prediction Interval
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06370v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06370v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giorgio Morales, John W. Sheppard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate uncertainty quantification is necessary to enhance the reliability
of deep learning models in real-world applications. In the case of regression
tasks, prediction intervals (PIs) should be provided along with the
deterministic predictions of deep learning models. Such PIs are useful or
"high-quality" as long as they are sufficiently narrow and capture most of the
probability density. In this paper, we present a method to learn prediction
intervals for regression-based neural networks automatically in addition to the
conventional target predictions. In particular, we train two companion neural
networks: one that uses one output, the target estimate, and another that uses
two outputs, the upper and lower bounds of the corresponding PI. Our main
contribution is the design of a novel loss function for the PI-generation
network that takes into account the output of the target-estimation network and
has two optimization objectives: minimizing the mean prediction interval width
and ensuring the PI integrity using constraints that maximize the prediction
interval probability coverage implicitly. Furthermore, we introduce a
self-adaptive coefficient that balances both objectives within the loss
function, which alleviates the task of fine-tuning. Experiments using a
synthetic dataset, eight benchmark datasets, and a real-world crop yield
prediction dataset showed that our method was able to maintain a nominal
probability coverage and produce significantly narrower PIs without detriment
to its target estimation accuracy when compared to those PIs generated by three
state-of-the-art neural-network-based methods. In other words, our method was
shown to produce higher-quality PIs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the IEEE Transactions on Neural Networks and Learning
  Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ricci flow-guided autoencoders in learning time-dependent dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.14591v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.14591v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Gracyk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a manifold-based autoencoder method for learning nonlinear
dynamics in time, notably partial differential equations (PDEs), in which the
manifold latent space evolves according to Ricci flow. This can be accomplished
by simulating Ricci flow in a physics-informed setting, and manifold quantities
can be matched so that Ricci flow is empirically achieved. With our
methodology, the manifold is learned as part of the training procedure, so
ideal geometries may be discerned, while the evolution simultaneously induces a
more accommodating latent representation over static methods. We present our
method on a range of numerical experiments consisting of PDEs that encompass
desirable characteristics such as periodicity and randomness, remarking error
on in-distribution and extrapolation scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multiscale Hodge Scattering Networks for Data Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10270v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10270v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naoki Saito, Stefan C. Schonsheck, Eugene Shvarts
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose new scattering networks for signals measured on simplicial
complexes, which we call \emph{Multiscale Hodge Scattering Networks} (MHSNs).
Our construction is based on multiscale basis dictionaries on simplicial
complexes, i.e., the $\kappa$-GHWT and $\kappa$-HGLET, which we recently
developed for simplices of dimension $\kappa \in \mathbb{N}$ in a given
simplicial complex by generalizing the node-based Generalized Haar-Walsh
Transform (GHWT) and Hierarchical Graph Laplacian Eigen Transform (HGLET). The
$\kappa$-GHWT and the $\kappa$-HGLET both form redundant sets (i.e.,
dictionaries) of multiscale basis vectors and the corresponding expansion
coefficients of a given signal. Our MHSNs use a layered structure analogous to
a convolutional neural network (CNN) to cascade the moments of the modulus of
the dictionary coefficients. The resulting features are invariant to reordering
of the simplices (i.e., node permutation of the underlying graphs).
Importantly, the use of multiscale basis dictionaries in our MHSNs admits a
natural pooling operation that is akin to local pooling in CNNs, and which may
be performed either locally or per-scale. These pooling operations are harder
to define in both traditional scattering networks based on Morlet wavelets, and
geometric scattering networks based on Diffusion Wavelets. As a result, we are
able to extract a rich set of descriptive yet robust features that can be used
along with very simple machine learning methods (i.e., logistic regression or
support vector machines) to achieve high-accuracy classification systems with
far fewer parameters to train than most modern graph neural networks. Finally,
we demonstrate the usefulness of our MHSNs in three distinct types of problems:
signal classification, domain (i.e., graph/simplex) classification, and
molecular dynamics prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 Pages, Comments Welcome</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-21T00:00:00Z">2024-03-21</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computer Science and Game Theory
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Language Models Can Reduce Asymmetry in Information Markets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nasim Rahaman, Martin Weiss, Manuel Wüthrich, <span class="highlight-author">Yoshua Bengio</span>, Li Erran Li, Chris Pal, Bernhard Schölkopf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work addresses the buyer's inspection paradox for information markets.
The paradox is that buyers need to access information to determine its value,
while sellers need to limit access to prevent theft. To study this, we
introduce an open-source simulated digital marketplace where intelligent
agents, powered by language models, buy and sell information on behalf of
external participants. The central mechanism enabling this marketplace is the
agents' dual capabilities: they not only have the capacity to assess the
quality of privileged information but also come equipped with the ability to
forget. This ability to induce amnesia allows vendors to grant temporary access
to proprietary information, significantly reducing the risk of unauthorized
retention while enabling agents to accurately gauge the information's relevance
to specific queries or tasks. To perform well, agents must make rational
decisions, strategically explore the marketplace through generated sub-queries,
and synthesize answers from purchased information. Concretely, our experiments
(a) uncover biases in language models leading to irrational behavior and
evaluate techniques to mitigate these biases, (b) investigate how price affects
demand in the context of informational goods, and (c) show that inspection and
higher budgets both lead to higher quality outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Player Zero-Sum Markov <span class="highlight-title">Game</span>s with Networked Separable Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.09470v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.09470v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chanwoo Park, Kaiqing Zhang, Asuman Ozdaglar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study a new class of Markov games, \emph(multi-player) zero-sum Markov
Games} with \emph{Networked separable interactions} (zero-sum NMGs), to model
the local interaction structure in non-cooperative multi-agent sequential
decision-making. We define a zero-sum NMG as a model where {the payoffs of the
auxiliary games associated with each state are zero-sum and} have some
separable (i.e., polymatrix) structure across the neighbors over some
interaction network. We first identify the necessary and sufficient conditions
under which an MG can be presented as a zero-sum NMG, and show that the set of
Markov coarse correlated equilibrium (CCE) collapses to the set of Markov Nash
equilibrium (NE) in these games, in that the product of per-state
marginalization of the former for all players yields the latter. Furthermore,
we show that finding approximate Markov \emph{stationary} CCE in
infinite-horizon discounted zero-sum NMGs is \texttt{PPAD}-hard, unless the
underlying network has a ``star topology''. Then, we propose
fictitious-play-type dynamics, the classical learning dynamics in normal-form
games, for zero-sum NMGs, and establish convergence guarantees to Markov
stationary NE under a star-shaped network structure. Finally, in light of the
hardness result, we focus on computing a Markov \emph{non-stationary} NE and
provide finite-iteration guarantees for a series of value-iteration-based
algorithms. We also provide numerical experiments to corroborate our
theoretical results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Emergent Dominance Hierarchies in <span class="highlight-title">Reinforcement</span> Learning <span class="highlight-title">Agent</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.12258v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.12258v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ram Rachum, Yonatan Nakar, Bill Tomlinson, Nitay Alon, Reuth Mirsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern Reinforcement Learning (RL) algorithms are able to outperform humans
in a wide variety of tasks. Multi-agent reinforcement learning (MARL) settings
present additional challenges, and successful cooperation in mixed-motive
groups of agents depends on a delicate balancing act between individual and
group objectives. Social conventions and norms, often inspired by human
institutions, are used as tools for striking this balance.
  In this paper, we examine a fundamental, well-studied social convention that
underlies cooperation in both animal and human societies: dominance
hierarchies.
  We adapt the ethological theory of dominance hierarchies to artificial
agents, borrowing the established terminology and definitions with as few
amendments as possible. We demonstrate that populations of RL agents, operating
without explicit programming or intrinsic rewards, can invent, learn, enforce,
and transmit a dominance hierarchy to new populations. The dominance
hierarchies that emerge have a similar structure to those studied in chickens,
mice, fish, and other species.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Designing Digital Voting Systems for Citizens: Achieving Fairness and
  Legitimacy in Participatory Budgeting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03501v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03501v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua C. Yang, Carina I. Hausladen, Dominik Peters, Evangelos Pournaras, Regula Hänggli Fricker, Dirk Helbing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Participatory Budgeting (PB) has evolved into a key democratic instrument for
resource allocation in cities. Enabled by digital platforms, cities now have
the opportunity to let citizens directly propose and vote on urban projects,
using different voting input and aggregation rules. However, the choices cities
make in terms of the rules of their PB have often not been informed by academic
studies on voter behaviour and preferences. Therefore, this work presents the
results of behavioural experiments where participants were asked to vote in a
fictional PB setting. We identified approaches to designing PB voting that
minimise cognitive load and enhance the perceived fairness and legitimacy of
the digital process from the citizens' perspective. In our study, participants
preferred voting input formats that are more expressive (like rankings and
distributing points) over simpler formats (like approval voting). Participants
also indicated a desire for the budget to be fairly distributed across city
districts and project categories. Participants found the Method of Equal Shares
voting rule to be fairer than the conventional Greedy voting rule. These
findings offer actionable insights for digital governance, contributing to the
development of fairer and more transparent digital systems and collective
decision-making processes for citizens.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review in ACM Digital Government: Research and Practice</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Role of Transparency in Repeated First-Price Auctions with Unknown
  Valuations <span class="chip">STOC 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.09478v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.09478v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolò Cesa-Bianchi, Tommaso Cesari, Roberto Colomboni, Federico Fusco, Stefano Leonardi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of regret minimization for a single bidder in a sequence
of first-price auctions where the bidder discovers the item's value only if the
auction is won. Our main contribution is a complete characterization, up to
logarithmic factors, of the minimax regret in terms of the auction's
\emph{transparency}, which controls the amount of information on competing bids
disclosed by the auctioneer at the end of each auction. Our results hold under
different assumptions (stochastic, adversarial, and their smoothed variants) on
the environment generating the bidder's valuations and competing bids. These
minimax rates reveal how the interplay between transparency and the nature of
the environment affects how fast one can learn to bid optimally in first-price
auctions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at STOC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Resolving social dilemmas with minimal reward transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12928v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12928v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard Willis, Yali Du, Joel Z Leibo, Michael Luck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-agent cooperation is an important topic, and is particularly
challenging in mixed-motive situations where it does not pay to be nice to
others. Consequently, self-interested agents often avoid collective behaviour,
resulting in suboptimal outcomes for the group. In response, in this paper we
introduce a metric to quantify the disparity between what is rational for
individual agents and what is rational for the group, which we call the general
self-interest level. This metric represents the maximum proportion of
individual rewards that all agents can retain while ensuring that achieving
social welfare optimum becomes a dominant strategy. By aligning the individual
and group incentives, rational agents acting to maximise their own reward will
simultaneously maximise the collective reward. As agents transfer their rewards
to motivate others to consider their welfare, we diverge from traditional
concepts of altruism or prosocial behaviours. The general self-interest level
is a property of a game that is useful for assessing the propensity of players
to cooperate and understanding how features of a game impact this. We
illustrate the effectiveness of our method on several novel games
representations of social dilemmas with arbitrary numbers of players.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 13 tables, submitted to the Journal of Autonomous Agents
  and Multi-Agent Systems: Special Issue on Citizen-Centric AI Systems</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Analysis of the Preferences of Distribution Indicators in
  Evolutionary Multi-Objective Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14838v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14838v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jesús Guillermo Falcón-Cardona, Mahboubeh Nezhadmoghaddam, Emilio Bernal-Zubieta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The distribution of objective vectors in a Pareto Front Approximation (PFA)
is crucial for representing the associated manifold accurately. Distribution
Indicators (DIs) assess the distribution of a PFA numerically, utilizing
concepts like distance calculation, Biodiversity, Entropy, Potential Energy, or
Clustering. Despite the diversity of DIs, their strengths and weaknesses across
assessment scenarios are not well-understood. This paper introduces a taxonomy
for classifying DIs, followed by a preference analysis of nine DIs, each
representing a category in the taxonomy. Experimental results, considering
various PFAs under controlled scenarios (loss of coverage, loss of uniformity,
pathological distributions), reveal that some DIs can be misleading and need
cautious use. Additionally, DIs based on Biodiversity and Potential Energy show
promise for PFA evaluation and comparison of Multi-Objective Evolutionary
Algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HyperGALE: ASD Classification via Hypergraph Gated Attention with
  Learnable Hyperedges <span class="chip">IJCNN 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14484v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14484v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehul Arora, Chirag Shantilal Jain, Lalith Bharadwaj Baru, Kamalaker Dadi, Bapi Raju Surampudi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autism Spectrum Disorder (ASD) is a neurodevelopmental condition
characterized by varied social cognitive challenges and repetitive behavioral
patterns. Identifying reliable brain imaging-based biomarkers for ASD has been
a persistent challenge due to the spectrum's diverse symptomatology. Existing
baselines in the field have made significant strides in this direction, yet
there remains room for improvement in both performance and interpretability. We
propose \emph{HyperGALE}, which builds upon the hypergraph by incorporating
learned hyperedges and gated attention mechanisms. This approach has led to
substantial improvements in the model's ability to interpret complex brain
graph data, offering deeper insights into ASD biomarker characterization.
Evaluated on the extensive ABIDE II dataset, \emph{HyperGALE} not only improves
interpretability but also demonstrates statistically significant enhancements
in key performance metrics compared to both previous baselines and the
foundational hypergraph model. The advancement \emph{HyperGALE} brings to ASD
research highlights the potential of sophisticated graph-based techniques in
neurodevelopmental studies. The source code and implementation instructions are
available at GitHub:https://github.com/mehular0ra/HyperGALE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IJCNN 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">reinforcement</span> learning guided hybrid evolutionary algorithm for the
  latency location routing problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14405v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14405v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuji Zou, Jin-Kao Hao, Qinghua Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The latency location routing problem integrates the facility location problem
and the multi-depot cumulative capacitated vehicle routing problem. This
problem involves making simultaneous decisions about depot locations and
vehicle routes to serve customers while aiming to minimize the sum of waiting
(arriving) times for all customers. To address this computationally challenging
problem, we propose a reinforcement learning guided hybrid evolutionary
algorithm following the framework of the memetic algorithm. The proposed
algorithm relies on a diversity-enhanced multi-parent edge assembly crossover
to build promising offspring and a reinforcement learning guided variable
neighborhood descent to determine the exploration order of multiple
neighborhoods. Additionally, strategic oscillation is used to achieve a
balanced exploration of both feasible and infeasible solutions. The
competitiveness of the algorithm against state-of-the-art methods is
demonstrated by experimental results on the three sets of 76 popular instances,
including 51 improved best solutions (new upper bounds) for the 59 instances
with unknown optima and equal best results for the remaining instances. We also
conduct additional experiments to shed light on the key components of the
algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SpikingResformer: Bridging ResNet and Vision Transformer in Spiking
  Neural Networks <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14302v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14302v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Shi, Zecheng Hao, Zhaofei Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The remarkable success of Vision Transformers in Artificial Neural Networks
(ANNs) has led to a growing interest in incorporating the self-attention
mechanism and transformer-based architecture into Spiking Neural Networks
(SNNs). While existing methods propose spiking self-attention mechanisms that
are compatible with SNNs, they lack reasonable scaling methods, and the overall
architectures proposed by these methods suffer from a bottleneck in effectively
extracting local features. To address these challenges, we propose a novel
spiking self-attention mechanism named Dual Spike Self-Attention (DSSA) with a
reasonable scaling method. Based on DSSA, we propose a novel spiking Vision
Transformer architecture called SpikingResformer, which combines the
ResNet-based multi-stage architecture with our proposed DSSA to improve both
performance and energy efficiency while reducing parameters. Experimental
results show that SpikingResformer achieves higher accuracy with fewer
parameters and lower energy consumption than other spiking Vision Transformer
counterparts. Notably, our SpikingResformer-L achieves 79.40% top-1 accuracy on
ImageNet with 4 time-steps, which is the state-of-the-art result in the SNN
field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in the 2024 IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reactor Optimization Benchmark by <span class="highlight-title">Reinforcement</span> Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14273v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14273v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deborah Schwarcz, Nadav Schneider, Gal Oren, Uri Steinitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neutronic calculations for reactors are a daunting task when using Monte
Carlo (MC) methods. As high-performance computing has advanced, the simulation
of a reactor is nowadays more readily done, but design and optimization with
multiple parameters is still a computational challenge. MC transport
simulations, coupled with machine learning techniques, offer promising avenues
for enhancing the efficiency and effectiveness of nuclear reactor optimization.
This paper introduces a novel benchmark problem within the OpenNeoMC framework
designed specifically for reinforcement learning. The benchmark involves
optimizing a unit cell of a research reactor with two varying parameters (fuel
density and water spacing) to maximize neutron flux while maintaining reactor
criticality. The test case features distinct local optima, representing
different physical regimes, thus posing a challenge for learning algorithms.
Through extensive simulations utilizing evolutionary and neuroevolutionary
algorithms, we demonstrate the effectiveness of reinforcement learning in
navigating complex optimization landscapes with strict constraints.
Furthermore, we propose acceleration techniques within the OpenNeoMC framework,
including model updating and cross-section usage by RAM utilization, to
expedite simulation times. Our findings emphasize the importance of machine
learning integration in reactor optimization and contribute to advancing
methodologies for addressing intricate optimization challenges in nuclear
engineering. The sources of this work are available at our GitHub repository:
https://github.com/Scientific-Computing-Lab-NRCN/RLOpenNeoMC
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stitching for Neuroevolution: Recombining Deep Neural Networks without
  Breaking Them <span class="chip">GECCO 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14224v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14224v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arthur Guijt, Dirk Thierens, Tanja Alderliesten, Peter A. N. Bosman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional approaches to neuroevolution often start from scratch. This
becomes prohibitively expensive in terms of computational and data requirements
when targeting modern, deep neural networks. Using a warm start could be highly
advantageous, e.g., using previously trained networks, potentially from
different sources. This moreover enables leveraging the benefits of transfer
learning (in particular vastly reduced training effort). However, recombining
trained networks is non-trivial because architectures and feature
representations typically differ. Consequently, a straightforward exchange of
layers tends to lead to a performance breakdown. We overcome this by matching
the layers of parent networks based on their connectivity, identifying
potential crossover points. To correct for differing feature representations
between these layers we employ stitching, which merges the networks by
introducing new layers at crossover points. To train the merged network, only
stitching layers need to be considered. New networks can then be created by
selecting a subnetwork by choosing which stitching layers to (not) use.
Assessing their performance is efficient as only their evaluation on data is
required. We experimentally show that our approach enables finding networks
that represent novel trade-offs between performance and computational cost,
with some even dominating the original networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, submitted to GECCO 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evolving Benchmark Functions to Compare Evolutionary Algorithms via
  Genetic Programming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14146v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14146v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan He, Claus Aranha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we use Genetic Programming (GP) to compose new optimization
benchmark functions. Optimization benchmarks have the important role of showing
the differences between evolutionary algorithms, making it possible for further
analysis and comparisons. We show that the benchmarks generated by GP are able
to differentiate algorithms better than human-made benchmark functions. The
fitness measure of the GP is the Wasserstein distance of the solutions found by
a pair of optimizers. Additionally, we use MAP-Elites to both enhance the
search power of the GP and also illustrate how the difference between
optimizers changes by various landscape features. Our approach provides a novel
way to automate the design of benchmark functions and to compare evolutionary
algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Genetic Programming for Explainable Manifold Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14139v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14139v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ben Cravens, Andrew Lensen, Paula Maddigan, Bing Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Manifold learning techniques play a pivotal role in machine learning by
revealing lower-dimensional embeddings within high-dimensional data, thus
enhancing both the efficiency and interpretability of data analysis by
transforming the data into a lower-dimensional representation. However, a
notable challenge with current manifold learning methods is their lack of
explicit functional mappings, crucial for explainability in many real-world
applications. Genetic programming, known for its interpretable functional
tree-based models, has emerged as a promising approach to address this
challenge. Previous research leveraged multi-objective GP to balance manifold
quality against embedding dimensionality, producing functional mappings across
a range of embedding sizes. Yet, these mapping trees often became complex,
hindering explainability. In response, in this paper, we introduce Genetic
Programming for Explainable Manifold Learning (GP-EMaL), a novel approach that
directly penalises tree complexity. Our new method is able to maintain high
manifold quality while significantly enhancing explainability and also allows
customisation of complexity measures, such as symmetry balancing, scaling, and
node complexity, catering to diverse application needs. Our experimental
analysis demonstrates that GP-EMaL is able to match the performance of the
existing approach in most cases, while using simpler, smaller, and more
interpretable tree structures. This advancement marks a significant step
towards achieving interpretable manifold learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SpikeGraphormer: A High-Performance Graph Transformer with Spiking Graph
  Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15480v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15480v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yundong Sun, Dongjie Zhu, Yansong Wang, Zhaoshuo Tian, Ning Cao, Gregory O'Hared
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Graph Transformers have emerged as a promising solution to
alleviate the inherent limitations of Graph Neural Networks (GNNs) and enhance
graph representation performance. Unfortunately, Graph Transformers are
computationally expensive due to the quadratic complexity inherent in
self-attention when applied over large-scale graphs, especially for node tasks.
In contrast, spiking neural networks (SNNs), with event-driven and binary
spikes properties, can perform energy-efficient computation. In this work, we
propose a novel insight into integrating SNNs with Graph Transformers and
design a Spiking Graph Attention (SGA) module. The matrix multiplication is
replaced by sparse addition and mask operations. The linear complexity enables
all-pair node interactions on large-scale graphs with limited GPU memory. To
our knowledge, our work is the first attempt to introduce SNNs into Graph
Transformers. Furthermore, we design SpikeGraphormer, a Dual-branch
architecture, combining a sparse GNN branch with our SGA-driven Graph
Transformer branch, which can simultaneously perform all-pair node interactions
and capture local neighborhoods. SpikeGraphormer consistently outperforms
existing state-of-the-art approaches across various datasets and makes
substantial improvements in training time, inference time, and GPU memory cost
(10 ~ 20x lower than vanilla self-attention). It also performs well in
cross-domain applications (image and text classification). We release our code
at https://github.com/PHD-lanyu/SpikeGraphormer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Tempered to Benign Overfitting in ReLU Neural Networks <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.15141v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.15141v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guy Kornowski, Gilad Yehudai, Ohad Shamir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Overparameterized neural networks (NNs) are observed to generalize well even
when trained to perfectly fit noisy data. This phenomenon motivated a large
body of work on "benign overfitting", where interpolating predictors achieve
near-optimal performance. Recently, it was conjectured and empirically observed
that the behavior of NNs is often better described as "tempered overfitting",
where the performance is non-optimal yet also non-trivial, and degrades as a
function of the noise level. However, a theoretical justification of this claim
for non-linear NNs has been lacking so far. In this work, we provide several
results that aim at bridging these complementing views. We study a simple
classification setting with 2-layer ReLU NNs, and prove that under various
assumptions, the type of overfitting transitions from tempered in the extreme
case of one-dimensional data, to benign in high dimensions. Thus, we show that
the input dimension has a crucial role on the type of overfitting in this
setting, which we also validate empirically for intermediate dimensions.
Overall, our results shed light on the intricate connections between the
dimension, sample size, architecture and training algorithm on the one hand,
and the type of resulting overfitting on the other hand.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023; fixed bug</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalized Early Stopping in Evolutionary Direct <span class="highlight-title">Policy</span> Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.03574v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.03574v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Etor Arza, Leni K. Le Goff, Emma Hart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lengthy evaluation times are common in many optimization problems such as
direct policy search tasks, especially when they involve conducting evaluations
in the physical world, e.g. in robotics applications. Often when evaluating
solution over a fixed time period it becomes clear that the objective value
will not increase with additional computation time (for example when a two
wheeled robot continuously spins on the spot). In such cases, it makes sense to
stop the evaluation early to save computation time. However, most approaches to
stop the evaluation are problem specific and need to be specifically designed
for the task at hand. Therefore, we propose an early stopping method for direct
policy search. The proposed method only looks at the objective value at each
time step and requires no problem specific knowledge. We test the introduced
stopping criterion in five direct policy search environments drawn from games,
robotics and classic control domains, and show that it can save up to 75% of
the computation time. We also compare it with problem specific stopping
criteria and show that it performs comparably, while being more generally
applicable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PyVRP: a high-performance VRP solver package 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13795v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13795v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niels A. Wouda, Leon Lan, Wouter Kool
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce PyVRP, a Python package that implements hybrid genetic search in
a state-of-the-art vehicle routing problem (VRP) solver. The package is
designed for the VRP with time windows (VRPTW), but can be easily extended to
support other VRP variants. PyVRP combines the flexibility of Python with the
performance of C++, by implementing (only) performance critical parts of the
algorithm in C++, while being fully customisable at the Python level. PyVRP is
a polished implementation of the algorithm that ranked 1st in the 2021 DIMACS
VRPTW challenge and, after improvements, ranked 1st on the static variant of
the EURO meets NeurIPS 2022 vehicle routing competition. The code follows good
software engineering practices, and is well-documented and unit tested. PyVRP
is freely available under the liberal MIT license. Through numerical
experiments we show that PyVRP achieves state-of-the-art results on the VRPTW
and capacitated VRP. We hope that PyVRP enables researchers and practitioners
to easily and quickly build on a state-of-the-art VRP solver.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-print of accepted paper in INFORMS Journal on Computing. 24
  pages, 1 figure, 2 listings</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Clustering Evaluation: How to Validate Internal Clustering
  Validation Measures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14830v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14830v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeya Wang, Chenglong Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep clustering, a method for partitioning complex, high-dimensional data
using deep neural networks, presents unique evaluation challenges. Traditional
clustering validation measures, designed for low-dimensional spaces, are
problematic for deep clustering, which involves projecting data into
lower-dimensional embeddings before partitioning. Two key issues are
identified: 1) the curse of dimensionality when applying these measures to raw
data, and 2) the unreliable comparison of clustering results across different
embedding spaces stemming from variations in training procedures and parameter
settings in different clustering models. This paper addresses these challenges
in evaluating clustering quality in deep learning. We present a theoretical
framework to highlight ineffectiveness arising from using internal validation
measures on raw and embedded data and propose a systematic approach to applying
clustering validity indices in deep clustering contexts. Experiments show that
this framework aligns better with external validation measures, effectively
reducing the misguidance from the improper use of clustering validity indices
in deep learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hyperbolic Secant representation of the logistic function: Application
  to probabilistic Multiple Instance Learning for CT intracranial hemorrhage
  detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14829v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14829v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        F. M. Castro-Macías, P. Morales-Álvarez, Y. Wu, R. Molina, A. K. Katsaggelos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiple Instance Learning (MIL) is a weakly supervised paradigm that has
been successfully applied to many different scientific areas and is
particularly well suited to medical imaging. Probabilistic MIL methods, and
more specifically Gaussian Processes (GPs), have achieved excellent results due
to their high expressiveness and uncertainty quantification capabilities. One
of the most successful GP-based MIL methods, VGPMIL, resorts to a variational
bound to handle the intractability of the logistic function. Here, we formulate
VGPMIL using P\'olya-Gamma random variables. This approach yields the same
variational posterior approximations as the original VGPMIL, which is a
consequence of the two representations that the Hyperbolic Secant distribution
admits. This leads us to propose a general GP-based MIL method that takes
different forms by simply leveraging distributions other than the Hyperbolic
Secant one. Using the Gamma distribution we arrive at a new approach that
obtains competitive or superior predictive performance and efficiency. This is
validated in a comprehensive experimental study including one synthetic MIL
dataset, two well-known MIL benchmarks, and a real-world medical problem. We
expect that this work provides useful ideas beyond MIL that can foster further
research in the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>48 pages, 12 figures, published in Artificial Intelligence Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Non-Convex Robust Hypothesis Testing using Sinkhorn Uncertainty Sets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14822v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14822v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Wang, Rui Gao, Yao Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a new framework to address the non-convex robust hypothesis
testing problem, wherein the goal is to seek the optimal detector that
minimizes the maximum of worst-case type-I and type-II risk functions. The
distributional uncertainty sets are constructed to center around the empirical
distribution derived from samples based on Sinkhorn discrepancy. Given that the
objective involves non-convex, non-smooth probabilistic functions that are
often intractable to optimize, existing methods resort to approximations rather
than exact solutions. To tackle the challenge, we introduce an exact
mixed-integer exponential conic reformulation of the problem, which can be
solved into a global optimum with a moderate amount of input data.
Subsequently, we propose a convex approximation, demonstrating its superiority
over current state-of-the-art methodologies in literature. Furthermore, we
establish connections between robust hypothesis testing and regularized
formulations of non-robust risk functions, offering insightful interpretations.
Our numerical study highlights the satisfactory testing performance and
computational efficiency of the proposed framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Curvature Augmented Manifold Embedding and Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14813v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14813v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongming Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A new dimensional reduction (DR) and data visualization method,
Curvature-Augmented Manifold Embedding and Learning (CAMEL), is proposed. The
key novel contribution is to formulate the DR problem as a mechanistic/physics
model, where the force field among nodes (data points) is used to find an
n-dimensional manifold representation of the data sets. Compared with many
existing attractive-repulsive force-based methods, one unique contribution of
the proposed method is to include a non-pairwise force. A new force field model
is introduced and discussed, inspired by the multi-body potential in
lattice-particle physics and Riemann curvature in topology. A
curvature-augmented force is included in CAMEL. Following this, CAMEL
formulation for unsupervised learning, supervised learning, semi-supervised
learning/metric learning, and inverse learning are provided. Next, CAMEL is
applied to many benchmark datasets by comparing existing models, such as tSNE,
UMAP, TRIMAP, and PacMap. Both visual comparison and metrics-based evaluation
are performed. 14 open literature and self-proposed metrics are employed for a
comprehensive comparison. Conclusions and future work are suggested based on
the current investigation. Related code and demonstration are available on
https://github.com/ymlasu/CAMEL for interested readers to reproduce the results
and other applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Adversarial Inverse <span class="highlight-title">Reinforcement</span> Learning: From the Angles
  of <span class="highlight-title">Policy</span> Imitation and Transferable Reward Recovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14593v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14593v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangchun Zhang, Yirui Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial inverse reinforcement learning (AIRL) stands as a cornerstone
approach in imitation learning. This paper rethinks the two different angles of
AIRL: policy imitation and transferable reward recovery. We begin with
substituting the built-in algorithm in AIRL with soft actor-critic (SAC) during
the policy optimization process to enhance sample efficiency, thanks to the
off-policy formulation of SAC and identifiable Markov decision process (MDP)
models with respect to AIRL. It indeed exhibits a significant improvement in
policy imitation but accidentally brings drawbacks to transferable reward
recovery. To learn this issue, we illustrate that the SAC algorithm itself is
not feasible to disentangle the reward function comprehensively during the AIRL
training process, and propose a hybrid framework, PPO-AIRL + SAC, for
satisfactory transfer effect. Additionally, we analyze the capability of
environments to extract disentangled rewards from an algebraic theory
perspective.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Transfer Learning Causal Approach to Evaluate Racial/Ethnic and
  Geographic Variation in Outcomes Following Congenital Heart Surgery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14573v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14573v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Larry Han, Yi Zhang, Meena Nathan, John E. Mayer, Jr., Sara K. Pasquali, Katya Zelevinsky, Rui Duan, Sharon-Lise T. Normand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Congenital heart defects (CHD) are the most prevalent birth defects in the
United States and surgical outcomes vary considerably across the country. The
outcomes of treatment for CHD differ for specific patient subgroups, with
non-Hispanic Black and Hispanic populations experiencing higher rates of
mortality and morbidity. A valid comparison of outcomes within racial/ethnic
subgroups is difficult given large differences in case-mix and small subgroup
sizes. We propose a causal inference framework for outcome assessment and
leverage advances in transfer learning to incorporate data from both target and
source populations to help estimate causal effects while accounting for
different sources of risk factor and outcome differences across populations.
Using the Society of Thoracic Surgeons' Congenital Heart Surgery Database
(STS-CHSD), we focus on a national cohort of patients undergoing the Norwood
operation from 2016-2022 to assess operative mortality and morbidity outcomes
across U.S. geographic regions by race/ethnicity. We find racial and ethnic
outcome differences after controlling for potential confounding factors. While
geography does not have a causal effect on outcomes for non-Hispanic Caucasian
patients, non-Hispanic Black patients experience wide variability in outcomes
with estimated 30-day mortality ranging from 5.9% (standard error 2.2%) to
21.6% (4.4%) across U.S. regions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Estimating Causal Effects with Double Machine Learning -- A Method
  Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14385v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14385v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Fuhr, Philipp Berens, Dominik Papies
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The estimation of causal effects with observational data continues to be a
very active research area. In recent years, researchers have developed new
frameworks which use machine learning to relax classical assumptions necessary
for the estimation of causal effects. In this paper, we review one of the most
prominent methods - "double/debiased machine learning" (DML) - and empirically
evaluate it by comparing its performance on simulated data relative to more
traditional statistical methods, before applying it to real-world data. Our
findings indicate that the application of a suitably flexible machine learning
algorithm within DML improves the adjustment for various nonlinear confounding
relationships. This advantage enables a departure from traditional functional
form assumptions typically necessary in causal effect estimation. However, we
demonstrate that the method continues to critically depend on standard
assumptions about causal structure and identification. When estimating the
effects of air pollution on housing prices in our application, we find that DML
estimates are consistently larger than estimates of less flexible methods. From
our overall results, we provide actionable recommendations for specific choices
researchers must make when applying DML in practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recovering Latent Confounders from High-dimensional Proxy Variables 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14228v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14228v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Mankovich, Homer Durand, Emiliano Diaz, Gherardo Varando, Gustau Camps-Valls
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting latent confounders from proxy variables is an essential problem in
causal effect estimation. Previous approaches are limited to low-dimensional
proxies, sorted proxies, and binary treatments. We remove these assumptions and
present a novel Proxy Confounder Factorization (PCF) framework for continuous
treatment effect estimation when latent confounders manifest through
high-dimensional, mixed proxy variables. For specific sample sizes, our
two-step PCF implementation, using Independent Component Analysis (ICA-PCF),
and the end-to-end implementation, using Gradient Descent (GD-PCF), achieve
high correlation with the latent confounder and low absolute error in causal
effect estimation with synthetic datasets in the high sample size regime. Even
when faced with climate data, ICA-PCF recovers four components that explain
$75.9\%$ of the variance in the North Atlantic Oscillation, a known confounder
of precipitation patterns in Europe. Code for our PCF implementations and
experiments can be found here: https://github.com/IPL-UV/confound_it. The
proposed methodology constitutes a stepping stone towards discovering latent
confounders and can be applied to many problems in disciplines dealing with
high-dimensional observed proxies, e.g., spatiotemporal fields.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Posterior concentrations of fully-connected Bayesian neural networks
  with general priors on the weights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14225v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14225v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Insung Kong, Yongdai Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian approaches for training deep neural networks (BNNs) have received
significant interest and have been effectively utilized in a wide range of
applications. There have been several studies on the properties of posterior
concentrations of BNNs. However, most of these studies only demonstrate results
in BNN models with sparse or heavy-tailed priors. Surprisingly, no theoretical
results currently exist for BNNs using Gaussian priors, which are the most
commonly used one. The lack of theory arises from the absence of approximation
results of Deep Neural Networks (DNNs) that are non-sparse and have bounded
parameters. In this paper, we present a new approximation theory for non-sparse
DNNs with bounded parameters. Additionally, based on the approximation theory,
we show that BNNs with non-sparse general priors can achieve near-minimax
optimal posterior concentration rates to the true model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OTSeg: Multi-prompt Sinkhorn Attention for Zero-Shot Semantic
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14183v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14183v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kwanyoung Kim, Yujin Oh, Jong Chul Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent success of CLIP has demonstrated promising results in zero-shot
semantic segmentation by transferring muiltimodal knowledge to pixel-level
classification. However, leveraging pre-trained CLIP knowledge to closely align
text embeddings with pixel embeddings still has limitations in existing
approaches. To address this issue, we propose OTSeg, a novel multimodal
attention mechanism aimed at enhancing the potential of multiple text prompts
for matching associated pixel embeddings. We first propose Multi-Prompts
Sinkhorn (MPS) based on the Optimal Transport (OT) algorithm, which leads
multiple text prompts to selectively focus on various semantic features within
image pixels. Moreover, inspired by the success of Sinkformers in unimodal
settings, we introduce the extension of MPS, called Multi-Prompts Sinkhorn
Attention (MPSA), which effectively replaces cross-attention mechanisms within
Transformer framework in multimodal settings. Through extensive experiments, we
demonstrate that OTSeg achieves state-of-the-art (SOTA) performance with
significant gains on Zero-Shot Semantic Segmentation (ZS3) tasks across three
benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Let's do the time-warp-attend: Learning topological invariants of
  dynamical systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.09234v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.09234v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noa Moriel, Matthew Ricci, Mor Nitzan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamical systems across the sciences, from electrical circuits to ecological
networks, undergo qualitative and often catastrophic changes in behavior,
called bifurcations, when their underlying parameters cross a threshold.
Existing methods predict oncoming catastrophes in individual systems but are
primarily time-series-based and struggle both to categorize qualitative
dynamical regimes across diverse systems and to generalize to real data. To
address this challenge, we propose a data-driven, physically-informed
deep-learning framework for classifying dynamical regimes and characterizing
bifurcation boundaries based on the extraction of topologically invariant
features. We focus on the paradigmatic case of the supercritical Hopf
bifurcation, which is used to model periodic dynamics across a wide range of
applications. Our convolutional attention method is trained with data
augmentations that encourage the learning of topological invariants which can
be used to detect bifurcation boundaries in unseen systems and to design models
of biological systems like oscillatory gene regulatory networks. We further
demonstrate our method's use in analyzing real data by recovering distinct
proliferation and differentiation dynamics along pancreatic endocrinogenesis
trajectory in gene expression space based on single-cell data. Our method
provides valuable insights into the qualitative, long-term behavior of a wide
range of dynamical systems, and can detect bifurcations or catastrophic
transitions in large-scale physical and biological systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exact and general decoupled solutions of the LMC Multitask Gaussian
  Process model <span class="chip">UAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12032v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12032v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olivier Truffinet, Karim Ammar, Jean-Philippe Argaud, Bertrand Bouriquet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Linear Model of Co-regionalization (LMC) is a very general model of
multitask gaussian process for regression or classification. While its
expressivity and conceptual simplicity are appealing, naive implementations
have cubic complexity in the number of datapoints and number of tasks, making
approximations mandatory for most applications. However, recent work has shown
that under some conditions the latent processes of the model can be decoupled,
leading to a complexity that is only linear in the number of said processes. We
here extend these results, showing from the most general assumptions that the
only condition necessary to an efficient exact computation of the LMC is a mild
hypothesis on the noise model. We introduce a full parametrization of the
resulting \emph{projected LMC} model, and an expression of the marginal
likelihood enabling efficient optimization. We perform a parametric study on
synthetic data to show the excellent performance of our approach, compared to
an unrestricted exact LMC and approximations of the latter. Overall, the
projected LMC appears as a credible and simpler alternative to state-of-the art
models, which greatly facilitates some computations such as leave-one-out
cross-validation and fantasization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 10 figures, submitted to UAI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Instance-dependent uniform tail bounds for empirical processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.10053v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.10053v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sohail Bahmani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We formulate a uniform tail bound for empirical processes indexed by a class
of functions, in terms of the individual deviations of the functions rather
than the worst-case deviation in the considered class. The tail bound is
established by introducing an initial "deflation" step to the standard generic
chaining argument. The resulting tail bound is the sum of the complexity of the
"deflated function class" in terms of a generalization of Talagrand's $\gamma$
functional, and the deviation of the function instance, both of which are
formulated based on the natural seminorm induced by the corresponding
Cram\'{e}r functions. We also provide certain approximations for the mentioned
seminorm when the function class lies in a given (exponential type) Orlicz
space, that can be used to make the complexity term and the deviation term more
explicit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages. Revised and extended one of the examples for a more clear,
  detailed, and accurate description</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mathematical Opportunities in Digital Twins (MATH-DT) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10326v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10326v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harbir Antil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The report describes the discussions from the Workshop on Mathematical
Opportunities in Digital Twins (MATH-DT) from December 11-13, 2023, George
Mason University.
  It illustrates that foundational Mathematical advances are required for
Digital Twins (DTs) that are different from traditional approaches. A
traditional model, in biology, physics, engineering or medicine, starts with a
generic physical law (e.g., equations) and is often a simplification of
reality. A DT starts with a specific ecosystem, object or person (e.g.,
personalized care) representing reality, requiring multi -scale, -physics
modeling and coupling. Thus, these processes begin at opposite ends of the
simulation and modeling pipeline, requiring different reliability criteria and
uncertainty assessments. Additionally, unlike existing approaches, a DT assists
humans to make decisions for the physical system, which (via sensors) in turn
feeds data into the DT, and operates for the life of the physical system.
  While some of the foundational mathematical research can be done without a
specific application context, one must also keep specific applications in mind
for DTs. E.g., modeling a bridge or a biological system (a patient), or a
socio-technical system (a city) is very different. The models range from
differential equations (deterministic/uncertain) in engineering, to stochastic
in biology, including agent-based. These are multi-scale hybrid models or large
scale (multi-objective) optimization problems under uncertainty. There are no
universal models or approaches. For e.g., Kalman filters for forecasting might
work in engineering, but can fail in biomedical domain. Ad hoc studies, with
limited systematic work, have shown that AI/ML methods can fail for simple
engineering systems and can work well for biomedical problems.
  A list of `Mathematical Opportunities and Challenges' concludes the report.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Posterior Sampling Based on Gradient Flows of the MMD with Negative
  Distance Kernel <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03054v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03054v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Hagemann, Johannes Hertrich, Fabian Altekrüger, Robert Beinert, Jannis Chemseddine, Gabriele Steidl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose conditional flows of the maximum mean discrepancy (MMD) with the
negative distance kernel for posterior sampling and conditional generative
modeling. This MMD, which is also known as energy distance, has several
advantageous properties like efficient computation via slicing and sorting. We
approximate the joint distribution of the ground truth and the observations
using discrete Wasserstein gradient flows and establish an error bound for the
posterior distributions. Further, we prove that our particle flow is indeed a
Wasserstein gradient flow of an appropriate functional. The power of our method
is demonstrated by numerical examples including conditional image generation
and inverse problems like superresolution, inpainting and computed tomography
in low-dose and limited-angle settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mixture of segmentation for heterogeneous functional data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10712v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10712v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent Brault, Émilie Devijver, Charlotte Laclau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we consider functional data with heterogeneity in time and in
population. We propose a mixture model with segmentation of time to represent
this heterogeneity while keeping the functional structure. Maximum likelihood
estimator is considered, proved to be identifiable and consistent. In practice,
an EM algorithm is used, combined with dynamic programming for the maximization
step, to approximate the maximum likelihood estimator. The method is
illustrated on a simulated dataset, and used on a real dataset of electricity
consumption.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>45 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Tempered to Benign Overfitting in ReLU Neural Networks <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.15141v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.15141v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guy Kornowski, Gilad Yehudai, Ohad Shamir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Overparameterized neural networks (NNs) are observed to generalize well even
when trained to perfectly fit noisy data. This phenomenon motivated a large
body of work on "benign overfitting", where interpolating predictors achieve
near-optimal performance. Recently, it was conjectured and empirically observed
that the behavior of NNs is often better described as "tempered overfitting",
where the performance is non-optimal yet also non-trivial, and degrades as a
function of the noise level. However, a theoretical justification of this claim
for non-linear NNs has been lacking so far. In this work, we provide several
results that aim at bridging these complementing views. We study a simple
classification setting with 2-layer ReLU NNs, and prove that under various
assumptions, the type of overfitting transitions from tempered in the extreme
case of one-dimensional data, to benign in high dimensions. Thus, we show that
the input dimension has a crucial role on the type of overfitting in this
setting, which we also validate empirically for intermediate dimensions.
Overall, our results shed light on the intricate connections between the
dimension, sample size, architecture and training algorithm on the one hand,
and the type of resulting overfitting on the other hand.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023; fixed bug</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalized Early Stopping in Evolutionary Direct <span class="highlight-title">Policy</span> Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.03574v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.03574v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Etor Arza, Leni K. Le Goff, Emma Hart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lengthy evaluation times are common in many optimization problems such as
direct policy search tasks, especially when they involve conducting evaluations
in the physical world, e.g. in robotics applications. Often when evaluating
solution over a fixed time period it becomes clear that the objective value
will not increase with additional computation time (for example when a two
wheeled robot continuously spins on the spot). In such cases, it makes sense to
stop the evaluation early to save computation time. However, most approaches to
stop the evaluation are problem specific and need to be specifically designed
for the task at hand. Therefore, we propose an early stopping method for direct
policy search. The proposed method only looks at the objective value at each
time step and requires no problem specific knowledge. We test the introduced
stopping criterion in five direct policy search environments drawn from games,
robotics and classic control domains, and show that it can save up to 75% of
the computation time. We also compare it with problem specific stopping
criteria and show that it performs comparably, while being more generally
applicable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the consistency of supervised learning with missing values 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1902.06931v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1902.06931v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julie Josse, Jacob M. Chen, Nicolas Prost, Erwan Scornet, Gaël Varoquaux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many application settings, the data have missing entries which make
analysis challenging. An abundant literature addresses missing values in an
inferential framework: estimating parameters and their variance from incomplete
tables. Here, we consider supervised-learning settings: predicting a target
when missing values appear in both training and testing data. We show the
consistency of two approaches in prediction. A striking result is that the
widely-used method of imputing with a constant, such as the mean prior to
learning is consistent when missing values are not informative. This contrasts
with inferential settings where mean imputation is pointed at for distorting
the distribution of the data. That such a simple approach can be consistent is
important in practice. We also show that a predictor suited for complete
observations can predict optimally on incomplete data, through multiple
imputation. Finally, to compare imputation with learning directly with a model
that accounts for missing values, we analyze further decision trees. These can
naturally tackle empirical risk minimization with missing values, due to their
ability to handle the half-discrete nature of incomplete variables. After
comparing theoretically and empirically different missing values strategies in
trees, we recommend using the "missing incorporated in attribute" method as it
can handle both non-informative and informative missing values.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ED-NeRF: Efficient Text-Guided Editing of 3D Scene with Latent Space
  NeRF <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02712v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02712v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jangho Park, Gihyun Kwon, Jong Chul Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there has been a significant advancement in text-to-image diffusion
models, leading to groundbreaking performance in 2D image generation. These
advancements have been extended to 3D models, enabling the generation of novel
3D objects from textual descriptions. This has evolved into NeRF editing
methods, which allow the manipulation of existing 3D objects through textual
conditioning. However, existing NeRF editing techniques have faced limitations
in their performance due to slow training speeds and the use of loss functions
that do not adequately consider editing. To address this, here we present a
novel 3D NeRF editing approach dubbed ED-NeRF by successfully embedding
real-world scenes into the latent space of the latent diffusion model (LDM)
through a unique refinement layer. This approach enables us to obtain a NeRF
backbone that is not only faster but also more amenable to editing compared to
traditional image space NeRF editing. Furthermore, we propose an improved loss
function tailored for editing by migrating the delta denoising score (DDS)
distillation loss, originally used in 2D image editing to the three-dimensional
domain. This novel loss function surpasses the well-known score distillation
sampling (SDS) loss in terms of suitability for editing purposes. Our
experimental results demonstrate that ED-NeRF achieves faster editing speed
while producing improved output quality compared to state-of-the-art 3D editing
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024; Project Page: https://jhq1234.github.io/ed-nerf.github.io/</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-20T00:00:00Z">2024-03-20</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computer Science and Game Theory
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Unified Toll Lane Framework for Autonomous and High-Occupancy Vehicles
  in Interactive Mixed Autonomy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14011v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14011v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruolin Li, Philip N. Brown, Roberto Horowitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we introduce a toll lane framework that optimizes the mixed
flow of autonomous and high-occupancy vehicles on freeways, where human-driven
and autonomous vehicles of varying commuter occupancy share a segment.
Autonomous vehicles, with their ability to maintain shorter headways, boost
traffic throughput. Our framework designates a toll lane for autonomous
vehicles with high occupancy to use free of charge, while others pay a toll. We
explore the lane choice equilibria when all vehicles minimize travel costs, and
characterize the equilibria by ranking vehicles by their mobility enhancement
potential, a concept we term the mobility degree. Through numerical examples,
we demonstrate the framework's utility in addressing design challenges such as
setting optimal tolls, determining occupancy thresholds, and designing lane
policies, showing how it facilitates the integration of high-occupancy and
autonomous vehicles. We also propose an algorithm for assigning rational tolls
to decrease total commuter delay and examine the effects of toll
non-compliance. Our findings suggest that self-interest-driven behavior
mitigates moderate non-compliance impacts, highlighting the framework's
resilience. This work presents a pioneering comprehensive analysis of a toll
lane framework that emphasizes the coexistence of autonomous and high-occupancy
vehicles, offering insights for traffic management improvements and the
integration of autonomous vehicles into existing transportation
infrastructures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analysing Guarantees in Australian Senate Outcomes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13275v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13275v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michelle Blom
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Single Transferable Vote (STV) is used to elect candidates to the 76 seat
Australian Senate across six states and two territories. These eight STV
contests are counted using a combination of ballot scanners, manual data entry
and tabulation software. On election night, some properties of the set of cast
ballots are determined by hand. This includes the first preference tallies of
each party. This technical report considers whether there are some properties,
such as individual candidates' first preference tallies, that, if assumed to be
accurate, imply a portion of the election outcome. The paper also presents an
interesting example showing that the rules of STV tabulation used for the
Australian Senate can allow bizarre behaviour, such as votes increasing in
value over time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cardinal-Utility Matching Markets: The Quest for Envy-Freeness,
  Pareto-Optimality, and Efficient Computability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08851v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08851v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thorben Tröbst, Vijay V. Vazirani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unlike ordinal-utility matching markets, which are well-developed from the
viewpoint of both theory and practice, recent insights from a computer science
perspective have left cardinal-utility matching markets in a quandary. The
celebrated pricing-based mechanism for one-sided cardinal-utility matching
markets due to Hylland and Zeckhauser, which had long eluded efficient
algorithms, was finally shown to be PPAD-complete.
  This led us to ask the question: is there an alternative, polynomial time,
mechanism for one-sided cardinal-utility matching markets which achieves the
desirable properties of HZ, i.e.\ (ex-ante) envy-freeness (EF) and
Pareto-optimality (PO)? In this paper we show:
  1. The problem of finding an EF+PO lottery in a one-sided cardinal-utility
matching market is PPAD-complete.
  2. A $(2 + \epsilon)$-approximately envy-free and (exactly) Pareto-optimal
lottery can be found in polynomial time using Nash bargaining. Moreover, the
resulting mechanism is $(2 + \epsilon)$-approximately incentive compatible.
  We also present several results on two-sided cardinal-utility matching
markets, including non-existence of EF+PO lotteries as well as existence of
justified-envy-free and weak Pareto-optimal lotteries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Entropy-Based Strategies for Multi-Bracket Pools 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.14339v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.14339v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan S. Brill, Abraham J. Wyner, Ian J. Barnett
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Much work in the parimutuel betting literature has discussed estimating event
outcome probabilities or developing optimal wagering strategies, particularly
for horse race betting. Some betting pools, however, involve betting not just
on a single event, but on a tuple of events. For example, pick six betting in
horse racing, March Madness bracket challenges, and predicting a randomly drawn
bitstring each involve making a series of individual forecasts. Although
traditional optimal wagering strategies work well when the size of the tuple is
very small (e.g., betting on the winner of a horse race), they are intractable
for more general betting pools in higher dimensions (e.g., March Madness
bracket challenges). Hence we pose the multi-brackets problem: supposing we
wish to predict a tuple of events and that we know the true probabilities of
each potential outcome of each event, what is the best way to tractably
generate a set of $n$ predicted tuples? The most general version of this
problem is extremely difficult, so we begin with a simpler setting. In
particular, we generate $n$ independent predicted tuples according to a
distribution having optimal entropy. This entropy-based approach is tractable,
scalable, and performs well.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Searching Search Spaces: Meta-evolving a Geometric Encoding for Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14019v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14019v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tarek Kunze, Paul Templier, Dennis G Wilson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In evolutionary policy search, neural networks are usually represented using
a direct mapping: each gene encodes one network weight. Indirect encoding
methods, where each gene can encode for multiple weights, shorten the genome to
reduce the dimensions of the search space and better exploit permutations and
symmetries. The Geometric Encoding for Neural network Evolution (GENE)
introduced an indirect encoding where the weight of a connection is computed as
the (pseudo-)distance between the two linked neurons, leading to a genome size
growing linearly with the number of genes instead of quadratically in direct
encoding. However GENE still relies on hand-crafted distance functions with no
prior optimization. Here we show that better performing distance functions can
be found for GENE using Cartesian Genetic Programming (CGP) in a meta-evolution
approach, hence optimizing the encoding to create a search space that is easier
to exploit. We show that GENE with a learned function can outperform both
direct encoding and the hand-crafted distances, generalizing on unseen
problems, and we study how the encoding impacts neural network properties.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evo* 2023 -- Late-Breaking Abstracts Volume 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13950v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13950v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        A. M. Mora, A. I. Esparcia-Alcázar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Volume with the Late-Breaking Abstracts submitted to the Evo* 2023
Conference, held in Brno (Czech Republic), from 12 to 14 of April. These papers
present ongoing research and preliminary results investigating on the
application of different approaches of Bioinspired Methods (mainly Evolutionary
Computation) to different problems, most of them real world ones.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LBAs accepted in Evo* 2023. Part of the Conference Proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Network bottlenecks and task structure control the evolution of
  interpretable learning rules in a foraging <span class="highlight-title">agent</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emmanouil Giannakakis, Sina Khajehabdollahi, Anna Levina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing reliable mechanisms for continuous local learning is a central
challenge faced by biological and artificial systems. Yet, how the
environmental factors and structural constraints on the learning network
influence the optimal plasticity mechanisms remains obscure even for simple
settings. To elucidate these dependencies, we study meta-learning via
evolutionary optimization of simple reward-modulated plasticity rules in
embodied agents solving a foraging task. We show that unconstrained
meta-learning leads to the emergence of diverse plasticity rules. However,
regularization and bottlenecks to the model help reduce this variability,
resulting in interpretable rules. Our findings indicate that the meta-learning
of plasticity rules is very sensitive to various parameters, with this
sensitivity possibly reflected in the learning rules found in biological
networks. When included in models, these dependencies can be used to discover
potential objective functions and details of biological learning via
comparisons with experimental observations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Analyzing and Improving the Training Dynamics of Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02696v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02696v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, Samuli Laine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models currently dominate the field of data-driven image synthesis
with their unparalleled scaling to large datasets. In this paper, we identify
and rectify several causes for uneven and ineffective training in the popular
ADM diffusion model architecture, without altering its high-level structure.
Observing uncontrolled magnitude changes and imbalances in both the network
activations and weights over the course of training, we redesign the network
layers to preserve activation, weight, and update magnitudes on expectation. We
find that systematic application of this philosophy eliminates the observed
drifts and imbalances, resulting in considerably better networks at equal
computational complexity. Our modifications improve the previous record FID of
2.41 in ImageNet-512 synthesis to 1.81, achieved using fast deterministic
sampling.
  As an independent contribution, we present a method for setting the
exponential moving average (EMA) parameters post-hoc, i.e., after completing
the training run. This allows precise tuning of EMA length without the cost of
performing several training runs, and reveals its surprising interactions with
network architecture, training time, and guidance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PAGE: Prototype-Based Model-Level Explanations for Graph Neural Networks <span class="chip">AAAI-22</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.17159v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.17159v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yong-Min Shin, Sun-Woo Kim, Won-Yong Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aside from graph neural networks (GNNs) attracting significant attention as a
powerful framework revolutionizing graph representation learning, there has
been an increasing demand for explaining GNN models. Although various
explanation methods for GNNs have been developed, most studies have focused on
instance-level explanations, which produce explanations tailored to a given
graph instance. In our study, we propose Prototype-bAsed GNN-Explainer (PAGE),
a novel model-level GNN explanation method that explains what the underlying
GNN model has learned for graph classification by discovering
human-interpretable prototype graphs. Our method produces explanations for a
given class, thus being capable of offering more concise and comprehensive
explanations than those of instance-level explanations. First, PAGE selects
embeddings of class-discriminative input graphs on the graph-level embedding
space after clustering them. Then, PAGE discovers a common subgraph pattern by
iteratively searching for high matching node tuples using node-level embeddings
via a prototype scoring function, thereby yielding a prototype graph as our
explanation. Using six graph classification datasets, we demonstrate that PAGE
qualitatively and quantitatively outperforms the state-of-the-art model-level
explanation method. We also carry out systematic experimental studies by
demonstrating the relationship between PAGE and instance-level explanation
methods, the robustness of PAGE to input data scarce environments, and the
computational efficiency of the proposed prototype scoring function in PAGE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 12 figures, 5 tables; to appear in the IEEE Transactions on
  Pattern Analysis and Machine Intelligence (Please cite our journal version
  that will appear in an upcoming issue. Its two-page extended summary was
  presented in the AAAI-22 Student Abstract and Poster Program.)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Runtime of Random Local Search on the Generalized Needle Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08153v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08153v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Doerr, Andrew James Kelley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In their recent work, C. Doerr and Krejca (Transactions on Evolutionary
Computation, 2023) proved upper bounds on the expected runtime of the
randomized local search heuristic on generalized Needle functions. Based on
these upper bounds, they deduce in a not fully rigorous manner a drastic
influence of the needle radius $k$ on the runtime.
  In this short article, we add the missing lower bound necessary to determine
the influence of parameter $k$ on the runtime. To this aim, we derive an exact
description of the expected runtime, which also significantly improves the
upper bound given by C. Doerr and Krejca. We also describe asymptotic estimates
of the expected runtime.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-19T00:00:00Z">2024-03-19</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computer Science and Game Theory
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uber Stable: Formulating the Rideshare System as a Stable Matching
  Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13083v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13083v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rhea Acharya, Jessica Chen, Helen Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Peer-to-peer ride-sharing platforms like Uber, Lyft, and DiDi have
revolutionized the transportation industry and labor market. At its essence,
these systems tackle the bipartite matching problem between two populations:
riders and drivers. This research paper comprises two main components: an
initial literature review of existing ride-sharing platforms and efforts to
enhance driver satisfaction, and the development of a novel algorithm
implemented through simulation testing to allow us to make our own
observations. The core algorithm utilized is the Gale-Shapley deferred
acceptance algorithm, applied to a static matching problem over multiple time
periods. In this simulation, we construct a preference-aware task assignment
model, considering both overall revenue maximization and individual preference
satisfaction. Specifically, the algorithm design incorporates factors such as
passenger willingness-to-pay, driver preferences, and location attractiveness,
with an overarching goal of achieving equitable income distribution for drivers
while maintaining overall system efficiency.
  Through simulation, the paper compares the performance of the proposed
algorithm with random matching and closest neighbor algorithms, looking at
metrics such as total revenue, revenue per ride, and standard deviation to
identify trends and impacts of shifting priorities. Additionally, the DA
algorithm is compared to the Boston algorithm, and the paper explores the
effect of prioritizing proximity to passengers versus distance from city
center. Ultimately, the research underscores the importance of continued
exploration in areas such as dynamic pricing models and additional modeling for
unconventional driving times to further enhance the findings on the
effectiveness and fairness of ride-sharing platforms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Freshness-aware Block Propagation Optimization in 6G-based Web 3.0: An
  Evolutionary <span class="highlight-title">Game</span> Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12807v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12807v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinbo Wen, Jiawen Kang, Zehui Xiong, Hongyang Du, Zhaohui Yang, Dusit Niyato, Meng Shen, Yutao Jiao, Yang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Driven by the aspiration to establish a decentralized digital economy, Web
3.0 is emerging as the fundamental technology for digital transformation.
Incorporating the promising sixth-generation (6G) technology with large
bandwidth and space-air-ground integrated coverage, 6G-based Web 3.0 holds
great potential in empowering users with enhanced data control and facilitating
secure peer-to-peer transactions, especially in consumer electronics, through
the utilization of blockchain technologies. However, 6G-based Web 3.0 is still
in its infancy, such as ensuring block freshness and optimizing block
propagation to improve blockchain performance. In this paper, we develop a
freshness-aware block propagation optimization framework for 6G-based Web 3.0.
We first propose a novel metric called Age of Block Information (AoBI) based on
the concept of age of information to quantify block freshness. To make block
propagation optimization tractable, we classify miners into five different
states and propose a block propagation model for public blockchains inspired by
epidemic models. Moreover, considering that the miners are bounded rational, we
propose an incentive mechanism based on the evolutionary game for block
propagation to improve block propagation efficiency. Numerical results
demonstrate that compared with other block propagation mechanisms, the proposed
scheme has a higher block forwarding probability, which improves block
propagation efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Game</span> Theory with Simulation of Other Players 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.11261v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.11261v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vojtech Kovarik, Caspar Oesterheld, Vincent Conitzer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Game-theoretic interactions with AI agents could differ from traditional
human-human interactions in various ways. One such difference is that it may be
possible to simulate an AI agent (for example because its source code is
known), which allows others to accurately predict the agent's actions. This
could lower the bar for trust and cooperation. In this paper, we formalize
games in which one player can simulate another at a cost. We first derive some
basic properties of such games and then prove a number of results for them,
including: (1) introducing simulation into generic-payoff normal-form games
makes them easier to solve; (2) if the only obstacle to cooperation is a lack
of trust in the possibly-simulated agent, simulation enables equilibria that
improve the outcome for both agents; and however (3) there are settings where
introducing simulation results in strictly worse outcomes for both players.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The latest version fixes some typos in the proof of Theorem 5</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One-Shot Strategic Classification Under Unknown Costs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.02761v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.02761v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elan Rosenfeld, Nir Rosenfeld
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of strategic classification is to learn decision rules which are
robust to strategic input manipulation. Earlier works assume that these
responses are known; while some recent works handle unknown responses, they
exclusively study online settings with repeated model deployments. But there
are many domains$\unicode{x2014}$particularly in public policy, a common
motivating use case$\unicode{x2014}$where multiple deployments are infeasible,
or where even one bad round is unacceptable. To address this gap, we initiate
the formal study of one-shot strategic classification under unknown responses,
which requires committing to a single classifier once. Focusing on uncertainty
in the users' cost function, we begin by proving that for a broad class of
costs, even a small mis-estimation of the true cost can entail trivial accuracy
in the worst case. In light of this, we frame the task as a minimax problem,
with the goal of identifying the classifier with the smallest worst-case risk
over an uncertainty set of possible costs. We design efficient algorithms for
both the full-batch and stochastic settings, which we prove converge (offline)
to the minimax solution at the dimension-independent rate of
$\tilde{\mathcal{O}}(T^{-\frac{1}{2}})$. Our theoretical analysis reveals
important structure stemming from strategic responses, particularly the value
of dual norm regularization with respect to the cost function.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Fixed a bug in Algorithm 1, significantly strengthened Theorem 4.2,
  and added Figure 1 to help visualize the lower bound in Theorem 3.2</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fundamental Limits of Throughput and Availability: Applications to
  prophet inequalities & transaction fee mechanism design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.19292v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.19292v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aadityan Ganesh, Jason Hartline, Atanu R Sinha, Matthew vonAllmen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the fundamental limits of availability and throughput for
independent and heterogeneous demands of a limited resource. Availability is
the probability that the demands are below the capacity of the resource.
Throughput is the expected fraction of the resource that is utilized by the
demands. We offer a concentration inequality generator that gives lower bounds
on feasible availability and throughput pairs with a given capacity and
independent but not necessarily identical distributions of up-to-unit demands.
We show that availability and throughput cannot both be poor. These bounds are
analogous to tail inequalities on sums of independent random variables, but
hold throughout the support of the demand distribution. This analysis gives
analytically tractable bounds supporting the unit-demand characterization of
Chawla, Devanur, and Lykouris (2023) and generalizes to up-to-unit demands. Our
bounds also provide an approach towards improved multi-unit prophet
inequalities (Hajiaghayi, Kleinberg, and Sandholm, 2007). They have
applications to transaction fee mechanism design (for blockchains) where high
availability limits the probability of profitable user-miner coalitions (Chung
and Shi, 2023).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 7 figures; updated author information to include
  institutions and email addresses</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evolutionary Optimization of Model Merging Recipes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takuya Akiba, Makoto Shing, Yujin Tang, Qi Sun, David Ha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel application of evolutionary algorithms to automate the
creation of powerful foundation models. While model merging has emerged as a
promising approach for LLM development due to its cost-effectiveness, it
currently relies on human intuition and domain knowledge, limiting its
potential. Here, we propose an evolutionary approach that overcomes this
limitation by automatically discovering effective combinations of diverse
open-source models, harnessing their collective intelligence without requiring
extensive additional training data or compute. Our approach operates in both
parameter space and data flow space, allowing for optimization beyond just the
weights of the individual models. This approach even facilitates cross-domain
merging, generating models like a Japanese LLM with Math reasoning
capabilities. Surprisingly, our Japanese Math LLM achieved state-of-the-art
performance on a variety of established Japanese LLM benchmarks, even
surpassing models with significantly more parameters, despite not being
explicitly trained for such tasks. Furthermore, a culturally-aware Japanese VLM
generated through our approach demonstrates its effectiveness in describing
Japanese culture-specific content, outperforming previous Japanese VLMs. This
work not only contributes new state-of-the-art models back to the open-source
community, but also introduces a new paradigm for automated model composition,
paving the way for exploring alternative, efficient approaches to foundation
model development.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using evolutionary computation to optimize task performance of
  unclocked, recurrent Boolean circuits in FPGAs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13105v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13105v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raphael Norman-Tenazas, David Kleinberg, Erik C. Johnson, Daniel P. Lathrop, Matthew J. Roos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It has been shown that unclocked, recurrent networks of Boolean gates in
FPGAs can be used for low-SWaP reservoir computing. In such systems, topology
and node functionality of the network are randomly initialized. To create a
network that solves a task, weights are applied to output nodes and learning is
achieved by adjusting those weights with conventional machine learning methods.
However, performance is often limited compared to networks where all parameters
are learned. Herein, we explore an alternative learning approach for unclocked,
recurrent networks in FPGAs. We use evolutionary computation to evolve the
Boolean functions of network nodes. In one type of implementation the output
nodes are used directly to perform a task and all learning is via evolution of
the network's node functions. In a second type of implementation a back-end
classifier is used as in traditional reservoir computing. In that case, both
evolution of node functions and adjustment of output node weights contribute to
learning. We demonstrate the practicality of node function evolution, obtaining
an accuracy improvement of ~30% on an image classification task while
processing at a rate of over three million samples per second. We additionally
demonstrate evolvability of network memory and dynamic output signals.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Path Planning in a dynamic environment using Spherical Particle Swarm
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohssen E. Elshaar, Mohammed R. Elbalshy, A. Hussien, Mohammed Abido
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficiently planning an Unmanned Aerial Vehicle (UAV) path is crucial,
especially in dynamic settings where potential threats are prevalent. A Dynamic
Path Planner (DPP) for UAV using the Spherical Vector-based Particle Swarm
Optimisation (SPSO) technique is proposed in this study. The UAV is supposed to
go from a starting point to an end point through an optimal path according to
some flight criteria. Path length, Safety, Attitude and Path Smoothness are all
taken into account upon deciding how an optimal path should be. The path is
constructed as a set of way-points that stands as re-planning checkpoints. At
each path way-point, threats are allowed some constrained random motion, where
their exact positions are updated and fed to the SPSO-solver. Four test
scenarios are carried out using real digital elevation models. Each test gives
different priorities to path length and safety, in order to show how well the
SPSO-DPP is capable of generating a safe yet efficient path segments. Finally,
a comparison is made to reveal the persistent overall superior performance of
the SPSO, in a dynamic environment, over both the Particle Swarm Optimisation
(PSO) and the Genetic Algorithm (GA). The methods are compared directly, by
averaging costs over multiple runs, and by considering different challenging
levels of obstacle motion. SPSO outperformed both PSO and GA, showcasing cost
reductions ranging from 330\% to 675\% compared to both algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as conference paper in IEEE WCCI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EAS-SNN: End-to-End Adaptive Sampling and Representation for Event-based
  Detection with Recurrent Spiking Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziming Wang, Ziling Wang, Huaning Li, Lang Qin, Runhao Jiang, De Ma, Huajin Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event cameras, with their high dynamic range and temporal resolution, are
ideally suited for object detection, especially under scenarios with motion
blur and challenging lighting conditions. However, while most existing
approaches prioritize optimizing spatiotemporal representations with advanced
detection backbones and early aggregation functions, the crucial issue of
adaptive event sampling remains largely unaddressed. Spiking Neural Networks
(SNNs), which operate on an event-driven paradigm through sparse spike
communication, emerge as a natural fit for addressing this challenge. In this
study, we discover that the neural dynamics of spiking neurons align closely
with the behavior of an ideal temporal event sampler. Motivated by this
insight, we propose a novel adaptive sampling module that leverages recurrent
convolutional SNNs enhanced with temporal memory, facilitating a fully
end-to-end learnable framework for event-based detection. Additionally, we
introduce Residual Potential Dropout (RPD) and Spike-Aware Training (SAT) to
regulate potential distribution and address performance degradation encountered
in spike-based sampling modules. Through rigorous testing on neuromorphic
datasets for event-based detection, our approach demonstrably surpasses
existing state-of-the-art spike-based methods, achieving superior performance
with significantly fewer parameters and time steps. For instance, our method
achieves a 4.4\% mAP improvement on the Gen1 dataset, while requiring 38\%
fewer parameters and three time steps. Moreover, the applicability and
effectiveness of our adaptive sampling methodology extend beyond SNNs, as
demonstrated through further validation on conventional non-spiking detection
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Genetically programmable optical random neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12490v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12490v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bora Çarpınlıoğlu, Bahrem Serhat Daniş, Uğur Teğin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Today, machine learning tools, particularly artificial neural networks, have
become crucial for diverse applications. However, current digital computing
tools to train and deploy artificial neural networks often struggle with
massive data sizes and high power consumptions. Optical computing provides
inherent parallelism and perform fundamental operations with passive optical
components. However, most of the optical computing platforms suffer from
relatively low accuracies for machine learning tasks due to fixed connections
while avoiding complex and sensitive techniques. Here, we demonstrate a
genetically programmable yet simple optical neural network to achieve high
performances with optical random projection. By genetically programming the
orientation of the scattering medium which acts as a random projection kernel
and only using 1% of the search space, our novel technique finds an optimum
kernel and improves its initial test accuracies 7-22% for various machine
learning tasks. Our optical computing method presents a promising approach to
achieve high performance in optical neural networks with a simple and scalable
design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Manuscript: 11 pages, 5 Figures. Supplementary material: 8 pages, 6
  Figures, 2 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Topological Representations of Heterogeneous Learning Dynamics of
  Recurrent Spiking Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12462v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12462v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Biswadeep Chakraborty, Saibal Mukhopadhyay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking Neural Networks (SNNs) have become an essential paradigm in
neuroscience and artificial intelligence, providing brain-inspired computation.
Recent advances in literature have studied the network representations of deep
neural networks. However, there has been little work that studies
representations learned by SNNs, especially using unsupervised local learning
methods like spike-timing dependent plasticity (STDP). Recent work by
\cite{barannikov2021representation} has introduced a novel method to compare
topological mappings of learned representations called Representation Topology
Divergence (RTD). Though useful, this method is engineered particularly for
feedforward deep neural networks and cannot be used for recurrent networks like
Recurrent SNNs (RSNNs). This paper introduces a novel methodology to use RTD to
measure the difference between distributed representations of RSNN models with
different learning methods. We propose a novel reformulation of RSNNs using
feedforward autoencoder networks with skip connections to help us compute the
RTD for recurrent networks. Thus, we investigate the learning capabilities of
RSNN trained using STDP and the role of heterogeneity in the synaptic dynamics
in learning such representations. We demonstrate that heterogeneous STDP in
RSNNs yield distinct representations than their homogeneous and surrogate
gradient-based supervised learning counterparts. Our results provide insights
into the potential of heterogeneous SNN models, aiding the development of more
efficient and biologically plausible hybrid artificial intelligence systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in IEEE World Congress on Computational Intelligence (IEEE
  WCCI) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning-guided iterated local search for the minmax multiple traveling
  salesman problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12389v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12389v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengfei He, Jin-Kao Hao, Jinhui Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The minmax multiple traveling salesman problem involves minimizing the
longest tour among a set of tours. The problem is of great practical interest
because it can be used to formulate several real-life applications. To solve
this computationally challenging problem, we propose a leaning-driven iterated
local search approach that combines an aggressive local search procedure with a
probabilistic acceptance criterion to find high-quality local optimal solutions
and a multi-armed bandit algorithm to select various removal and insertion
operators to escape local optimal traps. Extensive experiments on 77 commonly
used benchmark instances show that our algorithm achieves excellent results in
terms of solution quality and running time. In particular, it achieves 32 new
best-known results and matches the best-known results for 35 other instances.
Additional experiments shed light on the understanding of the composing
elements of the algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Lossless ANN-SNN Conversion under Ultra-Low Latency with
  Dual-Phase Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.07473v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.07473v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziming Wang, Shuang Lian, Yuhao Zhang, Xiaoxin Cui, Rui Yan, Huajin Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking neural networks (SNNs) operating with asynchronous discrete events
show higher energy efficiency with sparse computation. A popular approach for
implementing deep SNNs is ANN-SNN conversion combining both efficient training
of ANNs and efficient inference of SNNs. However, the accuracy loss is usually
non-negligible, especially under a few time steps, which restricts the
applications of SNN on latency-sensitive edge devices greatly. In this paper,
we first identify that such performance degradation stems from the
misrepresentation of the negative or overflow residual membrane potential in
SNNs. Inspired by this, we decompose the conversion error into three parts:
quantization error, clipping error, and residual membrane potential
representation error. With such insights, we propose a two-stage conversion
algorithm to minimize those errors respectively. Besides, We show each stage
achieves significant performance gains in a complementary manner. By evaluating
on challenging datasets including CIFAR-10, CIFAR- 100 and ImageNet, the
proposed method demonstrates the state-of-the-art performance in terms of
accuracy, latency and energy preservation. Furthermore, our method is evaluated
using a more challenging object detection task, revealing notable gains in
regression performance under ultra-low latency when compared to existing
spike-based detection algorithms. Codes are available at
https://github.com/Windere/snn-cvt-dual-phase.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Neural Networks and Learning Systems
  (TNNLS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Successor Features with Distributed Hebbian Temporal Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13391v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13391v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evgenii Dzhivelikian, Petr Kuderov, Aleksandr I. Panov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel approach to address the challenge of online
temporal memory learning for decision-making under uncertainty in
non-stationary, partially observable environments. The proposed algorithm,
Distributed Hebbian Temporal Memory (DHTM), is based on factor graph formalism
and a multicomponent neuron model. DHTM aims to capture sequential data
relationships and make cumulative predictions about future observations,
forming Successor Features (SF). Inspired by neurophysiological models of the
neocortex, the algorithm utilizes distributed representations, sparse
transition matrices, and local Hebbian-like learning rules to overcome the
instability and slow learning process of traditional temporal memory algorithms
like RNN and HMM. Experimental results demonstrate that DHTM outperforms LSTM
and a biologically inspired HMM-like algorithm, CSCG, in the case of
non-stationary datasets. Our findings suggest that DHTM is a promising approach
for addressing the challenges of online sequence learning and planning in
dynamic environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-18T00:00:00Z">2024-03-18</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computer Science and Game Theory
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Information Compression in Dynamic Information Disclosure <span class="highlight-title">Game</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12204v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12204v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dengwang Tang, Vijay G. Subramanian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a two-player dynamic information design problem between a
principal and a receiver -- a game is played between the two agents on top of a
Markovian system controlled by the receiver's actions, where the principal
obtains and strategically shares some information about the underlying system
with the receiver in order to influence their actions. In our setting, both
players have long-term objectives, and the principal sequentially commits to
their strategies instead of committing at the beginning. Further, the principal
cannot directly observe the system state, but at every turn they can choose
randomized experiments to observe the system partially. The principal can share
details about the experiments to the receiver. For our analysis we impose the
truthful disclosure rule: the principal is required to truthfully announce the
details and the result of each experiment to the receiver immediately after the
experiment result is revealed. Based on the received information, the receiver
takes an action when its their turn, with the action influencing the state of
the underlying system. We show that there exist Perfect Bayesian equilibria in
this game where both agents play Canonical Belief Based (CBB) strategies using
a compressed version of their information, rather than full information, to
choose experiments (for the principal) or actions (for the receiver). We also
provide a backward inductive procedure to solve for an equilibrium in CBB
strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PackIt! Gamified Rectangle Packing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12195v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12195v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Garrison, Marijn J. H. Heule, Bernardo Subercaseaux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present and analyze PackIt!, a turn-based game consisting of packing
rectangles on an $n \times n$ grid. PackIt! can be easily played on paper,
either as a competitive two-player game or in \emph{solitaire} fashion. On the
$t$-th turn, a rectangle of area $t$ or $t+1$ must be placed in the grid. In
the two-player format of PackIt! whichever player places a rectangle last wins,
whereas the goal in the solitaire variant is to perfectly pack the $n \times n$
grid. We analyze conditions for the existence of a perfect packing over $n
\times n$, then present an automated reasoning approach that allows finding
perfect games of PackIt! up to $n = 50$ which includes a novel SAT-encoding
technique of independent interest, and conclude by proving an NP-hardness
result.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 10 figures, Submitted to Fun with Algorithms</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fragile Stable Matchings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12183v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12183v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kirill Rudov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We show how fragile stable matchings are in a decentralized one-to-one
matching setting. The classical work of Roth and Vande Vate (1990) suggests
simple decentralized dynamics in which randomly-chosen blocking pairs match
successively. Such decentralized interactions guarantee convergence to a stable
matching. Our first theorem shows that, under mild conditions, any unstable
matching -- including a small perturbation of a stable matching -- can
culminate in any stable matching through these dynamics. Our second theorem
highlights another aspect of fragility: stabilization may take a long time.
Even in markets with a unique stable matching, where the dynamics always
converge to the same matching, decentralized interactions can require an
exponentially long duration to converge. A small perturbation of a stable
matching may lead the market away from stability and involve a sizable
proportion of mismatched participants for extended periods. Our results hold
for a broad class of dynamics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MAC Advice for Facility Location Mechanism Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12181v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12181v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zohar Barak, Anupam Gupta, Inbal Talgam-Cohen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Algorithms with predictions have attracted much attention in the last years
across various domains, including variants of facility location, as a way to
surpass traditional worst-case analyses. We study the $k$-facility location
mechanism design problem, where the $n$ agents are strategic and might
misreport their location.
  Unlike previous models, where predictions are for the $k$ optimal facility
locations, we receive $n$ predictions for the locations of each of the agents.
However, these predictions are only "mostly" and "approximately" correct (or
MAC for short) -- i.e., some $\delta$-fraction of the predicted locations are
allowed to be arbitrarily incorrect, and the remainder of the predictions are
allowed to be correct up to an $\varepsilon$-error. We make no assumption on
the independence of the errors. Can such predictions allow us to beat the
current best bounds for strategyproof facility location?
  We show that the $1$-median (geometric median) of a set of points is
naturally robust under corruptions, which leads to an algorithm for
single-facility location with MAC predictions. We extend the robustness result
to a "balanced" variant of the $k$ facilities case. Without balancedness, we
show that robustness completely breaks down, even for the setting of $k=2$
facilities on a line. For this "unbalanced" setting, we devise a truthful
random mechanism that outperforms the best known result of Lu et al. [2010],
which does not use predictions. En route, we introduce the problem of "second"
facility location (when the first facility's location is already fixed). Our
findings on the robustness of the $1$-median and more generally $k$-medians may
be of independent interest, as quantitative versions of classic breakdown-point
results in robust statistics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Simple 2-Approximation Algorithm For Minimum Manhattan Network Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11811v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11811v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md. Musfiqur Rahman Sanim, Safrunnesa Saira, Fatin Faiaz Ahsan, Rajon Bardhan, S. M. Ferdous
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given a n points in two dimensional space, a Manhattan Network G is a network
that connects all n points with either horizontal or vertical edges, with the
property that for any two point in G should be connected by a Manhattan path
and distance between this two points is equal to Manhattan Distance. The
Minimum Manhattan Network problem is to find a Manhattan network with minimum
network length, i.e., summation of all line segment in network should be
minimize. In this paper, we proposed a 2-approximation algorithm with time
complexity O(|E|lgN) where |E| is the number of edges and N is the number of
nodes. Using randomly generated datasets, we compare our result with the
optimal one.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ARSSS International Conference, Dhaka, Bangladesh</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cooperative Agri-Food Export under Minimum Quantity Commitments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11633v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11633v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luis A. Guardiola, Behzad Hezarkhani, Ana Meca
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  International trade can be a profitable business for agri-food communities.
However, access to international markets can be costly and thus unattainable
for small and medium sized enterprises (SMEs). This problem is exacerbated
under trade policies which require minimum quantity commitments (MQCs) on
export volumes, e.g., licensing tariff rate quota (TRQ) mechanisms.
  We show how cooperative exporting among agri-food SMEs can tackle the
barriers posed by the MQCs, and give market access to a broader range of SMEs.
We formulate a class of cooperative games associated with these situations and
find a gain-sharing mechanism that result in allocations in their corresponding
cores. Thus, grand coalitions of cooperative exporting SMEs can form in stable
manners.
  This allocation rule shares the export surplus only among the "essential" SME
exporters, that is, the players who are sufficiently cost efficient. Thus, less
cost efficient "complimentary" SMEs whose capacities are needed to maintain
MQCs receive no benefit from collaborative exporting and their participation
have to be altruistic. We propose two modifications to our original allocation
rule to share a portion of export surplus among the complementary SMEs through
taxing the essential SMEs: the first through egalitarian, and the second
through revenue-based rates. We compare the performance of these allocations
with the numerical examples and discuss their practical implications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Expanding the Resolution Boundary of Outcome-Based Imperfect-Recall
  Abstraction in <span class="highlight-title">Game</span>s with Ordered Signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanchang Fu, Junge Zhang, Dongdong Bai, Lingyun Zhao, Jialu Song, Kaiqi Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the development of advanced Texas Hold'em AI systems, abstraction
technology has garnered widespread attention due to its significant effect in
simplifying game complexity. This study adopts a more specific model, the games
of ordered signal, to describe Texas Hold'em-style games and optimizes this
model to streamline its mathematical representation and broaden its
applicability. By transitioning from a broad imperfect information game model
to a game with ordered signals model, we have separated the previously
intertwined infoset abstraction and action abstraction into independent signal
abstraction and action abstraction. Importantly, this signal abstraction
provides a mathematical framework for the hand abstraction task, which is
emphatically discussed in this paper. Additionally, a novel common refinement
principle is introduced, revealing the limit performance of hand abstraction
algorithms. We introduce potential outcome isomorphism (POI) and pinpoint that
it suffers from the issue of excessive abstraction. Futher, We demonstrate that
POI serves as a common refinement for leading outcome-based hand abstraction
algorithms, such as E[HS] and PA\&PAEMD. Consequently, excessive abstraction
also inherently affects these algorithms, leading to suboptimal performance.
Our investigation reveals the omission of historical data as a primary
contributor to excessive abstraction. To remedy this, we propose the K-Recall
Outcome Isomorphism (KROI) to incorporate the missing information. Compared
with POI, KROI more accurately mirrors lossless isomorphism (LI), the ground
truth, offering enhanced signal abstraction resolution. Experimental results in
the Numeral211 Hold'em indicate that strategies developed through KROI
approximate the exploitability of those developed through LI more closely than
those trained through POI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contract-Based Distributed Synthesis in Two-Objective Parity <span class="highlight-title">Game</span>s <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06212v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06212v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashwani Anand, Satya Prakash Nayak, Anne-Kathrin Schmuck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel method to compute $\textit{assume-guarantee contracts}$ in
non-zerosum two-player games over finite graphs where each player has a
different $ \omega $-regular winning condition. Given a game graph $G$ and two
parity winning conditions $\Phi_0$ and $\Phi_1$ over $G$, we compute
$\textit{contracted strategy-masks}$ ($\texttt{csm}$) $(\Psi_{i},\Phi_{i})$ for
each Player $i$. Within a $\texttt{csm}$, $\Phi_{i}$ is a $\textit{permissive
strategy template}$ which collects an infinite number of winning strategies for
Player $i$ under the assumption that Player $1-i$ chooses any strategy from the
$\textit{permissive assumption template}$ $\Psi_{i}$. The main feature of
$\texttt{csm}$'s is their power to $\textit{fully decentralize all remaining
strategy choices}$ -- if the two player's $\texttt{csm}$'s are compatible, they
provide a pair of new local specifications $\Phi_0^\bullet$ and
$\Phi_1^\bullet$ such that Player $i$ can locally and fully independently
choose any strategy satisfying $\Phi_i^\bullet$ and the resulting strategy
profile is ensured to be winning in the original two-objective game
$(G,\Phi_0,\Phi_1)$.
  In addition, the new specifications $\Phi_i^\bullet$ are $\textit{maximally
cooperative}$, i.e., allow for the distributed synthesis of any cooperative
solution. Further, our algorithmic computation of $\texttt{csm}$'s is complete
and ensured to terminate.
  We illustrate how the unique features of our synthesis framework effectively
address multiple challenges in the context of \enquote{correct-by-design}
logical control software synthesis for cyber-physical systems and provide
empirical evidence that our approach possess desirable structural and
computational properties compared to state-of-the-art techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>HSCC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fair Division of Multi-layered Cakes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.00726v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.00726v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Azharuddin Sanpui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider multi-layered cake cutting in order to fairly allocate numerous
divisible resources (layers of cake) among a group of agents under two
constraints: contiguity and feasibility. We first introduce a new computational
model in a multi-layered cake named ``a pair of knives''. Then, we show the
existence of an exact multi-allocation for two agents and two layers using the
new computational model. We demonstrate the computation procedure of a feasible
and contiguous proportional multi-allocation over a three-layered cake for more
than three agents. Finally, we develop a technique for computing proportional
allocations for any number $n\geq 2^a3$ of agents and $2^a3$ layers, where $a$
is any positive integer.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Molecular Classification Using Hyperdimensional Graph Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pere Verges, Igor Nunes, Mike Heddes, Tony Givargis, Alexandru Nicolau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our work introduces an innovative approach to graph learning by leveraging
Hyperdimensional Computing. Graphs serve as a widely embraced method for
conveying information, and their utilization in learning has gained significant
attention. This is notable in the field of chemoinformatics, where learning
from graph representations plays a pivotal role. An important application
within this domain involves the identification of cancerous cells across
diverse molecular structures.
  We propose an HDC-based model that demonstrates comparable Area Under the
Curve results when compared to state-of-the-art models like Graph Neural
Networks (GNNs) or the Weisfieler-Lehman graph kernel (WL). Moreover, it
outperforms previously proposed hyperdimensional computing graph learning
methods. Furthermore, it achieves noteworthy speed enhancements, boasting a 40x
acceleration in the training phase and a 15x improvement in inference time
compared to GNN and WL models. This not only underscores the efficacy of the
HDC-based method, but also highlights its potential for expedited and
resource-efficient graph learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised End-to-End Training with a Self-Defined Bio-Inspired Target 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12116v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12116v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongshu Liu, Jérémie Laydevant, Adrien Pontlevy, Damien Querlioz, Julie Grollier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current unsupervised learning methods depend on end-to-end training via deep
learning techniques such as self-supervised learning, with high computational
requirements, or employ layer-by-layer training using bio-inspired approaches
like Hebbian learning, using local learning rules incompatible with supervised
learning. Both approaches are problematic for edge AI hardware that relies on
sparse computational resources and would strongly benefit from alternating
between unsupervised and supervised learning phases - thus leveraging widely
available unlabeled data from the environment as well as labeled training
datasets. To solve this challenge, in this work, we introduce a 'self-defined
target' that uses Winner-Take-All (WTA) selectivity at the network's final
layer, complemented by regularization through biologically inspired homeostasis
mechanism. This approach, framework-agnostic and compatible with both global
(Backpropagation) and local (Equilibrium propagation) learning rules, achieves
a 97.6% test accuracy on the MNIST dataset. Furthermore, we demonstrate that
incorporating a hidden layer enhances classification accuracy and the quality
of learned features across all training methods, showcasing the advantages of
end-to-end unsupervised training. Extending to semi-supervised learning, our
method dynamically adjusts the target according to data availability, reaching
a 96.6% accuracy with just 600 labeled MNIST samples. This result highlights
our 'unsupervised target' strategy's efficacy and flexibility in scenarios
ranging from abundant to no labeled data availability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing Neuromorphic Computing: Mixed-Signal Design Techniques
  Leveraging Brain Code Units and Fundamental Code Units 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11563v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11563v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Murat Isik, Sols Miziev, Wiktoria Pawlak, Newton Howard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a groundbreaking digital neuromorphic architecture that
innovatively integrates Brain Code Unit (BCU) and Fundamental Code Unit (FCU)
using mixedsignal design methodologies. Leveraging open-source datasets and the
latest advances in materials science, our research focuses on enhancing the
computational efficiency, accuracy, and adaptability of neuromorphic systems.
The core of our approach lies in harmonizing the precision and scalability of
digital systems with the robustness and energy efficiency of analog processing.
Through experimentation, we demonstrate the effectiveness of our system across
various metrics. The BCU achieved an accuracy of 88.0% and a power efficiency
of 20.0 GOP/s/W, while the FCU recorded an accuracy of 86.5% and a power
efficiency of 18.5 GOP/s/W. Our mixed-signal design approach significantly
improved latency and throughput, achieving a latency as low as 0.75 ms and
throughput up to 213 TOP/s. These results firmly establish the potential of our
architecture in neuromorphic computing, providing a solid foundation for future
developments in this domain. Our study underscores the feasibility of
mixedsignal neuromorphic systems and their promise in advancing the field,
particularly in applications requiring high efficiency and adaptability
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 2024 International Joint Conference on Neural Networks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM Guided Evolution -- The Automation of Models Advancing Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11446v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11446v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clint Morris, Michael Jurado, Jason Zutty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of machine learning, traditional model development and automated
approaches like AutoML typically rely on layers of abstraction, such as
tree-based or Cartesian genetic programming. Our study introduces "Guided
Evolution" (GE), a novel framework that diverges from these methods by
utilizing Large Language Models (LLMs) to directly modify code. GE leverages
LLMs for a more intelligent, supervised evolutionary process, guiding mutations
and crossovers. Our unique "Evolution of Thought" (EoT) technique further
enhances GE by enabling LLMs to reflect on and learn from the outcomes of
previous mutations. This results in a self-sustaining feedback loop that
augments decision-making in model evolution. GE maintains genetic diversity,
crucial for evolutionary algorithms, by leveraging LLMs' capability to generate
diverse responses from expertly crafted prompts and modulate model temperature.
This not only accelerates the evolution process but also injects expert like
creativity and insight into the process. Our application of GE in evolving the
ExquisiteNetV2 model demonstrates its efficacy: the LLM-driven GE autonomously
produced variants with improved accuracy, increasing from 92.52% to 93.34%,
without compromising model compactness. This underscores the potential of LLMs
to accelerate the traditional model design pipeline, enabling models to
autonomously evolve and enhance their own designs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Library of Mirrors: Deep Neural Nets in Low Dimensions are Convex
  Lasso Models with Reflection Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01046v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01046v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emi Zeger, Yifei Wang, Aaron Mishkin, Tolga Ergen, Emmanuel Candès, Mert Pilanci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We prove that training neural networks on 1-D data is equivalent to solving a
convex Lasso problem with a fixed, explicitly defined dictionary matrix of
features. The specific dictionary depends on the activation and depth. We
consider 2-layer networks with piecewise linear activations, deep narrow ReLU
networks with up to 4 layers, and rectangular and tree networks with sign
activation and arbitrary depth. Interestingly in ReLU networks, a fourth layer
creates features that represent reflections of training data about themselves.
The Lasso representation sheds insight to globally optimal networks and the
solution landscape.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-17T00:00:00Z">2024-03-17</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computer Science and Game Theory
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Independent RL for Cooperative-Competitive <span class="highlight-title">Agent</span>s: A Mean-Field
  Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11345v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11345v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Aneeq uz Zaman, Alec Koppel, Mathieu Laurière, Tamer Başar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address in this paper Reinforcement Learning (RL) among agents that are
grouped into teams such that there is cooperation within each team but
general-sum (non-zero sum) competition across different teams. To develop an RL
method that provably achieves a Nash equilibrium, we focus on a
linear-quadratic structure. Moreover, to tackle the non-stationarity induced by
multi-agent interactions in the finite population setting, we consider the case
where the number of agents within each team is infinite, i.e., the mean-field
setting. This results in a General-Sum LQ Mean-Field Type Game (GS-MFTGs). We
characterize the Nash equilibrium (NE) of the GS-MFTG, under a standard
invertibility condition. This MFTG NE is then shown to be $\mathcal{O}(1/M)$-NE
for the finite population game where $M$ is a lower bound on the number of
agents in each team. These structural results motivate an algorithm called
Multi-player Receding-horizon Natural Policy Gradient (MRPG), where each team
minimizes its cumulative cost independently in a receding-horizon manner.
Despite the non-convexity of the problem, we establish that the resulting
algorithm converges to a global NE through a novel problem decomposition into
sub-problems using backward recursive discrete-time Hamilton-Jacobi-Isaacs
(HJI) equations, in which independent natural policy gradient is shown to
exhibit linear convergence under time-independent diagonal dominance.
Experiments illuminate the merits of this approach in practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An upper bound of the mutation probability in the genetic algorithm for
  general 0-1 knapsack problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As an important part of genetic algorithms (GAs), mutation operators is
widely used in evolutionary algorithms to solve $\mathcal{NP}$-hard problems
because it can increase the population diversity of individual. Due to
limitations in mathematical tools, the mutation probability of the mutation
operator is primarily empirically set in practical applications.
  In this paper, we propose a novel reduction method for the 0-1 knapsack
problem(0-1 KP) and an improved mutation operator (IMO) based on the assumption
$\mathcal{NP}\neq\mathcal{P}$, along with the utilization of linear relaxation
techniques and a recent result by Dey et al. (Math. Prog., pp 569-587, 2022).
We employ this method to calculate an upper bound of the mutation probability
in general instances of the 0-1 KP, and construct an instance where the
mutation probability does not tend towards 0 as the problem size increases.
Finally, we prove that the probability of the IMO hitting the optimal solution
within only a single iteration in large-scale instances is superior to that of
the traditional mutation operator.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Barely Random Algorithms for Metrical Task Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Romain Cosson, Laurent Massoulié
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider metrical task systems on general metric spaces with $n$ points,
and show that any fully randomized algorithm can be turned into a randomized
algorithm that uses only $2\log n$ random bits, and achieves the same
competitive ratio up to a factor $2$. This provides the first order-optimal
barely random algorithms for metrical task systems, i.e. which use a number of
random bits that does not depend on the number of requests addressed to the
system. We put forward an equivalent view that we call collective metrical task
systems where $k$ agents in a metrical task system team up, and suffer the
average cost paid by each agent. Our results imply that such team can be
$O(\log n^2)$-competitive, as soon as $k\geq n^2$ (in comparison, a single
agent is $\Omega(n)$-competitive at best). We discuss implications on various
aspects of online decision making such as: distributed systems, transaction
costs, and advice complexity, suggesting broad applicability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Mutual Insurance Model for Hedging Against Cyber Risks in Power
  Systems Deploying Smart Technologies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11054v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11054v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pikkin Lau, Lingfeng Wang, Wei Wei, Zhaoxi Liu, Chee-Wooi Ten
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, a novel cyber-insurance model design is proposed based on
system risk evaluation with smart technology applications. The cyber insurance
policy for power systems is tailored via cyber risk modeling, reliability
impact analysis, and insurance premium calculation. A stochastic Epidemic
Network Model is developed to evaluate the cyber risk by propagating
cyberattacks among graphical vulnerabilities. Smart technologies deployed in
risk modeling include smart monitoring and job thread assignment. Smart
monitoring boosts the substation availability against cyberattacks with
preventive and corrective measures. The job thread assignment solution reduces
the execution failures by distributing the control and monitoring tasks to
multiple threads. Reliability assessment is deployed to estimate load losses
convertible to monetary losses. These monetary losses would be shared through a
mutual insurance plan. To ensure a fair distribution of indemnity, a new
Shapley mutual insurance principle is devised. Effectiveness of the proposed
Shapley mutual insurance design is validated via case studies. The Shapley
premium is compared with existent premium designs. It is shown that the Shapley
premium has high indemnity levels closer to those of Tail Conditional
Expectation premium. Meanwhile, the Shapley premium is nearly as affordable as
the coalitional premium and keeps a relatively low insolvency probability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Power system reliability, cyber-insurance, power system security,
  cyber-physical systems, cyber risk modeling, actuarial design, tail risk</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Strategic Bidding Wars in On-chain Auctions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14510v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14510v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Wu, Thomas Thiery, Stefanos Leonardos, Carmine Ventre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Ethereum block-building process has changed significantly since the
emergence of Proposer-Builder Separation. Validators access blocks through a
marketplace, where block builders bid for the right to construct the block and
earn MEV (Maximal Extractable Value) rewards in an on-chain competition, known
as the MEV-boost auction. While more than 90% of blocks are currently built via
MEV-Boost, trade-offs between builders' strategic behaviors and auction design
remain poorly understood. In this paper we address this gap. We introduce a
game-theoretic model for MEV-Boost auctions and use simulations to study
different builders' bidding strategies observed in practice. We study various
strategic interactions and auction setups and evaluate how the interplay
between critical elements such as access to MEV opportunities and improved
connectivity to relays impact bidding performance. Our results demonstrate the
importance of latency on the effectiveness of builders' strategies and the
overall auction outcome from the proposer's perspective.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An upper bound of the mutation probability in the genetic algorithm for
  general 0-1 knapsack problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As an important part of genetic algorithms (GAs), mutation operators is
widely used in evolutionary algorithms to solve $\mathcal{NP}$-hard problems
because it can increase the population diversity of individual. Due to
limitations in mathematical tools, the mutation probability of the mutation
operator is primarily empirically set in practical applications.
  In this paper, we propose a novel reduction method for the 0-1 knapsack
problem(0-1 KP) and an improved mutation operator (IMO) based on the assumption
$\mathcal{NP}\neq\mathcal{P}$, along with the utilization of linear relaxation
techniques and a recent result by Dey et al. (Math. Prog., pp 569-587, 2022).
We employ this method to calculate an upper bound of the mutation probability
in general instances of the 0-1 KP, and construct an instance where the
mutation probability does not tend towards 0 as the problem size increases.
Finally, we prove that the probability of the IMO hitting the optimal solution
within only a single iteration in large-scale instances is superior to that of
the traditional mutation operator.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Objective Evolutionary Neural Architecture Search for Recurrent
  Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11173v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11173v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reinhard Booysen, Anna Sergeevna Bosman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial neural network (NN) architecture design is a nontrivial and
time-consuming task that often requires a high level of human expertise. Neural
architecture search (NAS) serves to automate the design of NN architectures and
has proven to be successful in automatically finding NN architectures that
outperform those manually designed by human experts. NN architecture
performance can be quantified based on multiple objectives, which include model
accuracy and some NN architecture complexity objectives, among others. The
majority of modern NAS methods that consider multiple objectives for NN
architecture performance evaluation are concerned with automated feed forward
NN architecture design, which leaves multi-objective automated recurrent neural
network (RNN) architecture design unexplored. RNNs are important for modeling
sequential datasets, and prominent within the natural language processing
domain. It is often the case in real world implementations of machine learning
and NNs that a reasonable trade-off is accepted for marginally reduced model
accuracy in favour of lower computational resources demanded by the model. This
paper proposes a multi-objective evolutionary algorithm-based RNN architecture
search method. The proposed method relies on approximate network morphisms for
RNN architecture complexity optimisation during evolution. The results show
that the proposed method is capable of finding novel RNN architectures with
comparable performance to state-of-the-art manually designed RNN architectures,
but with reduced computational demand.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Neural Crossover <span class="chip">GECCO 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11159v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11159v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eliad Shem-Tov, Achiya Elyasaf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel multi-parent crossover operator in genetic algorithms
(GAs) called ``Deep Neural Crossover'' (DNC). Unlike conventional GA crossover
operators that rely on a random selection of parental genes, DNC leverages the
capabilities of deep reinforcement learning (DRL) and an encoder-decoder
architecture to select the genes. Specifically, we use DRL to learn a policy
for selecting promising genes. The policy is stochastic, to maintain the
stochastic nature of GAs, representing a distribution for selecting genes with
a higher probability of improving fitness. Our architecture features a
recurrent neural network (RNN) to encode the parental genomes into latent
memory states, and a decoder RNN that utilizes an attention-based pointing
mechanism to generate a distribution over the next selected gene in the
offspring. To improve the training time, we present a pre-training approach,
wherein the architecture is initially trained on a single problem within a
specific domain and then applied to solving other problems of the same domain.
We compare DNC to known operators from the literature over two benchmark
domains -- bin packing and graph coloring. We compare with both two- and
three-parent crossover, outperforming all baselines. DNC is domain-independent
and can be easily applied to other problem domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 figures, 4 tables. Submitted to the Genetic and
  Evolutionary Computation Conference (GECCO 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Expansion in Pruned Recurrent Neural Network Layers Preserve
  Performance <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11100v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11100v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suryam Arnav Kalra, Arindam Biswas, Pabitra Mitra, Biswajit Basu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Expansion property of a graph refers to its strong connectivity as well as
sparseness. It has been reported that deep neural networks can be pruned to a
high degree of sparsity while maintaining their performance. Such pruning is
essential for performing real time sequence learning tasks using recurrent
neural networks in resource constrained platforms. We prune recurrent networks
such as RNNs and LSTMs, maintaining a large spectral gap of the underlying
graphs and ensuring their layerwise expansion properties. We also study the
time unfolded recurrent network graphs in terms of the properties of their
bipartite layers. Experimental results for the benchmark sequence MNIST,
CIFAR-10, and Google speech command data show that expander graph properties
are key to preserving classification accuracy of RNN and LSTM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as tiny paper in ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Expressive Leaky Memory Neuron: an Efficient and Expressive
  Phenomenological Neuron Model Can Solve Long-Horizon Tasks <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.16922v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.16922v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aaron Spieler, Nasim Rahaman, Georg Martius, Bernhard Schölkopf, Anna Levina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Biological cortical neurons are remarkably sophisticated computational
devices, temporally integrating their vast synaptic input over an intricate
dendritic tree, subject to complex, nonlinearly interacting internal biological
processes. A recent study proposed to characterize this complexity by fitting
accurate surrogate models to replicate the input-output relationship of a
detailed biophysical cortical pyramidal neuron model and discovered it needed
temporal convolutional networks (TCN) with millions of parameters. Requiring
these many parameters, however, could stem from a misalignment between the
inductive biases of the TCN and cortical neuron's computations. In light of
this, and to explore the computational implications of leaky memory units and
nonlinear dendritic processing, we introduce the Expressive Leaky Memory (ELM)
neuron model, a biologically inspired phenomenological model of a cortical
neuron. Remarkably, by exploiting such slowly decaying memory-like hidden
states and two-layered nonlinear integration of synaptic input, our ELM neuron
can accurately match the aforementioned input-output relationship with under
ten thousand trainable parameters. To further assess the computational
ramifications of our neuron design, we evaluate it on various tasks with
demanding temporal structures, including the Long Range Arena (LRA) datasets,
as well as a novel neuromorphic dataset based on the Spiking Heidelberg Digits
dataset (SHD-Adding). Leveraging a larger number of memory units with
sufficiently long timescales, and correspondingly sophisticated synaptic
integration, the ELM neuron displays substantial long-range processing
capabilities, reliably outperforming the classic Transformer or Chrono-LSTM
architectures on LRA, and even solving the Pathfinder-X task with over 70%
accuracy (16k context length).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 14 figures, 13 tables, additional experiments and
  clarifications, accepted to ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Amplitude-Ensemble Quantum-Inspired Tabu Search Algorithm for Solving
  0/1 Knapsack Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12867v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12867v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuo-Chun Tseng, Wei-Chieh Lai, I-Chia Chen, Yun-Hsiang Hsiao, Jr-Yu Chiue, Wei-Chun Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, an improved version of QTS (Quantum-inspired Tabu Search) has
been proposed, which enhances the utilization of population information, called
"amplitude-ensemble" QTS (AE-QTS). This makes AE-QTS more similar to the real
quantum search algorithm, Grover Search Algorithm, in abstract concept, while
keeping the simplicity of the algorithm. Later, we demonstrate the AE-QTS on
the classical combinatorial optimization 0/1 knapsack problem. Experimental
results show that the AE-QTS outperforms other algorithms, including the QTS,
by at least an average of 20% in all cases and even by 30% in some cases. Even
as the problem complexity increases, the quality of the solutions found by our
method remains superior to that of the QTS. These results prove that our method
has better search performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-16T00:00:00Z">2024-03-16</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computer Science and Game Theory
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Auctions with Dynamic Scoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11022v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11022v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martino Banchio, Aranyak Mehta, Andres Perlroth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the design of auctions with dynamic scoring, which allocate a single
item according to a given scoring rule. We are motivated by online advertising
auctions when users interact with a platform over the course of a session. The
platform ranks ads based on a combination of bids and quality scores, and
updates the quality scores throughout the session based on the user's online
activity. The platform must decide when to show an ad during the session. By
delaying the auction, the auctioneer acquires information about an ad's
quality, improving her chances of selecting a high quality ad. However
information is costly, because delay reduces market thickness and in turn
revenue. When should the auctioneer allocate the impression to balance these
forces?
  We develop a theoretical model to study the effect of market design on the
trade-off between market thickness and information. In particular, we focus on
first- and second-price auctions. The auctioneer can commit to the auction
format, but not to its timing: her decision can thus be cast as a real options
problem. We show that under optimal stopping the first-price auction allocates
efficiently but with delay. Instead, the second-price auction generates more
revenue by avoiding delay. The auctioneer benefits from introducing reserve
prices, more so in a first-price auction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inverse learning of black-box aggregator for robust Nash equilibrium 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10980v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10980v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanpu Chen, Gehui Xu, Fengxiang He, Dacheng Tao, Thomas Parisini, Karl Henrik Johansson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this note, we investigate the robustness of Nash equilibria (NE) in
multi-player aggregative games with coupling constraints. There are many
algorithms for computing an NE of an aggregative game given a known aggregator.
When the coupling parameters are affected by uncertainty, robust NE need to be
computed. We consider a scenario where players' weight in the aggregator is
unknown, making the aggregator kind of "a black box". We pursue a suitable
learning approach to estimate the unknown aggregator by proposing an inverse
variational inequality-based relationship. We then utilize the counterpart to
reconstruct the game and obtain first-order conditions for robust NE in the
worst case. Furthermore, we characterize the generalization property of the
learning methodology via an upper bound on the violation probability.
Simulation experiments show the effectiveness of the proposed inverse learning
approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mediator Interpretation and Faster Learning Algorithms for Linear
  Correlated Equilibria in General Extensive-Form <span class="highlight-title">Game</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15935v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15935v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian Hu Zhang, Gabriele Farina, Tuomas Sandholm
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A recent paper by Farina & Pipis (2023) established the existence of
uncoupled no-linear-swap regret dynamics with polynomial-time iterations in
extensive-form games. The equilibrium points reached by these dynamics, known
as linear correlated equilibria, are currently the tightest known relaxation of
correlated equilibrium that can be learned in polynomial time in any finite
extensive-form game. However, their properties remain vastly unexplored, and
their computation is onerous. In this paper, we provide several contributions
shedding light on the fundamental nature of linear-swap regret. First, we show
a connection between linear deviations and a generalization of communication
deviations in which the player can make queries to a "mediator" who replies
with action recommendations, and, critically, the player is not constrained to
match the timing of the game as would be the case for communication deviations.
We coin this latter set the untimed communication (UTC) deviations. We show
that the UTC deviations coincide precisely with the linear deviations, and
therefore that any player minimizing UTC regret also minimizes linear-swap
regret. We then leverage this connection to develop state-of-the-art no-regret
algorithms for computing linear correlated equilibria, both in theory and in
practice. In theory, our algorithms achieve polynomially better per-iteration
runtimes; in practice, our algorithms represent the state of the art by several
orders of magnitude.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FAGH: Accelerating Federated Learning with Approximated Global Hessian 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11041v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11041v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mrinmay Sen, A. K. Qin, Krishna Mohan C
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In federated learning (FL), the significant communication overhead due to the
slow convergence speed of training the global model poses a great challenge.
Specifically, a large number of communication rounds are required to achieve
the convergence in FL. One potential solution is to employ the Newton-based
optimization method for training, known for its quadratic convergence rate.
However, the existing Newton-based FL training methods suffer from either
memory inefficiency or high computational costs for local clients or the
server. To address this issue, we propose an FL with approximated global
Hessian (FAGH) method to accelerate FL training. FAGH leverages the first
moment of the approximated global Hessian and the first moment of the global
gradient to train the global model. By harnessing the approximated global
Hessian curvature, FAGH accelerates the convergence of global model training,
leading to the reduced number of communication rounds and thus the shortened
training time. Experimental results verify FAGH's effectiveness in decreasing
the number of communication rounds and the time required to achieve the
pre-specified objectives of the global model performance in terms of training
and test losses as well as test accuracy. Notably, FAGH outperforms several
state-of-the-art FL training methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multiplane Quantitative Phase Imaging Using a Wavelength-Multiplexed
  Diffractive Optical Processor 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11035v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11035v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Che-Yung Shen, Jingxi Li, Tianyi Gan, Yuhang Li, Langxing Bai, Mona Jarrahi, Aydogan Ozcan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantitative phase imaging (QPI) is a label-free technique that provides
optical path length information for transparent specimens, finding utility in
biology, materials science, and engineering. Here, we present quantitative
phase imaging of a 3D stack of phase-only objects using a
wavelength-multiplexed diffractive optical processor. Utilizing multiple
spatially engineered diffractive layers trained through deep learning, this
diffractive processor can transform the phase distributions of multiple 2D
objects at various axial positions into intensity patterns, each encoded at a
unique wavelength channel. These wavelength-multiplexed patterns are projected
onto a single field-of-view (FOV) at the output plane of the diffractive
processor, enabling the capture of quantitative phase distributions of input
objects located at different axial planes using an intensity-only image sensor.
Based on numerical simulations, we show that our diffractive processor could
simultaneously achieve all-optical quantitative phase imaging across several
distinct axial planes at the input by scanning the illumination wavelength. A
proof-of-concept experiment with a 3D-fabricated diffractive processor further
validated our approach, showcasing successful imaging of two distinct phase
objects at different axial positions by scanning the illumination wavelength in
the terahertz spectrum. Diffractive network-based multiplane QPI designs can
open up new avenues for compact on-chip phase imaging and sensing devices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 Pages, 9 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Looped Transformers are Better at Learning Learning Algorithms <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12424v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12424v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liu Yang, Kangwook Lee, Robert Nowak, Dimitris Papailiopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have demonstrated effectiveness in in-context solving
data-fitting problems from various (latent) models, as reported by Garg et al.
However, the absence of an inherent iterative structure in the transformer
architecture presents a challenge in emulating the iterative algorithms, which
are commonly employed in traditional machine learning methods. To address this,
we propose the utilization of looped transformer architecture and its
associated training methodology, with the aim of incorporating iterative
characteristics into the transformer architectures. Experimental results
suggest that the looped transformer achieves performance comparable to the
standard transformer in solving various data-fitting problems, while utilizing
less than 10% of the parameter count.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Runtime Analysis of Competitive co-Evolutionary Algorithms for Maximin
  Optimisation of a Bilinear Function 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.15238v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.15238v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Per Kristian Lehre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Co-evolutionary algorithms have a wide range of applications, such as in
hardware design, evolution of strategies for board games, and patching software
bugs. However, these algorithms are poorly understood and applications are
often limited by pathological behaviour, such as loss of gradient, relative
over-generalisation, and mediocre objective stasis. It is an open challenge to
develop a theory that can predict when co-evolutionary algorithms find
solutions efficiently and reliable.
  This paper provides a first step in developing runtime analysis for
population-based competitive co-evolutionary algorithms. We provide a
mathematical framework for describing and reasoning about the performance of
co-evolutionary processes. An example application of the framework shows a
scenario where a simple co-evolutionary algorithm obtains a solution in
polynomial expected time. Finally, we describe settings where the
co-evolutionary algorithm needs exponential time with overwhelmingly high
probability to obtain a solution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in Algorithmica</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Analysis and Fully Memristor-based Reservoir Computing for Temporal Data
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01827v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01827v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ankur Singh, Sanghyeon Choi, Gunuk Wang, Maryaradhiya Daimari, Byung-Geun Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reservoir computing (RC) offers a neuromorphic framework that is particularly
effective for processing spatiotemporal signals. Known for its temporal
processing prowess, RC significantly lowers training costs compared to
conventional recurrent neural networks. A key component in its hardware
deployment is the ability to generate dynamic reservoir states. Our research
introduces a novel dual-memory RC system, integrating a short-term memory via a
WOx-based memristor, capable of achieving 16 distinct states encoded over 4
bits, and a long-term memory component using a TiOx-based memristor within the
readout layer. We thoroughly examine both memristor types and leverage the RC
system to process temporal data sets. The performance of the proposed RC system
is validated through two benchmark tasks: isolated spoken digit recognition
with incomplete inputs and Mackey-Glass time series prediction. The system
delivered an impressive 98.84% accuracy in digit recognition and sustained a
low normalized root mean square error (NRMSE) of 0.036 in the time series
prediction task, underscoring its capability. This study illuminates the
adeptness of memristor-based RC systems in managing intricate temporal
challenges, laying the groundwork for further innovations in neuromorphic
computing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 20 figures, Journal, Typo corrected and updated reference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spike-based Neuromorphic Computing for Next-Generation Computer Vision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09692v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09692v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Sakib Hasan, Catherine D. Schuman, Zhongyang Zhang, Tauhidur Rahman, Garrett S. Rose
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neuromorphic Computing promises orders of magnitude improvement in energy
efficiency compared to traditional von Neumann computing paradigm. The goal is
to develop an adaptive, fault-tolerant, low-footprint, fast, low-energy
intelligent system by learning and emulating brain functionality which can be
realized through innovation in different abstraction layers including material,
device, circuit, architecture and algorithm. As the energy consumption in
complex vision tasks keep increasing exponentially due to larger data set and
resource-constrained edge devices become increasingly ubiquitous, spike-based
neuromorphic computing approaches can be viable alternative to deep
convolutional neural network that is dominating the vision field today. In this
book chapter, we introduce neuromorphic computing, outline a few representative
examples from different layers of the design stack (devices, circuits and
algorithms) and conclude with a few exciting applications and future research
directions that seem promising for computer vision in the near future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pending to be published as a book chapter in the book 'Computer
  Vision: Challenges, Trends, and Opportunities' from CRC Press</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rank-Based Learning and Local Model Based Evolutionary Algorithm for
  High-Dimensional Expensive Multi-Objective Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.09444v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.09444v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guodong Chen, Jiu Jimmy Jiao, Xiaoming Xue, Zhongzheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surrogate-assisted evolutionary algorithms have been widely developed to
solve complex and computationally expensive multi-objective optimization
problems in recent years. However, when dealing with high-dimensional
optimization problems, the performance of these surrogate-assisted
multi-objective evolutionary algorithms deteriorate drastically. In this work,
a novel Classifier-assisted rank-based learning and Local Model based
multi-objective Evolutionary Algorithm (CLMEA) is proposed for high-dimensional
expensive multi-objective optimization problems. The proposed algorithm
consists of three parts: classifier-assisted rank-based learning,
hypervolume-based non-dominated search, and local search in the relatively
sparse objective space. Specifically, a probabilistic neural network is built
as classifier to divide the offspring into a number of ranks. The offspring in
different ranks uses rank-based learning strategy to generate more promising
and informative candidates for real function evaluations. Then, radial basis
function networks are built as surrogates to approximate the objective
functions. After searching non-dominated solutions assisted by the surrogate
model, the candidates with higher hypervolume improvement are selected for real
evaluations. Subsequently, in order to maintain the diversity of solutions, the
most uncertain sample point from the non-dominated solutions measured by the
crowding distance is selected as the guided parent to further infill in the
uncertain region of the front. The experimental results of benchmark problems
and a real-world application on geothermal reservoir heat extraction
optimization demonstrate that the proposed algorithm shows superior performance
compared with the state-of-the-art surrogate-assisted multi-objective
evolutionary algorithms. The source code for this work is available at
https://github.com/JellyChen7/CLMEA.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-15T00:00:00Z">2024-03-15</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computer Science and Game Theory
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sub<span class="highlight-title">game</span> Optimal and Prior-Independent Online Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jason Hartline, Aleck Johnsen, Anant Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper takes a game theoretic approach to the design and analysis of
online algorithms and illustrates the approach on the finite-horizon ski-rental
problem. This approach allows beyond worst-case analysis of online algorithms.
First, we define "subgame optimality" which is stronger than worst case
optimality in that it requires the algorithm to take advantage of an adversary
not playing a worst case input. Algorithms only focusing on the worst case can
be far from subgame optimal. Second, we consider prior-independent design and
analysis of online algorithms, where rather than choosing a worst case input,
the adversary chooses a worst case independent and identical distribution over
inputs. Prior-independent online algorithms are generally analytically
intractable; instead we give a fully polynomial time approximation scheme to
compute them. Highlighting the potential improvement from these paradigms for
the finite-horizon ski-rental problem, we empirically compare worst-case,
subgame optimal, and prior-independent algorithms in the prior-independent
framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 main pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Coordination in Noncooperative Multiplayer Matrix <span class="highlight-title">Game</span>s via Reduced Rank
  Correlated Equilibria 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10384v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10384v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaehan Im, Yue Yu, David Fridovich-Keil, Ufuk Topcu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Coordination in multiplayer games enables players to avoid the lose-lose
outcome that often arises at Nash equilibria. However, designing a coordination
mechanism typically requires the consideration of the joint actions of all
players, which becomes intractable in large-scale games. We develop a novel
coordination mechanism, termed reduced rank correlated equilibria, which
reduces the number of joint actions to be considered and thereby mitigates
computational complexity. The idea is to approximate the set of all joint
actions with the actions used in a set of pre-computed Nash equilibria via a
convex hull operation. In a game with n players and each player having m
actions, the proposed mechanism reduces the number of joint actions considered
from O(m^n) to O(mn). We demonstrate the application of the proposed mechanism
to an air traffic queue management problem. Compared with the correlated
equilibrium-a popular benchmark coordination mechanism-the proposed approach is
capable of solving a queue management problem involving four thousand times
more joint actions. In the meantime, it yields a solution that shows a 58.5% to
99.5% improvement in the fairness indicator and a 1.8% to 50.4% reduction in
average delay cost compared to the Nash solution, which does not involve
coordination.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling <span class="highlight-title">Game</span>-Theoretic Security Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10310v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10310v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sophie Rain, Lea Salome Brugger, Anja Petkovic Komel, Laura Kovacs, Michael Rawson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the CheckMate tool for automated verification of game-theoretic
security properties, with application to blockchain protocols. CheckMate
applies automated reasoning techniques to determine whether a game-theoretic
protocol model is game-theoretically secure, that is, Byzantine fault tolerant
and incentive compatible. We describe CheckMate's input format and its various
components, modes, and output. CheckMate is evaluated on 14 benchmarks,
including models of decentralized protocols, board games, and game-theoretic
examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DDPS: Dynamic Differential Pricing-based Edge Offloading System with
  Energy Harvesting Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09991v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09991v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hai Xue, Yun Xia, Neal N. Xiong, Di Zhang, Songwen Pei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mobile edge computing (MEC) paves the way to alleviate the burden of energy
and computation of mobile users (MUs) by offloading tasks to the network edge.
To enhance the MEC server utilization by optimizing its resource allocation, a
well-designed pricing strategy is indispensable. In this paper, we consider the
edge offloading scenario with energy harvesting devices, and propose a dynamic
differential pricing system (DDPS), which determines the price per unit time
according to the usage of computing resources to improve the edge server
utilization. Firstly, we propose an offloading decision algorithm to decide
whether to conduct the offloading operation and how much data to be offloaded
if conducted, the algorithm determines offloading operation by balancing the
energy harvested with the energy consumed. Secondly, for the offloading case,
we formulate the game between the MUs and the server as a Stackelberg game, and
propose a differential pricing algorithm to determine the optimal computing
resources required by MUs. Furthermore, the proposed algorithm also reallocates
computing resources for delay-sensitive devices while server resources are
surplus after the initial allocation, aiming to make full use of the server
computing resources. Extensive simulations are conducted to demonstrate the
effectiveness of the proposed DDPS scheme.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Single-<span class="highlight-title">Sample</span> Prophet Inequalities via Greedy-Ordered <span class="highlight-title">Selection</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.03174v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.03174v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Constantine Caramanis, Paul Dütting, Matthew Faw, Federico Fusco, Philip Lazos, Stefano Leonardi, Orestis Papadigenopoulos, Emmanouil Pountourakis, Rebecca Reiffenhäuser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study single-sample prophet inequalities (SSPIs), i.e., prophet
inequalities where only a single sample from each prior distribution is
available. Besides a direct, and optimal, SSPI for the basic single choice
problem [Rubinstein et al., 2020], most existing SSPI results were obtained via
an elegant, but inherently lossy, reduction to order-oblivious secretary (OOS)
policies [Azar et al., 2014]. Motivated by this discrepancy, we develop an
intuitive and versatile greedy-based technique that yields SSPIs directly
rather than through the reduction to OOSs. Our results can be seen as
generalizing and unifying a number of existing results in the area of prophet
and secretary problems. Our algorithms significantly improve on the competitive
guarantees for a number of interesting scenarios (including general matching
with edge arrivals, bipartite matching with vertex arrivals, and certain
matroids), and capture new settings (such as budget additive combinatorial
auctions). Complementing our algorithmic results, we also consider mechanism
design variants. Finally, we analyze the power and limitations of different
SSPI approaches by providing a partial converse to the reduction from SSPI to
OOS given by Azar et al.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Merges and extends arXiv:2103.13089 [cs.GT] and arXiv:2104.02050
  [cs.DS]</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Restricting Entries to All-Pay Contests 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.08104v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.08104v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fupeng Sun, Yanwei Sun, Chiwei Yan, Li Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study an all-pay contest where players with low abilities are filtered
prior to the round of competing for prizes. These are often practiced due to
limited resources or to enhance the competitiveness of the contest. We consider
a setting where the designer admits a certain number of top players into the
contest. The players admitted into the contest update their beliefs about their
opponents based on the signal that their abilities are among the top. We find
that their posterior beliefs, even with IID priors, are correlated and depend
on players' private abilities, representing a unique feature of this game. We
explicitly characterize the symmetric and unique Bayesian equilibrium strategy.
We find that each admitted player's equilibrium effort is in general not
monotone with the number of admitted players. Despite this non-monotonicity,
surprisingly, all players exert their highest efforts when all players are
admitted. This result holds generally -- it is true under any ranking-based
prize structure, ability distribution, and cost function. We also discuss a
two-stage extension where players with top first-stage efforts can proceed to
the second stage competing for prizes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Price-Discrimination <span class="highlight-title">Game</span> for Distributed Resource Management in
  Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.13838v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.13838v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Zhang, Halvin Yang, Guopeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In vanilla federated learning (FL) such as FedAvg, the parameter server (PS)
and multiple distributed clients can form a typical buyer's market, where the
number of PS/buyers of FL services is far less than the number of
clients/sellers. In order to improve the performance of FL and reduce the cost
of motivating clients to participate in FL, this paper proposes to
differentiate the pricing for services provided by different clients rather
than simply providing the same service pricing for different clients. The price
is differentiated based on the performance improvements brought to FL and their
heterogeneity in computing and communication capabilities. To this end, a
price-discrimination game (PDG) is formulated to comprehensively address the
distributed resource management problems in FL, including multi-objective
trade-off, client selection, and incentive mechanism. As the PDG is a
mixed-integer nonlinear programming (MINLP) problem, a distributed
semi-heuristic algorithm with low computational complexity and low
communication overhead is designed to solve it. The simulation result verifies
the effectiveness of the proposed approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stable Matching <span class="highlight-title">Game</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2008.01680v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2008.01680v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felipe Garrido-Lucero, Rida Laraki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gale and Shapley introduced a matching problem between two sets of agents
where each agent on one side has an exogenous preference ordering over the
agents on the other side. They defined a matching as stable if no unmatched
pair can both improve their utility by forming a new pair. They proved,
algorithmically, the existence of a stable matching. Shapley and Shubik,
Demange and Gale, and many others extended the model by allowing monetary
transfers. We offer a further extension by assuming that matched couples obtain
their payoff endogenously as the outcome of a strategic game they have to play
in a usual non-cooperative sense (without commitment) or in a semi-cooperative
way (with commitment, as the outcome of a bilateral binding contract in which
each player is responsible for her part of the contract). Depending on whether
the players can commit or not, we define in each case a solution concept that
combines Gale-Shapley pairwise stability with a (generalized) Nash equilibrium
stability. In each case we give necessary and sufficient conditions for the set
of solutions to be non-empty and provide an algorithm to compute a solution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data Monetization Pathways and Complex Dynamic <span class="highlight-title">Game</span> Equilibrium Analysis
  in the Energy Industry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08082v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08082v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongxian Wang, Jie Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the most critical production factor in the era of the digital economy,
data will have a significant impact on social production and development.
Energy enterprises possess data that is interconnected with multiple
industries, characterized by diverse needs, sensitivity, and long-term nature.
The path to monetizing energy enterprises' data is challenging yet crucial.
This paper explores the game-theoretic aspects of the data monetization process
in energy enterprises by considering the relationships between enterprises and
trading platforms. We construct a class of game decision models and study their
equilibrium strategies. Our analysis shows that enterprises and platforms can
adjust respective benefits by regulating the wholesale price of data and the
intensity of data value mining to form a benign equilibrium state. Furthermore,
by integrating nonlinear dynamical theory, we discuss the dynamic
characteristics present in multi-period repeated game processes. We find that
decision-makers should keep the adjustment parameters and initial states within
reasonable ranges in multi-period dynamic decision-making to avoid market
failure. Finally, based on the theoretical and numerical analysis, we provide
decision insights and recommendations for enterprise decision-making to
facilitate data monetization through strategic interactions with trading
platforms.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved discrete particle swarm optimization using Bee Algorithm and
  multi-parent crossover method (Case study: Allocation problem and benchmark
  functions) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10684v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10684v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamed Zibaei, Mohammad Saadi Mesgari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compared to other techniques, particle swarm optimization is more frequently
utilized because of its ease of use and low variability. However, it is
complicated to find the best possible solution in the search space in
large-scale optimization problems. Moreover, changing algorithm variables does
not influence algorithm convergence much. The PSO algorithm can be combined
with other algorithms. It can use their advantages and operators to solve this
problem. Therefore, this paper proposes the onlooker multi-parent crossover
discrete particle swarm optimization (OMPCDPSO). To improve the efficiency of
the DPSO algorithm, we utilized multi-parent crossover on the best solutions.
We performed an independent and intensive neighborhood search using the
onlooker bees of the bee algorithm. The algorithm uses onlooker bees and
crossover. They do local search (exploitation) and global search (exploration).
Each of these searches is among the best solutions (employed bees). The
proposed algorithm was tested on the allocation problem, which is an NP-hard
optimization problem. Also, we used two types of simulated data. They were used
to test the scalability and complexity of the better algorithm. Also, fourteen
2D test functions and thirteen 30D test functions were used. They also used
twenty IEEE CEC2005 benchmark functions to test the efficiency of OMPCDPSO.
Also, to test OMPCDPSO's performance, we compared it to four new binary
optimization algorithms and three classic ones. The results show that the
OMPCDPSO version had high capability. It performed better than other
algorithms. The developed algorithm in this research (OMCDPSO) in 36 test
functions out of 47 (76.60%) is better than other algorithms. The Onlooker bees
and multi-parent operators significantly impact the algorithm's performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 8 figures, 15 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Single- and Multi-<span class="highlight-title">Agent</span> Private Active Sensing: A Deep Neuroevolution
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10112v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10112v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George Stamatelis, Angelos-Nikolaos Kanatas, Ioannis Asprogerakas, George C. Alexandropoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we focus on one centralized and one decentralized problem of
active hypothesis testing in the presence of an eavesdropper. For the
centralized problem including a single legitimate agent, we present a new
framework based on NeuroEvolution (NE), whereas, for the decentralized problem,
we develop a novel NE-based method for solving collaborative multi-agent tasks,
which interestingly maintains all computational benefits of single-agent NE.
The superiority of the proposed EAHT approaches over conventional active
hypothesis testing policies, as well as learning-based methods, is validated
through numerical investigations in an example use case of anomaly detection
over wireless sensor networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures, accepted at IEEE ICC 2024 (to be presented)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Multiplayer Battle <span class="highlight-title">Game</span> Optimizer for Adversarial Robust
  Neural Architecture Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10100v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10100v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Zhong, Yuefeng Xu, Chao Zhang, Jun Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel metaheuristic algorithm, known as the efficient
multiplayer battle game optimizer (EMBGO), specifically designed for addressing
complex numerical optimization tasks. The motivation behind this research stems
from the need to rectify identified shortcomings in the original MBGO,
particularly in search operators during the movement phase, as revealed through
ablation experiments. EMBGO mitigates these limitations by integrating the
movement and battle phases to simplify the original optimization framework and
improve search efficiency. Besides, two efficient search operators:
differential mutation and L\'evy flight are introduced to increase the
diversity of the population. To evaluate the performance of EMBGO
comprehensively and fairly, numerical experiments are conducted on benchmark
functions such as CEC2017, CEC2020, and CEC2022, as well as engineering
problems. Twelve well-established MA approaches serve as competitor algorithms
for comparison. Furthermore, we apply the proposed EMBGO to the complex
adversarial robust neural architecture search (ARNAS) tasks and explore its
robustness and scalability. The experimental results and statistical analyses
confirm the efficiency and effectiveness of EMBGO across various optimization
tasks. As a potential optimization technique, EMBGO holds promise for diverse
applications in real-world problems and deep learning scenarios. The source
code of EMBGO is made available in
\url{https://github.com/RuiZhong961230/EMBGO}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neuroformer: Multimodal and Multitask Generative Pretraining for Brain
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.00136v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.00136v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonis Antoniades, Yiyi Yu, Joseph Canzano, William Wang, Spencer LaVere Smith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art systems neuroscience experiments yield large-scale
multimodal data, and these data sets require new tools for analysis. Inspired
by the success of large pretrained models in vision and language domains, we
reframe the analysis of large-scale, cellular-resolution neuronal spiking data
into an autoregressive spatiotemporal generation problem. Neuroformer is a
multimodal, multitask generative pretrained transformer (GPT) model that is
specifically designed to handle the intricacies of data in systems
neuroscience. It scales linearly with feature size, can process an arbitrary
number of modalities, and is adaptable to downstream tasks, such as predicting
behavior. We first trained Neuroformer on simulated datasets, and found that it
both accurately predicted simulated neuronal circuit activity, and also
intrinsically inferred the underlying neural circuit connectivity, including
direction. When pretrained to decode neural responses, the model predicted the
behavior of a mouse with only few-shot fine-tuning, suggesting that the model
begins learning how to do so directly from the neural representations
themselves, without any explicit supervision. We used an ablation study to show
that joint training on neuronal responses and behavior boosted performance,
highlighting the model's ability to associate behavioral and neural
representations in an unsupervised manner. These findings show that Neuroformer
can analyze neural datasets and their emergent properties, informing the
development of models and hypotheses associated with the brain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages for main paper. 22 pages in total. 13 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FEDORA: Flying Event <span class="highlight-title">Dataset</span> fOr Reactive behAvior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14392v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14392v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amogh Joshi, Adarsh Kosta, Wachirawit Ponghiran, Manish Nagaraj, Kaushik Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability of resource-constrained biological systems such as fruitflies to
perform complex and high-speed maneuvers in cluttered environments has been one
of the prime sources of inspiration for developing vision-based autonomous
systems. To emulate this capability, the perception pipeline of such systems
must integrate information cues from tasks including optical flow and depth
estimation, object detection and tracking, and segmentation, among others.
However, the conventional approach of employing slow, synchronous inputs from
standard frame-based cameras constrains these perception capabilities,
particularly during high-speed maneuvers. Recently, event-based sensors have
emerged as low latency and low energy alternatives to standard frame-based
cameras for capturing high-speed motion, effectively speeding up perception and
hence navigation. For coherence, all the perception tasks must be trained on
the same input data. However, present-day datasets are curated mainly for a
single or a handful of tasks and are limited in the rate of the provided ground
truths. To address these limitations, we present Flying Event Dataset fOr
Reactive behAviour (FEDORA) - a fully synthetic dataset for perception tasks,
with raw data from frame-based cameras, event-based cameras, and Inertial
Measurement Units (IMU), along with ground truths for depth, pose, and optical
flow at a rate much higher than existing datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Expressivity of Spiking Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.08218v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.08218v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manjot Singh, Adalbert Fono, Gitta Kutyniok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The synergy between spiking neural networks and neuromorphic hardware holds
promise for the development of energy-efficient AI applications. Inspired by
this potential, we revisit the foundational aspects to study the capabilities
of spiking neural networks where information is encoded in the firing time of
neurons. Under the Spike Response Model as a mathematical model of a spiking
neuron with a linear response function, we compare the expressive power of
artificial and spiking neural networks, where we initially show that they
realize piecewise linear mappings. In contrast to ReLU networks, we prove that
spiking neural networks can realize both continuous and discontinuous
functions. Moreover, we provide complexity bounds on the size of spiking neural
networks to emulate multi-layer (ReLU) neural networks. Restricting to the
continuous setting, we also establish complexity bounds in the reverse
direction for one-layer spiking neural networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Analyzing the Expected Hitting Time of Evolutionary Computation-based
  Neural Architecture Search Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.05397v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.05397v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeqiong Lv, Chao Qian, Gary G. Yen, Yanan Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evolutionary computation-based neural architecture search (ENAS) is a popular
technique for automating architecture design of deep neural networks. Despite
its groundbreaking applications, there is no theoretical study for ENAS. The
expected hitting time (EHT) is one of the most important theoretical issues,
since it implies the average computational time complexity. This paper proposes
a general method by integrating theory and experiment for estimating the EHT of
ENAS algorithms, which includes common configuration, search space partition,
transition probability estimation, population distribution fitting, and hitting
time analysis. By exploiting the proposed method, we consider the
($\lambda$+$\lambda$)-ENAS algorithms with different mutation operators and
estimate the lower bounds of the EHT. Furthermore, we study the EHT on the
NAS-Bench-101 problem, and the results demonstrate the validity of the proposed
method. To the best of our knowledge, this work is the first attempt to
establish a theoretical foundation for ENAS algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Traveling Waves Encode the Recent Past and Enhance Sequence Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08045v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08045v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        T. Anderson Keller, Lyle Muller, Terrence Sejnowski, Max Welling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traveling waves of neural activity have been observed throughout the brain at
a diversity of regions and scales; however, their precise computational role is
still debated. One physically inspired hypothesis suggests that the cortical
sheet may act like a wave-propagating system capable of invertibly storing a
short-term memory of sequential stimuli through induced waves traveling across
the cortical surface, and indeed many experimental results from neuroscience
correlate wave activity with memory tasks. To date, however, the computational
implications of this idea have remained hypothetical due to the lack of a
simple recurrent neural network architecture capable of exhibiting such waves.
In this work, we introduce a model to fill this gap, which we denote the
Wave-RNN (wRNN), and demonstrate how such an architecture indeed efficiently
encodes the recent past through a suite of synthetic memory tasks where wRNNs
learn faster and reach significantly lower error than wave-free counterparts.
We further explore the implications of this memory storage system on more
complex sequence modeling tasks such as sequential image classification and
find that wave-based models not only again outperform comparable wave-free RNNs
while using significantly fewer parameters, but additionally perform comparably
to more complex gated architectures such as LSTMs and GRUs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep learning in a bilateral brain with hemispheric specialization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.06862v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.06862v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chandramouli Rajagopalan, David Rawlinson, Elkhonon Goldberg, Gideon Kowadlo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The brains of all bilaterally symmetric animals on Earth are divided into
left and right hemispheres. The anatomy and functionality of the hemispheres
have a large degree of overlap, but there are asymmetries and they specialise
to possess different attributes. Other authors have used computational models
to mimic hemispheric asymmetries with a focus on reproducing human data on
semantic and visual processing tasks. We took a different approach and aimed to
understand how dual hemispheres in a bilateral architecture interact to perform
well in a given task. We propose a bilateral artificial neural network that
imitates lateralisation observed in nature: that the left hemisphere
specialises in specificity and the right in generality. We used different
training objectives to achieve the desired specialisation and tested it on an
image classification task with two different CNN backbones -- ResNet and VGG.
Our analysis found that the hemispheres represent complementary features that
are exploited by a network head which implements a type of weighted attention.
The bilateral architecture outperformed a range of baselines of similar
representational capacity that don't exploit differential specialisation, with
the exception of a conventional ensemble of unilateral networks trained on a
dual training objective for specifics and generalities. The results demonstrate
the efficacy of bilateralism, contribute to the discussion of bilateralism in
biological brains and the principle may serves as an inductive bias for new AI
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-14T00:00:00Z">2024-03-14</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computer Science and Game Theory
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Symbiotic <span class="highlight-title">Game</span> and Foundation Models for Cyber Deception Operations in
  Strategic Cyber Warfare 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10570v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10570v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Li, Quanyan Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We are currently facing unprecedented cyber warfare with the rapid evolution
of tactics, increasing asymmetry of intelligence, and the growing accessibility
of hacking tools. In this landscape, cyber deception emerges as a critical
component of our defense strategy against increasingly sophisticated attacks.
This chapter aims to highlight the pivotal role of game-theoretic models and
foundation models (FMs) in analyzing, designing, and implementing cyber
deception tactics. Game models (GMs) serve as a foundational framework for
modeling diverse adversarial interactions, allowing us to encapsulate both
adversarial knowledge and domain-specific insights. Meanwhile, FMs serve as the
building blocks for creating tailored machine learning models suited to given
applications. By leveraging the synergy between GMs and FMs, we can advance
proactive and automated cyber defense mechanisms by not only securing our
networks against attacks but also enhancing their resilience against
well-planned operations. This chapter discusses the games at the tactical,
operational, and strategic levels of warfare, delves into the symbiotic
relationship between these methodologies, and explores relevant applications
where such a framework can make a substantial impact in cybersecurity. The
chapter discusses the promising direction of the multi-agent neurosymbolic
conjectural learning (MANSCOL), which allows the defender to predict
adversarial behaviors, design adaptive defensive deception tactics, and
synthesize knowledge for the operational level synthesis and adaptation. FMs
serve as pivotal tools across various functions for MANSCOL, including
reinforcement learning, knowledge assimilation, formation of conjectures, and
contextual representation. This chapter concludes with a discussion of the
challenges associated with FMs and their application in the domain of
cybersecurity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Query Complexity of Contracts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Dütting, Michal Feldman, Yoav Gal-Tzur, Aviad Rubinstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Algorithmic contract design is a new frontier in the intersection of
economics and computation, with combinatorial contracts being a core problem in
this domain. A central model within combinatorial contracts explores a setting
where a principal delegates the execution of a task, which can either succeed
or fail, to an agent. The agent can choose any subset among a given set of
costly actions, where every subset is associated with a success probability.
The principal incentivizes the agent through a contract that specifies the
payment upon success of the task.
  A natural setting of interest is one with submodular success probabilities.
It is known that finding the optimal contract for the principal is
$\mathsf{NP}$-hard, but the hardness result is derived from the hardness of
demand queries. A major open problem is whether the hardness arises solely from
the hardness of demand queries, or if the complexity lies within the optimal
contract problem itself. In other words: does the problem retain its hardness,
even when provided access to a demand oracle? We resolve this question in the
affirmative, showing that any algorithm that computes the optimal contract for
submodular success probabilities requires an exponential number of demand
queries, thus settling the query complexity problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sequential Contracts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomer Ezra, Michal Feldman, Maya Schlesinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the principal-agent setting, where a principal delegates the
execution of a costly project to an agent. In the classical model, the agent
chooses an action among a set of available actions. Every action is associated
with some cost, and leads to a stochastic outcome for the project. The agent's
action is hidden from the principal, who only observes the outcome. The
principal incentivizes the agent through a payment scheme (a contract) that
maps outcomes to payments, with the objective of finding the optimal contract -
the contract maximizing the principal's expected utility.
  In this work, we introduce a sequential variant of the model, capturing many
real-life settings, where the agent engages in multiple attempts, incurring the
sum of costs of the actions taken and being compensated for the best realized
outcome. We study the contract design problem in this new setting. We first
observe that the agent's problem - finding the sequential set of actions that
maximizes his utility for a given contract - is equivalent to the well-known
Pandora's Box problem. With this insight at hand, we provide algorithms and
hardness results for the (principal's) contract design problem, under both
independent and correlated actions. For independent actions, we show that the
optimal linear contract can be computed in polynomial time. Furthermore, this
result extends to the optimal arbitrary contract when the number of outcomes is
a constant. For correlated actions we find that approximating the optimal
contract within any constant ratio is NP-hard.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trust AI Regulation? Discerning users are vital to build trust and
  effective AI regulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09510v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09510v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zainab Alalawi, Paolo Bova, Theodor Cimpeanu, Alessandro Di Stefano, Manh Hong Duong, Elias Fernandez Domingos, The Anh Han, Marcus Krellner, Bianca Ogbo, Simon T. Powers, Filippo Zimmaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is general agreement that some form of regulation is necessary both for
AI creators to be incentivised to develop trustworthy systems, and for users to
actually trust those systems. But there is much debate about what form these
regulations should take and how they should be implemented. Most work in this
area has been qualitative, and has not been able to make formal predictions.
Here, we propose that evolutionary game theory can be used to quantitatively
model the dilemmas faced by users, AI creators, and regulators, and provide
insights into the possible effects of different regulatory regimes. We show
that creating trustworthy AI and user trust requires regulators to be
incentivised to regulate effectively. We demonstrate the effectiveness of two
mechanisms that can achieve this. The first is where governments can recognise
and reward regulators that do a good job. In that case, if the AI system is not
too risky for users then some level of trustworthy development and user trust
evolves. We then consider an alternative solution, where users can condition
their trust decision on the effectiveness of the regulators. This leads to
effective regulation, and consequently the development of trustworthy AI and
user trust, provided that the cost of implementing regulations is not too high.
Our findings highlight the importance of considering the effect of different
regulatory regimes from an evolutionary game theoretic perspective.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ All-pay Auction Based Profit Maximization in End-to-End Computation
  Offloading System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09129v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09129v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hai Xue, Yun Xia, Di Zhang, Honghua Wei, Xiaolong Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pricing is an important issue in mobile edge computing. How to appropriately
determine the bid of end user (EU) is an incentive factor for edge cloud (EC)
to offer service. In this letter, we propose an equilibrium pricing scheme
based on the all-pay auction model in end-to-end collaboration environment,
wherein all EUs can acquire the service at a lower price than the own value of
the required resource. In addition, we propose a set allocation algorithm to
divide all the bidders into different sets according to the price, and the EUs
in each set get the service, which averts the case of getting no service due to
the low price. Extensive simulation results demonstrate that the proposed
scheme can effectively maximize the total profit of the edge offloading system,
and guarantee all EUs can access the service.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Prosumers Participation in Markets: A Scalar-Parameterized Function
  Bidding Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.15423v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.15423v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdullah Alawad, Muhammad Aneeq uz Zaman, Khaled Alshehri, Tamer Başar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In uniform-price markets, suppliers compete to supply a resource to
consumers, resulting in a single market price determined by their competition.
For sufficient flexibility, producers and consumers prefer to commit to a
function as their strategies, indicating their preferred quantity at any given
market price. Producers and consumers may wish to act as both, i.e., prosumers.
In this paper, we examine the behavior of profit-maximizing prosumers in a
uniform-price market for resource allocation with the objective of maximizing
the social welfare. We propose a scalar-parameterized function bidding
mechanism for the prosumers, in which we establish the existence and uniqueness
of Nash equilibrium. Furthermore, we provide an efficient way to compute the
Nash equilibrium through the computation of the market allocation at the Nash
equilibrium. Finally, we present a case study to illustrate the welfare loss
under different variations of market parameters, such as the market's supply
capacity and inelastic demand.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Corrected typos in the figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Approximating Nash Equilibria in Normal-Form <span class="highlight-title">Game</span>s via Stochastic
  Optimization <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06689v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06689v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ian Gemp, Luke Marris, Georgios Piliouras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose the first loss function for approximate Nash equilibria of
normal-form games that is amenable to unbiased Monte Carlo estimation. This
construction allows us to deploy standard non-convex stochastic optimization
techniques for approximating Nash equilibria, resulting in novel algorithms
with provable guarantees. We complement our theoretical analysis with
experiments demonstrating that stochastic gradient descent can outperform
previous state-of-the-art approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diversity-seeking Jump <span class="highlight-title">Game</span>s in Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.17757v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.17757v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lata Narayanan, Yasaman Sabbagh, Alexandros A. Voudouris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, strategic games inspired by Schelling's influential model of
residential segregation have been studied in the TCS and AI literature. In
these games, agents of k different types occupy the nodes of a network topology
aiming to maximize their utility, which is a function of the fraction of
same-type agents they are adjacent to in the network. As such, the agents
exhibit similarity seeking strategic behavior. In this paper, we introduce a
class of strategic jump games in which the agents are diversity-seeking: The
utility of an agent is defined as the fraction of its neighbors that are of
different type than itself. We show that in general it is computationally hard
to determine the existence of an equilibrium in such games. However, when the
network is a tree, diversity-seeking jump games always admit an equilibrium
assignment. For regular graphs and spider graphs with a single empty node, we
prove a stronger result: The game is potential, that is, the improving response
dynamics always converge to an equilibrium from any initial placement of the
agents. We also show (nearly tight) bounds on the price of anarchy and price of
stability in terms of the social welfare (the total utility of the agents).
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emotional Intelligence Through Artificial Intelligence : NLP and Deep
  Learning in the Analysis of Healthcare Texts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09762v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09762v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prashant Kumar Nag, Amit Bhagat, R. Vishnu Priya, Deepak kumar Khare
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This manuscript presents a methodical examination of the utilization of
Artificial Intelligence in the assessment of emotions in texts related to
healthcare, with a particular focus on the incorporation of Natural Language
Processing and deep learning technologies. We scrutinize numerous research
studies that employ AI to augment sentiment analysis, categorize emotions, and
forecast patient outcomes based on textual information derived from clinical
narratives, patient feedback on medications, and online health discussions. The
review demonstrates noteworthy progress in the precision of algorithms used for
sentiment classification, the prognostic capabilities of AI models for
neurodegenerative diseases, and the creation of AI-powered systems that offer
support in clinical decision-making. Remarkably, the utilization of AI
applications has exhibited an enhancement in personalized therapy plans by
integrating patient sentiment and contributing to the early identification of
mental health disorders. There persist challenges, which encompass ensuring the
ethical application of AI, safeguarding patient confidentiality, and addressing
potential biases in algorithmic procedures. Nevertheless, the potential of AI
to revolutionize healthcare practices is unmistakable, offering a future where
healthcare is not only more knowledgeable and efficient but also more
empathetic and centered around the needs of patients. This investigation
underscores the transformative influence of AI on healthcare, delivering a
comprehensive comprehension of its role in examining emotional content in
healthcare texts and highlighting the trajectory towards a more compassionate
approach to patient care. The findings advocate for a harmonious synergy
between AI's analytical capabilities and the human aspects of healthcare.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a theory of model distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09053v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09053v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enric Boix-Adsera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distillation is the task of replacing a complicated machine learning model
with a simpler model that approximates the original [BCNM06,HVD15]. Despite
many practical applications, basic questions about the extent to which models
can be distilled, and the runtime and amount of data needed to distill, remain
largely open.
  To study these questions, we initiate a general theory of distillation,
defining PAC-distillation in an analogous way to PAC-learning [Val84]. As
applications of this theory: (1) we propose new algorithms to extract the
knowledge stored in the trained weights of neural networks -- we show how to
efficiently distill neural networks into succinct, explicit decision tree
representations when possible by using the ``linear representation
hypothesis''; and (2) we prove that distillation can be much cheaper than
learning from scratch, and make progress on characterizing its complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>47 pages, 5 figures. Please reach out with comments! Feedback is
  welcome</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FlexNN: A Dataflow-aware Flexible Deep Learning Accelerator for
  Energy-Efficient Edge Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09026v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09026v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arnab Raha, Deepak A. Mathaikutty, Soumendu K. Ghosh, Shamik Kundu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces FlexNN, a Flexible Neural Network accelerator, which
adopts agile design principles to enable versatile dataflows, enhancing energy
efficiency. Unlike conventional convolutional neural network accelerator
architectures that adhere to fixed dataflows (such as input, weight, output, or
row stationary) for transferring activations and weights between storage and
compute units, our design revolutionizes by enabling adaptable dataflows of any
type through software configurable descriptors. Considering that data movement
costs considerably outweigh compute costs from an energy perspective, the
flexibility in dataflow allows us to optimize the movement per layer for
minimal data transfer and energy consumption, a capability unattainable in
fixed dataflow architectures. To further enhance throughput and reduce energy
consumption in the FlexNN architecture, we propose a novel sparsity-based
acceleration logic that utilizes fine-grained sparsity in both the activation
and weight tensors to bypass redundant computations, thus optimizing the
convolution engine within the hardware accelerator. Extensive experimental
results underscore a significant enhancement in the performance and energy
efficiency of FlexNN relative to existing DNN accelerators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Version 0. Work started in 2019</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Smooth Tchebycheff Scalarization for Multi-Objective Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.19078v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.19078v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi Lin, Xiaoyuan Zhang, Zhiyuan Yang, Fei Liu, Zhenkun Wang, Qingfu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-objective optimization problems can be found in many real-world
applications, where the objectives often conflict each other and cannot be
optimized by a single solution. In the past few decades, numerous methods have
been proposed to find Pareto solutions that represent different optimal
trade-offs among the objectives for a given problem. However, these existing
methods could have high computational complexity or may not have good
theoretical properties for solving a general differentiable multi-objective
optimization problem. In this work, by leveraging the smooth optimization
technique, we propose a novel and lightweight smooth Tchebycheff scalarization
approach for gradient-based multi-objective optimization. It has good
theoretical properties for finding all Pareto solutions with valid trade-off
preferences, while enjoying significantly lower computational complexity
compared to other methods. Experimental results on various real-world
application problems fully demonstrate the effectiveness of our proposed
method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>fix some typos</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Neural-Evolutionary Algorithm for Autonomous Transit Network Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07917v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07917v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Holliday, Gregory Dudek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Planning a public transit network is a challenging optimization problem, but
essential in order to realize the benefits of autonomous buses. We propose a
novel algorithm for planning networks of routes for autonomous buses. We first
train a graph neural net model as a policy for constructing route networks, and
then use the policy as one of several mutation operators in a evolutionary
algorithm. We evaluate this algorithm on a standard set of benchmarks for
transit network design, and find that it outperforms the learned policy alone
by up to 20% and a plain evolutionary algorithm approach by up to 53% on
realistic benchmark instances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible. arXiv admin note: text overlap with arXiv:2306.00720</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Random Search as a Baseline for Sparse Neural Network Architecture
  Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08265v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08265v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rezsa Farahani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse neural networks have shown similar or better generalization
performance than their dense counterparts while having higher parameter
efficiency. This has motivated a number of works to learn or search for high
performing sparse networks. While reports of task performance or efficiency
gains are impressive, standard baselines are lacking leading to poor
comparability and unreliable reproducibility across methods. In this work, we
propose Random Search as a baseline algorithm for finding good sparse
configurations and study its performance. We apply Random Search on the node
space of an overparameterized network with the goal of finding better
initialized sparse sub-networks that are positioned more advantageously in the
loss landscape. We record the post-training performances of the found sparse
networks and at various levels of sparsity, and compare against both their
fully connected parent networks and random sparse configurations at the same
sparsity levels. First, we demonstrate performance at different levels of
sparsity and highlight that a significant level of performance can still be
preserved even when the network is highly sparse. Second, we observe that for
this sparse architecture search task, initialized sparse networks found by
Random Search neither perform better nor converge more efficiently than their
random counterparts. Thus we conclude that Random Search may be viewed as a
reasonable neutral baseline for sparsity search methods.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-13T00:00:00Z">2024-03-13</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computer Science and Game Theory
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model-free Resilient Controller Design based on Incentive Feedback
  Stackelberg <span class="highlight-title">Game</span> and Q-learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08948v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08948v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiajun Shen, Fengjun Li, Morteza Hashemi, Huazhen Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the swift evolution of Cyber-Physical Systems (CPSs) within intelligent
environments, especially in the industrial domain shaped by Industry 4.0, the
surge in development brings forth unprecedented security challenges. This paper
explores the intricate security issues of Industrial CPSs (ICPSs), with a
specific focus on the unique threats presented by intelligent attackers capable
of directly compromising the controller, thereby posing a direct risk to
physical security. Within the framework of hierarchical control and incentive
feedback Stackelberg game, we design a resilient leading controller (leader)
that is adaptive to a compromised following controller (follower) such that the
compromised follower acts cooperatively with the leader, aligning its
strategies with the leader's objective to achieve a team-optimal solution.
First, we provide sufficient conditions for the existence of an incentive
Stackelberg solution when system dynamics are known. Then, we propose a
Q-learning-based Approximate Dynamic Programming (ADP) approach, and
corresponding algorithms for the online resolution of the incentive Stackelberg
solution without requiring prior knowledge of system dynamics. Last but not
least, we prove the convergence of our approach to the optimum.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language-based <span class="highlight-title">game</span> theory in the age of artificial intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08944v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08944v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valerio Capraro, Roberto Di Paolo, Matjaz Perc, Veronica Pizziol
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding human behaviour in decision problems and strategic interactions
has wide-ranging applications in economics, psychology, and artificial
intelligence. Game theory offers a robust foundation for this understanding,
based on the idea that individuals aim to maximize a utility function. However,
the exact factors influencing strategy choices remain elusive. While
traditional models try to explain human behaviour as a function of the outcomes
of available actions, recent experimental research reveals that linguistic
content significantly impacts decision-making, thus prompting a paradigm shift
from outcome-based to language-based utility functions. This shift is more
urgent than ever, given the advancement of generative AI, which has the
potential to support humans in making critical decisions through language-based
interactions. We propose sentiment analysis as a fundamental tool for this
shift and take an initial step by analyzing 61 experimental instructions from
the dictator game, an economic game capturing the balance between self-interest
and the interest of others, which is at the core of many social interactions.
Our meta-analysis shows that sentiment analysis can explain human behaviour
beyond economic outcomes. We discuss future research directions. We hope this
work sets the stage for a novel game theoretical approach that emphasizes the
importance of language in human decisions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Strategizing against Q-learners: A Control-theoretical Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08906v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08906v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuksel Arslantas, Ege Yuceel, Muhammed O. Sayin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we explore the susceptibility of the Q-learning algorithm (a
classical and widely used reinforcement learning method) to strategic
manipulation of sophisticated opponents in games. We quantify how much a
strategically sophisticated agent can exploit a naive Q-learner if she knows
the opponent's Q-learning algorithm. To this end, we formulate the strategic
actor's problem as a Markov decision process (with a continuum state space
encompassing all possible Q-values) as if the Q-learning algorithm is the
underlying dynamical system. We also present a quantization-based approximation
scheme to tackle the continuum state space and analyze its performance both
analytically and numerically.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning How to Strategically Disclose Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08741v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08741v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raj Kiriti Velicheti, Melih Bastopcu, S. Rasoul Etesami, Tamer Başar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Strategic information disclosure, in its simplest form, considers a game
between an information provider (sender) who has access to some private
information that an information receiver is interested in. While the receiver
takes an action that affects the utilities of both players, the sender can
design information (or modify beliefs) of the receiver through signal
commitment, hence posing a Stackelberg game. However, obtaining a Stackelberg
equilibrium for this game traditionally requires the sender to have access to
the receiver's objective. In this work, we consider an online version of
information design where a sender interacts with a receiver of an unknown type
who is adversarially chosen at each round. Restricting attention to Gaussian
prior and quadratic costs for the sender and the receiver, we show that
$\mathcal{O}(\sqrt{T})$ regret is achievable with full information feedback,
where $T$ is the total number of interactions between the sender and the
receiver. Further, we propose a novel parametrization that allows the sender to
achieve $\mathcal{O}(\sqrt{T})$ regret for a general convex utility function.
We then consider the Bayesian Persuasion problem with an additional cost term
in the objective function, which penalizes signaling policies that are more
informative and obtain $\mathcal{O}(\log(T))$ regret. Finally, we establish a
sublinear regret bound for the partial information feedback setting and provide
simulations to support our theoretical results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Algorithmic Theory of Simplicity in Mechanism Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diodato Ferraioli, Carmine Ventre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A growing body of work in economics and computation focuses on the trade-off
between implementability and simplicity in mechanism design. The goal is to
develop a theory that not only allows to design an incentive structure easy to
grasp for imperfectly rational agents, but also understand the ensuing
limitations on the class of mechanisms that enforce it. In this context, the
concept of OSP mechanisms has assumed a prominent role since they provably
account for the absence of contingent reasoning skills, a specific cognitive
limitation. For single-dimensional agents, it is known that OSP mechanisms need
to use certain greedy algorithms.
  In this work, we introduce a notion that interpolates between OSP and SOSP, a
more stringent notion where agents only plan a subset of their own future
moves. We provide an algorithmic characterization of this novel class of
mechanisms for single-dimensional domains and binary allocation problems, that
precisely measures the interplay between simplicity and implementability. We
build on this to show how mechanisms based on reverse greedy algorithms
(a.k.a., deferred acceptance auctions) are algorithmically more robust to
imperfectly rationality than those adopting greedy algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Single-token vs Two-token Blockchain Tokenomics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15429v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15429v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aggelos Kiayias, Philip Lazos, Paolo Penna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider long-term equilibria that arise in the tokenomics design of
proof-of-stake (PoS) blockchain systems that comprise of users and validators,
both striving to maximize their own utilities. Validators are system
maintainers who get rewarded with tokens for performing the work necessary for
the system to function properly, while users compete and pay with such tokens
for getting a desired system service level.
  We study how the system service provision and suitable rewards schemes
together can lead to equilibria with desirable characteristics (1) viability:
the system keeps parties engaged, (2) decentralization: multiple validators are
participating, (3) stability: the price path of the underlying token used to
transact with the system does not change widely over time, and (4) feasibility:
the mechanism is easy to implement as a smart contract, i.e., it does not
require fiat reserves on-chain for buy back of tokens or to perform bookkeeping
of exponentially growing token holdings. Furthermore, we consider both the
common single-token PoS model and a less widely used two-token approach (that
roughly, utilizes one token for the users to pay the transaction fees and a
different token for the validators to participate in the PoS protocol and get
rewarded). Our approach demonstrates, for the first time to our knowledge,
concrete advantages of the two-token approach in terms of the ability of the
mechanism to reach equilibrium.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Measures of relevance to the success of streaming platforms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08421v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08421v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan Carlos Gonçalves-Dosantos, Ricardo Martínez, Joaquín Sánchez-Soriano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Digital streaming platforms, including Twitch, Spotify, Netflix, Disney, and
Kindle, have emerged as one of the main sources of entertainment with
significant growth potential. Many of these platforms distribute royalties
among streamers, artists, producers, or writers based on their impact. In this
paper, we measure the relevance of each of these contributors to the overall
success of the platform, which is information that can play a key role in
revenue allocation. We perform an axiomatic analysis to provide normative
foundations for three relevance metrics: the uniform, the proportional, and the
subscriber-proportional indicators. The last two indicators implement the
so-called pro-rata and user-centric models, which are extensively applied to
distribute revenues in the music streaming market. The axioms we propose
formalize different principles of fairness, stability, and non-manipulability,
and are tailor-made for the streaming context. We complete our analysis with a
case study that measures the influence of the 19 most-followed streamers
worldwide on the Twitch platform.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tractable Local Equilibria in Non-Concave <span class="highlight-title">Game</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08171v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08171v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Cai, Constantinos Daskalakis, Haipeng Luo, Chen-Yu Wei, Weiqiang Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Online Gradient Descent and other no-regret learning procedures are
known to efficiently converge to coarse correlated equilibrium in games where
each agent's utility is concave in their own strategy, this is not the case
when the utilities are non-concave, a situation that is common in machine
learning applications where the agents' strategies are parameterized by deep
neural networks, or the agents' utilities are computed by a neural network, or
both. Indeed, non-concave games present a host of game-theoretic and
optimization challenges: (i) Nash equilibria may fail to exist; (ii) local Nash
equilibria exist but are intractable; and (iii) mixed Nash, correlated, and
coarse correlated equilibria have infinite support in general, and are
intractable. To sidestep these challenges we propose a new solution concept,
termed $(\varepsilon, \Phi(\delta))$-local equilibrium, which generalizes local
Nash equilibrium in non-concave games, as well as (coarse) correlated
equilibrium in concave games. Importantly, we show that two instantiations of
this solution concept capture the convergence guarantees of Online Gradient
Descent and no-regret learning, which we show efficiently converge to this type
of equilibrium in non-concave games with smooth utilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Algorithmic Information Disclosure in Optimal Auctions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08145v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08145v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Cai, Yingkai Li, Jinzhao Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies a joint design problem where a seller can design both the
signal structures for the agents to learn their values, and the allocation and
payment rules for selling the item. In his seminal work, Myerson (1981) shows
how to design the optimal auction with exogenous signals. We show that the
problem becomes NP-hard when the seller also has the ability to design the
signal structures. Our main result is a polynomial-time approximation scheme
(PTAS) for computing the optimal joint design with at most an $\epsilon$
multiplicative loss in expected revenue. Moreover, we show that in our joint
design problem, the seller can significantly reduce the information rent of the
agents by providing partial information, which ensures a revenue that is at
least $1 - \frac{1}{e}$ of the optimal welfare for all valuation distributions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Expected flow networks in stochastic environments and two-player
  zero-sum <span class="highlight-title">game</span>s <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02779v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02779v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Jiralerspong, Bilun Sun, Danilo Vucetic, Tianyu Zhang, <span class="highlight-author">Yoshua Bengio</span>, Gauthier Gidel, Nikolay Malkin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative flow networks (GFlowNets) are sequential sampling models trained
to match a given distribution. GFlowNets have been successfully applied to
various structured object generation tasks, sampling a diverse set of
high-reward objects quickly. We propose expected flow networks (EFlowNets),
which extend GFlowNets to stochastic environments. We show that EFlowNets
outperform other GFlowNet formulations in stochastic tasks such as protein
design. We then extend the concept of EFlowNets to adversarial environments,
proposing adversarial flow networks (AFlowNets) for two-player zero-sum games.
We show that AFlowNets learn to find above 80% of optimal moves in Connect-4
via self-play and outperform AlphaZero in tournaments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024; code: https://github.com/GFNOrg/AdversarialFlowNetworks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distributed Nash Equilibrium Seeking over Time-Varying Directed
  Communication Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.02323v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.02323v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Duong Thuy Anh Nguyen, Duong Tung Nguyen, Angelia Nedić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study distributed algorithms for finding a Nash equilibrium (NE) in a
class of non-cooperative convex games under partial information. Specifically,
each agent has access only to its own smooth local cost function and can
receive information from its neighbors in a time-varying directed communication
network. To this end, we propose a distributed gradient play algorithm to
compute a NE by utilizing local information exchange among the players. In this
algorithm, every agent performs a gradient step to minimize its own cost
function while sharing and retrieving information locally among its neighbors.
The existing methods impose strong assumptions such as balancedness of the
mixing matrices and global knowledge of the network communication structure,
including Perron-Frobenius eigenvector of the adjacency matrix and other graph
connectivity constants. In contrast, our approach relies only on a reasonable
and widely-used assumption of row-stochasticity of the mixing matrices. We
analyze the algorithm for time-varying directed graphs and prove its
convergence to the NE, when the agents' cost functions are strongly convex and
have Lipschitz continuous gradients. Numerical simulations are performed for a
Nash-Cournot game to illustrate the efficacy of the proposed algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Search and Rescue on a Poset 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06622v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06622v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan-Tino Brethouwer, Robbert Fokkink
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A Search and Rescue game (SR game) is a new type of game on a graph that has
quickly found applications in scheduling, object detection, and adaptive
search. In this paper, we broaden the definition of SR games by putting them
into the context of ordered sets and Bayesian networks, extending known
solutions of these games and opening up the way to further applications.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Wet TinyML: Chemical Neural Network Using Gene Regulation and Cell
  Plasticity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08549v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08549v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samitha Somathilaka, Adrian Ratwatte, Sasitharan Balasubramaniam, Mehmet Can Vuran, Witawas Srisa-an, Pietro Liò
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In our earlier work, we introduced the concept of Gene Regulatory Neural
Network (GRNN), which utilizes natural neural network-like structures inherent
in biological cells to perform computing tasks using chemical inputs. We define
this form of chemical-based neural network as Wet TinyML. The GRNN structures
are based on the gene regulatory network and have weights associated with each
link based on the estimated interactions between the genes. The GRNNs can be
used for conventional computing by employing an application-based search
process similar to the Network Architecture Search. This study advances this
concept by incorporating cell plasticity, to further exploit natural cell's
adaptability, in order to diversify the GRNN search that can match larger
spectrum as well as dynamic computing tasks. As an example application, we show
that through the directed cell plasticity, we can extract the mathematical
regression evolution enabling it to match to dynamic system applications. We
also conduct energy analysis by comparing the chemical energy of the GRNN to
its silicon counterpart, where this analysis includes both artificial neural
network algorithms executed on von Neumann architecture as well as neuromorphic
processors. The concept of Wet TinyML can pave the way for the new emergence of
chemical-based, energy-efficient and miniature Biological AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a full paper by the tinyML Research Symposium 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data augmentation with automated machine learning: approaches and
  performance comparison with classical data augmentation methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08352v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08352v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alhassan Mumuni, Fuseini Mumuni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data augmentation is arguably the most important regularization technique
commonly used to improve generalization performance of machine learning models.
It primarily involves the application of appropriate data transformation
operations to create new data samples with desired properties. Despite its
effectiveness, the process is often challenging because of the time-consuming
trial and error procedures for creating and testing different candidate
augmentations and their hyperparameters manually. Automated data augmentation
methods aim to automate the process. State-of-the-art approaches typically rely
on automated machine learning (AutoML) principles. This work presents a
comprehensive survey of AutoML-based data augmentation techniques. We discuss
various approaches for accomplishing data augmentation with AutoML, including
data manipulation, data integration and data synthesis techniques. We present
extensive discussion of techniques for realizing each of the major subtasks of
the data augmentation process: search space design, hyperparameter optimization
and model evaluation. Finally, we carried out an extensive comparison and
analysis of the performance of automated data augmentation techniques and
state-of-the-art methods based on classical augmentation approaches. The
results show that AutoML methods for data augmentation currently outperform
state-of-the-art techniques based on conventional approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cyclic Data Parallelism for Efficient Parallelism of Deep Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08837v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08837v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Louis Fournier, Edouard Oyallon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training large deep learning models requires parallelization techniques to
scale. In existing methods such as Data Parallelism or ZeRO-DP, micro-batches
of data are processed in parallel, which creates two drawbacks: the total
memory required to store the model's activations peaks at the end of the
forward pass, and gradients must be simultaneously averaged at the end of the
backpropagation step. We propose Cyclic Data Parallelism, a novel paradigm
shifting the execution of the micro-batches from simultaneous to sequential,
with a uniform delay. At the cost of a slight gradient delay, the total memory
taken by activations is constant, and the gradient communications are balanced
during the training step. With Model Parallelism, our technique reduces the
number of GPUs needed, by sharing GPUs across micro-batches. Within the ZeRO-DP
framework, our technique allows communication of the model states with
point-to-point operations rather than a collective broadcast operation. We
illustrate the strength of our approach on the CIFAR-10 and ImageNet datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Measuring the Energy Consumption and Efficiency of Deep Neural Networks:
  An Empirical Analysis and Design Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08151v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08151v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charles Edison Tripp, Jordan Perr-Sauer, Jamil Gafur, Amabarish Nag, Avi Purkayastha, Sagi Zisman, Erik A. Bensen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Addressing the so-called ``Red-AI'' trend of rising energy consumption by
large-scale neural networks, this study investigates the actual energy
consumption, as measured by node-level watt-meters, of training various fully
connected neural network architectures. We introduce the BUTTER-E dataset, an
augmentation to the BUTTER Empirical Deep Learning dataset, containing energy
consumption and performance data from 63,527 individual experimental runs
spanning 30,582 distinct configurations: 13 datasets, 20 sizes (number of
trainable parameters), 8 network ``shapes'', and 14 depths on both CPU and GPU
hardware collected using node-level watt-meters. This dataset reveals the
complex relationship between dataset size, network structure, and energy use,
and highlights the impact of cache effects. We propose a straightforward and
effective energy model that accounts for network size, computing, and memory
hierarchy. Our analysis also uncovers a surprising, hardware-mediated
non-linear relationship between energy efficiency and network design,
challenging the assumption that reducing the number of parameters or FLOPs is
the best way to achieve greater energy efficiency. Highlighting the need for
cache-considerate algorithm development, we suggest a combined approach to
energy efficient network, algorithm, and hardware design. This work contributes
to the fields of sustainable computing and Green AI, offering practical
guidance for creating more energy-efficient neural networks and promoting
sustainable AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 8 figures, for associated dataset see
  https://data.openei.org/submissions/5991</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ High-speed Low-consumption sEMG-based Transient-state micro-Gesture
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06998v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06998v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youfang Han, Wei Zhao, Xiangjin Chen, Xin Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gesture recognition on wearable devices is extensively applied in
human-computer interaction. Electromyography (EMG) has been used in many
gesture recognition systems for its rapid perception of muscle signals.
However, analyzing EMG signals on devices, like smart wristbands, usually needs
inference models to have high performances, such as low inference latency, low
power consumption, and low memory occupation. Therefore, this paper proposes an
improved spiking neural network (SNN) to achieve these goals. We propose an
adaptive multi-delta coding as a spiking coding method to improve recognition
accuracy. We propose two additive solvers for SNN, which can reduce inference
energy consumption and amount of parameters significantly, and improve the
robustness of temporal differences. In addition, we propose a linear action
detection method TAD-LIF, which is suitable for SNNs. TAD-LIF is an improved
LIF neuron that can detect transient-state gestures quickly and accurately. We
collected two datasets from 20 subjects including 6 micro gestures. The
collection devices are two designed lightweight consumer-level sEMG wristbands
(3 and 8 electrode channels respectively). Compared to CNN, FCN, and normal
SNN-based methods, the proposed SNN has higher recognition accuracy. The
accuracy of the proposed SNN is 83.85% and 93.52% on the two datasets
respectively. In addition, the inference latency of the proposed SNN is about
1% of CNN, the power consumption is about 0.1% of CNN, and the memory
occupation is about 20% of CNN. The proposed methods can be used for precise,
high-speed, and low-power micro-gesture recognition tasks, and are suitable for
consumer-level intelligent wearable devices, which is a general way to achieve
ubiquitous computing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SWAP-NAS: <span class="highlight-title">Sample</span>-Wise Activation Patterns for Ultra-fast NAS <span class="chip">ICLR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04161v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04161v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yameng Peng, Andy Song, Haytham M. Fayek, Vic Ciesielski, Xiaojun Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training-free metrics (a.k.a. zero-cost proxies) are widely used to avoid
resource-intensive neural network training, especially in Neural Architecture
Search (NAS). Recent studies show that existing training-free metrics have
several limitations, such as limited correlation and poor generalisation across
different search spaces and tasks. Hence, we propose Sample-Wise Activation
Patterns and its derivative, SWAP-Score, a novel high-performance training-free
metric. It measures the expressivity of networks over a batch of input samples.
The SWAP-Score is strongly correlated with ground-truth performance across
various search spaces and tasks, outperforming 15 existing training-free
metrics on NAS-Bench-101/201/301 and TransNAS-Bench-101. The SWAP-Score can be
further enhanced by regularisation, which leads to even higher correlations in
cell-based search space and enables model size control during the search. For
example, Spearman's rank correlation coefficient between regularised SWAP-Score
and CIFAR-100 validation accuracies on NAS-Bench-201 networks is 0.90,
significantly higher than 0.80 from the second-best metric, NWOT. When
integrated with an evolutionary algorithm for NAS, our SWAP-NAS achieves
competitive performance on CIFAR-10 and ImageNet in approximately 6 minutes and
9 minutes of GPU time respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR2024 Spotlight</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-12T00:00:00Z">2024-03-12</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computer Science and Game Theory
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Apartment Rent Division 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08051v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08051v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ariel D. Procaccia, Benjamin Schiffer, Shirley Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rent division is the well-studied problem of fairly assigning rooms and
dividing rent among a set of roommates within a single apartment. A shortcoming
of existing solutions is that renters are assumed to be considering apartments
in isolation, whereas in reality, renters can choose among multiple apartments.
In this paper, we generalize the rent division problem to the multi-apartment
setting, where the goal is to both fairly choose an apartment among a set of
alternatives and fairly assign rooms and rents within the chosen apartment. Our
main contribution is a generalization of envy-freeness called rearrangeable
envy-freeness. We show that a solution satisfying rearrangeable envy-freeness
is guaranteed to exist and that it is possible to optimize over all
rearrangeable envy-free solutions in polynomial time. We also define an even
stronger fairness notion called universal envy-freeness and study its existence
when values are drawn randomly.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Equitable Pricing in Auctions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07799v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07799v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Finster, Patrick Loiseau, Simon Mauras, Mathieu Molina, Bary Pradelski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study how pricing affects the division of surplus among buyers in auctions
for multiple units. Our equity objective may be important, e.g., for
competition concerns in downstream markets, complementing the long-standing
debate on revenue and efficiency. We study a canonical model of auctions for
multiple indivisible units with unit demand buyers and valuations with a
private and a common component and consider all pricing rules that are a
mixture (i.e., a convex combination) of pay-as-bid and uniform pricing. We
propose the winners' empirical variance (WEV), the expected empirical variance
of surplus among the winners, as a metric for surplus equity. We show that, for
a range of private-common value proportions, a strictly interior mix of
pay-as-bid and uniform pricing minimizes WEV. From an equity perspective,
auctions with a higher private value component benefit from more price
discrimination, whereas only auctions with a sufficiently high common value
justify a more uniform pricing rule. We provide a criterion under which
strictly mixed pricing dominates uniform pricing, a partial ranking of
different mixed pricing formats, and bounds on the WEV-minimizing pricing under
the assumption of log-concave signal distributions. In numerical experiments,
we further illustrate the WEV-minimal pricing as a function of the
private-common-value mix.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Controlling Delegations in Liquid Democracy <span class="chip">AAMAS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07558v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07558v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiri Alouf-Heffetz, Tanmay Inamdar, Pallavi Jain, Yash More, Nimrod Talmon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In liquid democracy, agents can either vote directly or delegate their vote
to a different agent of their choice. This results in a power structure in
which certain agents possess more voting weight than others. As a result, it
opens up certain possibilities of vote manipulation, including control and
bribery, that do not exist in standard voting scenarios of direct democracy.
Here we formalize a certain kind of election control -- in which an external
agent may change certain delegation arcs -- and study the computational
complexity of the corresponding combinatorial problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in 23rd International Conference on Autonomous Agents and
  Multiagent Systems(AAMAS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Existence of Reactive Strategies Resilient to Delay 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.19985v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.19985v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Fränzle, Paul Kröger, Sarah Winter, Martin Zimmermann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We compare games under delayed control and delay games, two types of infinite
games modelling asynchronicity in reactive synthesis. In games under delayed
control both players suffer from partial informedness due to symmetrically
delayed communication, while in delay games, the protagonist has to grant
lookahead to the alter player. Our first main result, the interreducibility of
the existence of sure winning strategies for the protagonist, allows to
transfer known complexity results and bounds on the delay from delay games to
games under delayed control, for which no such results had been known. We
furthermore analyse existence of randomized strategies that win almost surely,
where this correspondence between the two types of games breaks down. In this
setting, some games surely won by the alter player in delay games can now be
won almost surely by the protagonist in the corresponding game under delayed
control, showing that it indeed makes a difference whether the protagonist has
to grant lookahead or both players suffer from partial informedness. These
results get even more pronounced when we finally address the quantitative goal
of winning with a probability in $[0,1]$. We show that for any rational
threshold $\theta \in [0,1]$ there is a game that can be won by the protagonist
with exactly probability $\theta$ under delayed control, while being surely won
by alter in the delay game setting. All these findings refine our original
result that games under delayed control are not determined.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Full version of arXiv:2310.01010, contains all proofs omitted in the
  conference version as well as a new section on winning games under delayed
  control with mixed strategies with respect to a fixed threshold</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TSO <span class="highlight-title">Game</span>s -- On the decidability of safety <span class="highlight-title">game</span>s under the total store
  order semantics (extended LMCS version with appendix) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.02862v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.02862v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stephan Spengler, Sanchari Sil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider an extension of the classical Total Store Order (TSO) semantics
by expanding it to turn-based 2-player safety games. During her turn, a player
can select any of the communicating processes and perform its next transition.
We consider different formulations of the safety game problem depending on
whether one player or both of them transfer messages from the process buffers
to the shared memory. We give the complete decidability picture for all the
possible alternatives.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 11 figures, presented at GandALF 2023, submitted to LMCS
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bidding efficiently in Simultaneous Ascending Auctions with budget and
  eligibility constraints using Simultaneous Move Monte Carlo Tree Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.11428v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.11428v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandre Pacaud, Aurelien Bechler, Marceau Coupechoux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For decades, Simultaneous Ascending Auction (SAA) has been the most popular
mechanism used for spectrum auctions. It has recently been employed by many
countries for the allocation of 5G licences. Although SAA presents relatively
simple rules, it induces a complex strategic game for which the optimal bidding
strategy is unknown. Considering the fact that sometimes billions of euros are
at stake in an SAA, establishing an efficient bidding strategy is crucial. In
this work, we model the auction as a $n$-player simultaneous move game with
complete information and propose the first efficient bidding algorithm that
tackles simultaneously its four main strategic issues: the $\textit{exposure
problem}$, the $\textit{own price effect}$, $\textit{budget constraints}$ and
the $\textit{eligibility management problem}$. Our solution, called
$SMS^\alpha$, is based on Simultaneous Move Monte Carlo Tree Search (SM-MCTS)
and relies on a new method for the prediction of closing prices. By introducing
a new reward function in $SMS^\alpha$, we give the possibility to bidders to
define their own level of risk-aversion. Through extensive numerical
experiments on instances of realistic size, we show that $SMS^\alpha$ largely
outperforms state-of-the-art algorithms, notably by achieving higher expected
utility while taking less risks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 19 figures, The paper has been submitted to IEEE journal
  for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Defending Against Poisoning Attacks in Federated Learning with
  Blockchain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.00543v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.00543v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nanqing Dong, Zhipeng Wang, Jiahao Sun, Michael Kampffmeyer, William Knottenbelt, Eric Xing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the era of deep learning, federated learning (FL) presents a promising
approach that allows multi-institutional data owners, or clients, to
collaboratively train machine learning models without compromising data
privacy. However, most existing FL approaches rely on a centralized server for
global model aggregation, leading to a single point of failure. This makes the
system vulnerable to malicious attacks when dealing with dishonest clients. In
this work, we address this problem by proposing a secure and reliable FL system
based on blockchain and distributed ledger technology. Our system incorporates
a peer-to-peer voting mechanism and a reward-and-slash mechanism, which are
powered by on-chain smart contracts, to detect and deter malicious behaviors.
Both theoretical and empirical analyses are presented to demonstrate the
effectiveness of the proposed approach, showing that our framework is robust
against malicious client-side behaviors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Artificial Intelligence</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> of Lottery Ticket Hypothesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04861v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04861v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bohan Liu, Zijie Zhang, Peixiong He, Zhensen Wang, Yang Xiao, Ruimeng Ye, Yang Zhou, Wei-Shinn Ku, Bo Hui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Lottery Ticket Hypothesis (LTH) states that a dense neural network model
contains a highly sparse subnetwork (i.e., winning tickets) that can achieve
even better performance than the original model when trained in isolation.
While LTH has been proved both empirically and theoretically in many works,
there still are some open issues, such as efficiency and scalability, to be
addressed. Also, the lack of open-source frameworks and consensual experimental
setting poses a challenge to future research on LTH. We, for the first time,
examine previous research and studies on LTH from different perspectives. We
also discuss issues in existing works and list potential directions for further
exploration. This survey aims to provide an in-depth look at the state of LTH
and develop a duly maintained platform to conduct experiments and compare with
the most updated baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Solving Close Enough Orienteering Problem with Overlapped
  Neighborhoods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04257v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04257v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiuchen Qian, Yanran Wang, David Boyle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Close Enough Traveling Salesman Problem (CETSP) is a well-known variant of
TSP whereby the agent may complete its mission at any point within a target
neighborhood. Heuristics based on overlapped neighborhoods, known as Steiner
Zones (SZ), have gained attention in addressing CETSP. While SZs offer
effective approximations to the original graph, their inherent overlap imposes
constraints on search space, potentially conflicting with global optimization
objectives. Here we show how such limitations can be converted into advantages
in a Close Enough Orienteering Problem (CEOP) by aggregating prizes across
overlapped neighborhoods. We further extend classic CEOP with Non-uniform
Neighborhoods (CEOP-N) by introducing non-uniform costs for prize collection.
To tackle CEOP and CEOP-N, we develop a new approach featuring a Randomized
Steiner Zone Discretization (RSZD) scheme coupled with a hybrid algorithm based
on Particle Swarm Optimization (PSO) and Ant Colony System (ACS), CRaSZe-AntS.
The RSZD scheme identifies sub-regions for PSO exploration, and ACS determines
the discrete visiting sequence. We evaluate the RSZD's discretization
performance on CEOP instances derived from established CETSP instances and
compare CRaSZe-AntS against the most relevant state-of-the-art heuristic
focused on single-neighborhood optimization for CEOP instances. We also compare
the performance of the interior search within SZs and the boundary search on
individual neighborhoods in the context of CEOP-N. Our experimental results
show that CRaSZe-AntS can yield comparable solution quality with significantly
reduced computation time compared to the single neighborhood strategy, where we
observe an average 140.44% increase in prize collection and a 55.18% reduction
in algorithm execution time. CRaSZe-AntS is thus highly effective in solving
emerging CEOP-N, examples of which include truck-and-drone delivery scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lander.AI: Adaptive Landing Behavior <span class="highlight-title">Agent</span> for Expertise in 3D Dynamic
  Platform Landings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06572v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06572v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robinroy Peter, Lavanya Ratnabala, Demetros Aschu, Aleksey Fedoseev, Dzmitry Tsetserukou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mastering autonomous drone landing on dynamic platforms presents formidable
challenges due to unpredictable velocities and external disturbances caused by
the wind, ground effect, turbines or propellers of the docking platform. This
study introduces an advanced Deep Reinforcement Learning (DRL) agent,
Lander:AI, designed to navigate and land on platforms in the presence of windy
conditions, thereby enhancing drone autonomy and safety. Lander:AI is
rigorously trained within the gym-pybullet-drone simulation, an environment
that mirrors real-world complexities, including wind turbulence, to ensure the
agent's robustness and adaptability.
  The agent's capabilities were empirically validated with Crazyflie 2.1 drones
across various test scenarios, encompassing both simulated environments and
real-world conditions. The experimental results showcased Lander:AI's
high-precision landing and its ability to adapt to moving platforms, even under
wind-induced disturbances. Furthermore, the system performance was benchmarked
against a baseline PID controller augmented with an Extended Kalman Filter,
illustrating significant improvements in landing precision and error recovery.
Lander:AI leverages bio-inspired learning to adapt to external forces like
birds, enhancing drone adaptability without knowing force magnitudes.This
research not only advances drone landing technologies, essential for inspection
and emergency applications, but also highlights the potential of DRL in
addressing intricate aerodynamic challenges.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-11T00:00:00Z">2024-03-11</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computer Science and Game Theory
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Algorithmic Bayesian Epistemology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07949v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07949v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Neyman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One aspect of the algorithmic lens in theoretical computer science is a view
on other scientific disciplines that focuses on satisfactory solutions that
adhere to real-world constraints, as opposed to solutions that would be optimal
ignoring such constraints. The algorithmic lens has provided a unique and
important perspective on many academic fields, including molecular biology,
ecology, neuroscience, quantum physics, economics, and social science.
  This thesis applies the algorithmic lens to Bayesian epistemology.
Traditional Bayesian epistemology provides a comprehensive framework for how an
individual's beliefs should evolve upon receiving new information. However,
these methods typically assume an exhaustive model of such information,
including the correlation structure between different pieces of evidence. In
reality, individuals might lack such an exhaustive model, while still needing
to form beliefs. Beyond such informational constraints, an individual may be
bounded by limited computation, or by limited communication with agents that
have access to information, or by the strategic behavior of such agents. Even
when these restrictions prevent the formation of a *perfectly* accurate belief,
arriving at a *reasonably* accurate belief remains crucial. In this thesis, we
establish fundamental possibility and impossibility results about belief
formation under a variety of restrictions, and lay the groundwork for further
exploration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>385 pages, PhD thesis, 14 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stochastic Extragradient with Random Reshuffling: Improved Convergence
  for Variational Inequalities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07148v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07148v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantinos Emmanouilidis, René Vidal, Nicolas Loizou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Stochastic Extragradient (SEG) method is one of the most popular
algorithms for solving finite-sum min-max optimization and variational
inequality problems (VIPs) appearing in various machine learning tasks.
However, existing convergence analyses of SEG focus on its with-replacement
variants, while practical implementations of the method randomly reshuffle
components and sequentially use them. Unlike the well-studied with-replacement
variants, SEG with Random Reshuffling (SEG-RR) lacks established theoretical
guarantees. In this work, we provide a convergence analysis of SEG-RR for three
classes of VIPs: (i) strongly monotone, (ii) affine, and (iii) monotone. We
derive conditions under which SEG-RR achieves a faster convergence rate than
the uniform with-replacement sampling SEG. In the monotone setting, our
analysis of SEG-RR guarantees convergence to an arbitrary accuracy without
large batch sizes, a strong requirement needed in the classical
with-replacement SEG. As a byproduct of our results, we provide convergence
guarantees for Shuffle Once SEG (shuffles the data only at the beginning of the
algorithm) and the Incremental Extragradient (does not shuffle the data). We
supplement our analysis with experiments validating empirically the superior
performance of SEG-RR over the classical with-replacement sampling SEG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ New Perspectives in Online Contract Design: Heterogeneous, Homogeneous,
  Non-myopic <span class="highlight-title">Agent</span>s and Team Production 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiliang Zuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work studies the repeated principal-agent problem from an online
learning perspective. The principal's goal is to learn the optimal contract
that maximizes her utility through repeated interactions, without prior
knowledge of the agent's type (i.e., the agent's cost and production
functions).
  I study three different settings when the principal contracts with a
$\textit{single}$ agent each round: 1. The agents are heterogeneous; 2. the
agents are homogenous; 3. the principal interacts with the same agent and the
agent is non-myopic. I present different approaches and techniques for
designing learning algorithms in each setting. For heterogeneous agent types, I
identify a condition that allows the problem to be reduced to Lipschitz bandits
directly. For identical agents, I give a polynomial sample complexity scheme to
learn the optimal contract based on inverse game theory. For strategic
non-myopic agents, I design a low strategic-regret mechanism. Also, I identify
a connection between linear contracts and posted-price auctions, showing the
two can be reduced to one another, and give a regret lower bound on learning
the optimal linear contract based on this observation.
  I also study a $\textit{team production}$ model. I identify a condition under
which the principal's learning problem can be reformulated as solving a family
of convex programs, thereby showing the optimal contract can be found
efficiently.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthesis of Robust Optimal Strategies in Weighted Timed <span class="highlight-title">Game</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06921v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06921v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Monmege, Julie Parreaux, Pierre-Alain Reynier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weighted Timed Games (WTG for short) are the most widely used model to
describe controller synthesis problems involving real-time issues. The
synthesized strategies rely on a perfect measure of time elapse, which is not
realistic in practice. In order to produce strategies tolerant to timing
imprecisions, we rely on a notion of robustness first introduced for timed
automata. More precisely, WTGs are two-player zero-sum games played in a timed
automaton equipped with integer weights in which one of the players, that we
call Min, wants to reach a target location while minimising the cumulated
weight. In this work, we equip the underlying timed automaton with a semantics
depending on some parameter (representing the maximal possible perturbation) in
which the opponent of Min can in addition perturb delays chosen by Min.
  The robust value problem can then be stated as follows: given some threshold,
determine whether there exists a positive perturbation and a strategy for Min
ensuring to reach the target, with an accumulated weight below the threshold,
whatever the opponent does.
  We provide the first decidability result for this robust value problem by
computing the robust value function, in a parametric way, for the class of
divergent WTGs (introduced to obtain decidability of the (classical) value
problem in WTGs without bounding the number of clocks). To this end, we show
that the robust value is the fixpoint of some operators, as is classically done
for value iteration algorithms. We then combine in a very careful way two
representations: piecewise affine functions introduced in [1] to analyse WTGs,
and shrunk Difference Bound Matrices considered in [29] to analyse robustness
in timed automata. Last, we also study qualitative decision problems and close
an open problem on robust reachability, showing it is EXPTIME-complete for
general WTGs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Defaults: a double-edged sword in governing common resources 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06796v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06796v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eladio Montero-Porras, Rémi Suchon, Tom Lenaerts, Elias Fernández Domingos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extracting from shared resources requires making choices to balance personal
profit and sustainability. We present the results of a behavioural experiment
wherein we manipulate the default extraction from a finite resource.
Participants were exposed to two treatments -- pro-social or self-serving
extraction defaults -- and a control without defaults. We examined the
persistence of these nudges by removing the default after five rounds. Results
reveal that a self-serving default increased the average extraction while
present, whereas a pro-social default only decreased extraction for the first
two rounds. Notably, the influence of defaults depended on individual
inclinations, with cooperative individuals extracting more under a self-serving
default, and selfish individuals less under a pro-social default. After the
removal of the default, we observed no significant differences with the control
treatment. Our research highlights the potential of defaults as cost-effective
tools for promoting sustainability, while also advocating for a careful use to
avoid adverse effects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 11 pages of Supplementary Information, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Provable Mutual Benefits from Federated Learning in Privacy-Sensitive
  Domains <span class="chip">AISTATS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06672v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06672v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikita Tsoy, Anna Mihalkova, Teodora Todorova, Nikola Konstantinov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-silo federated learning (FL) allows data owners to train accurate
machine learning models by benefiting from each others private datasets.
Unfortunately, the model accuracy benefits of collaboration are often
undermined by privacy defenses. Therefore, to incentivize client participation
in privacy-sensitive domains, a FL protocol should strike a delicate balance
between privacy guarantees and end-model accuracy. In this paper, we study the
question of when and how a server could design a FL protocol provably
beneficial for all participants. First, we provide necessary and sufficient
conditions for the existence of mutually beneficial protocols in the context of
mean estimation and convex stochastic optimization. We also derive protocols
that maximize the total clients' utility, given symmetric privacy preferences.
Finally, we design protocols maximizing end-model accuracy and demonstrate
their benefits in synthetic experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AISTATS 2024; Camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Bayesian Learning Algorithm for Unknown Zero-sum Stochastic <span class="highlight-title">Game</span>s with
  an Arbitrary Opponent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.03396v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.03396v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehdi Jafarnia-Jahromi, Rahul Jain, Ashutosh Nayyar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose Posterior Sampling Reinforcement Learning for
Zero-sum Stochastic Games (PSRL-ZSG), the first online learning algorithm that
achieves Bayesian regret bound of $O(HS\sqrt{AT})$ in the infinite-horizon
zero-sum stochastic games with average-reward criterion. Here $H$ is an upper
bound on the span of the bias function, $S$ is the number of states, $A$ is the
number of joint actions and $T$ is the horizon. We consider the online setting
where the opponent can not be controlled and can take any arbitrary
time-adaptive history-dependent strategy. Our regret bound improves on the best
existing regret bound of $O(\sqrt[3]{DS^2AT^2})$ by Wei et al. (2017) under the
same assumption and matches the theoretical lower bound in $T$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Designing Equilibria in Concurrent <span class="highlight-title">Game</span>s with Social Welfare and
  Temporal Logic Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03045v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03045v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Gutierrez, Muhammad Najib, Giuseppe Perelli, Michael Wooldridge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In game theory, mechanism design is concerned with the design of incentives
so that a desired outcome of the game can be achieved. In this paper, we
explore the concept of equilibrium design, where incentives are designed to
obtain a desirable equilibrium that satisfies a specific temporal logic
property. Our study is based on a framework where system specifications are
represented as temporal logic formulae, games as quantitative concurrent game
structures, and players' goals as mean-payoff objectives. We consider system
specifications given by LTL and GR(1) formulae, and show that designing
incentives to ensure that a given temporal logic property is satisfied on
some/every Nash equilibrium of the game can be achieved in PSPACE for LTL
properties and in NP/{\Sigma}P 2 for GR(1) specifications. We also examine the
complexity of related decision and optimisation problems, such as optimality
and uniqueness of solutions, as well as considering social welfare, and show
that the complexities of these problems lie within the polynomial hierarchy.
Equilibrium design can be used as an alternative solution to rational synthesis
and verification problems for concurrent games with mean-payoff objectives when
no solution exists or as a technique to repair concurrent games with
undesirable Nash equilibria in an optimal way.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The manuscript is going to be submitted to the Journal on Logical
  Methods in Computer Science. arXiv admin note: substantial text overlap with
  arXiv:2106.10192</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MAP-Elites with Transverse Assessment for Multimodal Problems in
  Creative Domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marvin Zammit, Antonios Liapis, Georgios N. Yannakakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent advances in language-based generative models have paved the way
for the orchestration of multiple generators of different artefact types (text,
image, audio, etc.) into one system. Presently, many open-source pre-trained
models combine text with other modalities, thus enabling shared vector
embeddings to be compared across different generators. Within this context we
propose a novel approach to handle multimodal creative tasks using Quality
Diversity evolution. Our contribution is a variation of the MAP-Elites
algorithm, MAP-Elites with Transverse Assessment (MEliTA), which is tailored
for multimodal creative tasks and leverages deep learned models that assess
coherence across modalities. MEliTA decouples the artefacts' modalities and
promotes cross-pollination between elites. As a test bed for this algorithm, we
generate text descriptions and cover images for a hypothetical video game and
assign each artefact a unique modality-specific behavioural characteristic.
Results indicate that MEliTA can improve text-to-image mappings within the
solution space, compared to a baseline MAP-Elites algorithm that strictly
treats each image-text pair as one solution. Our approach represents a
significant step forward in multimodal bottom-up orchestration and lays the
groundwork for more complex systems coordinating multimodal creative agents in
the future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 5 figures To be published in the proceedings of the 13th
  International Conference on Artificial Intelligence in Music, Sound, Art and
  Design (EvoMUSART) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Ant Colony Sampling with GFlowNets for Combinatorial Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07041v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07041v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minsu Kim, Sanghyeok Choi, Jiwoo Son, Hyeonah Kim, Jinkyoo Park, <span class="highlight-author">Yoshua Bengio</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces the Generative Flow Ant Colony Sampler (GFACS), a novel
neural-guided meta-heuristic algorithm for combinatorial optimization. GFACS
integrates generative flow networks (GFlowNets) with the ant colony
optimization (ACO) methodology. GFlowNets, a generative model that learns a
constructive policy in combinatorial spaces, enhance ACO by providing an
informed prior distribution of decision variables conditioned on input graph
instances. Furthermore, we introduce a novel combination of training tricks,
including search-guided local exploration, energy normalization, and energy
shaping to improve GFACS. Our experimental results demonstrate that GFACS
outperforms baseline ACO algorithms in seven CO tasks and is competitive with
problem-specific heuristics for vehicle routing problems. The source code is
available at \url{https://github.com/ai4co/gfacs}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Robustness of Lexicase <span class="highlight-title">Selection</span> to Contradictory Objectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06805v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06805v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shakiba Shahbandegan, Emily Dolson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lexicase and epsilon-lexicase selection are state of the art parent selection
techniques for problems featuring multiple selection criteria. Originally,
lexicase selection was developed for cases where these selection criteria are
unlikely to be in conflict with each other, but preliminary work suggests it is
also a highly effective many-objective optimization algorithm. However, to
predict whether these results generalize, we must understand lexicase
selection's performance on contradictory objectives. Prior work has shown mixed
results on this question. Here, we develop theory identifying circumstances
under which lexicase selection will succeed or fail to find a Pareto-optimal
solution. To make this analysis tractable, we restrict our investigation to a
theoretical problem with maximally contradictory objectives. Ultimately, we
find that lexicase and epsilon-lexicase selection each have a region of
parameter space where they are incapable of optimizing contradictory
objectives. Outside of this region, however, they perform well despite the
presence of contradictory objectives. Based on these findings, we propose
theoretically-backed guidelines for parameter choice. Additionally, we identify
other properties that may affect whether a many-objective optimization problem
is a good fit for lexicase or epsilon-lexicase selection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multiple Population Alternate Evolution Neural Architecture Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07035v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07035v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan Zou, Han Chu, Yizhang Xia, Junwen Xu, Yuan Liu, Zhanglu Hou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The effectiveness of Evolutionary Neural Architecture Search (ENAS) is
influenced by the design of the search space. Nevertheless, common methods
including the global search space, scalable search space and hierarchical
search space have certain limitations. Specifically, the global search space
requires a significant amount of computational resources and time, the scalable
search space sacrifices the diversity of network structures and the
hierarchical search space increases the search cost in exchange for network
diversity. To address above limitation, we propose a novel paradigm of
searching neural network architectures and design the Multiple Population
Alternate Evolution Neural Architecture Search (MPAE), which can achieve module
diversity with a smaller search cost. MPAE converts the search space into L
interconnected units and sequentially searches the units, then the above search
of the entire network be cycled several times to reduce the impact of previous
units on subsequent units. To accelerate the population evolution process, we
also propose the the population migration mechanism establishes an excellent
migration archive and transfers the excellent knowledge and experience in the
migration archive to new populations. The proposed method requires only 0.3 GPU
days to search a neural network on the CIFAR dataset and achieves the
state-of-the-art results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Universality of Linear Recurrences Followed by Non-linear Projections:
  Finite-Width Guarantees and Benefits of Complex Eigenvalues 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.11888v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.11888v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Orvieto, Soham De, Caglar Gulcehre, Razvan Pascanu, Samuel L. Smith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks based on linear complex-valued RNNs interleaved with
position-wise MLPs are gaining traction as competitive approaches to sequence
modeling. Examples of such architectures include state-space models (SSMs) like
S4, LRU, and Mamba: recently proposed models that achieve promising performance
on text, genetics, and other data that require long-range reasoning. Despite
experimental evidence highlighting these architectures' effectiveness and
computational efficiency, their expressive power remains relatively unexplored,
especially in connection to specific choices crucial in practice - e.g.,
carefully designed initialization distribution and use of complex numbers. In
this paper, we show that combining MLPs with both real or complex linear
diagonal recurrences leads to arbitrarily precise approximation of regular
causal sequence-to-sequence maps. At the heart of our proof, we rely on a
separation of concerns: the linear RNN provides a lossless encoding of the
input sequence, and the MLP performs non-linear processing on this encoding.
While we show that using real diagonal linear recurrences is enough to achieve
universality in this architecture, we prove that employing complex eigenvalues
near unit disk - i.e., empirically the most successful strategy in SSMs -
greatly helps the RNN in storing information. We connect this finding with the
vanishing gradient issue and provide experimental evidence supporting our
claims.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v1: Accepted at HLD 2023: 1st Workshop on High-dimensional Learning
  Dynamics v2: Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Toward Neuromic Computing: Neurons as Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02331v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02331v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Larry Bull
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This short paper presents the idea that neural backpropagation is using
dendritic processing to enable individual neurons to perform autoencoding.
Using a very simple connection weight search heuristic and artificial neural
network model, the effects of interleaving autoencoding for each neuron in a
hidden layer of a feedforward network are explored. This is contrasted to the
standard layered approach to autoencoding. It is shown that such individualised
processing is not detrimental and can improve network learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pointer Networks Trained Better via Evolutionary Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.01150v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.01150v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muyao Zhong, Shengcai Liu, Bingdong Li, Haobo Fu, Ke Tang, Peng Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pointer Network (PtrNet) is a specific neural network for solving
Combinatorial Optimization Problems (COPs). While PtrNets offer real-time
feed-forward inference for complex COPs instances, its quality of the results
tends to be less satisfactory. One possible reason is that such issue suffers
from the lack of global search ability of the gradient descent, which is
frequently employed in traditional PtrNet training methods including both
supervised learning and reinforcement learning. To improve the performance of
PtrNet, this paper delves deeply into the advantages of training PtrNet with
Evolutionary Algorithms (EAs), which have been widely acknowledged for not
easily getting trapped by local optima. Extensive empirical studies based on
the Travelling Salesman Problem (TSP) have been conducted. Results demonstrate
that PtrNet trained with EA can consistently perform much better inference
results than eight state-of-the-art methods on various problem scales. Compared
with gradient descent based PtrNet training methods, EA achieves up to 30.21\%
improvement in quality of the solution with the same computational time. With
this advantage, this paper is able to at the first time report the results of
solving 1000-dimensional TSPs by training a PtrNet on the same dimensionality,
which strongly suggests that scaling up the training instances is in need to
improve the performance of PtrNet on solving higher-dimensional COPs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>None</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-10T00:00:00Z">2024-03-10</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computer Science and Game Theory
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disentangling Resilience from Robustness: Contextual Dualism,
  Interactionism, and <span class="highlight-title">Game</span>-Theoretic Paradigms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06299v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06299v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quanyan Zhu, Tamer Basar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article explains the distinctions between robustness and resilience in
control systems. Resilience confronts a distinct set of challenges, posing new
ones for designing controllers for feedback systems, networks, and machines
that prioritize resilience over robustness. The concept of resilience is
explored through a three-stage model, emphasizing the need for a proactive
preparation and automated response to elastic events. A toy model is first used
to illustrate the tradeoffs between resilience and robustness. Then, it delves
into contextual dualism and interactionism, and introduces game-theoretic
paradigms as a unifying framework to consolidate resilience and robustness. The
article concludes by discussing the interplay between robustness and
resilience, suggesting that a comprehensive theory of resilience and
quantification metrics, and formalization through game-theoretic frameworks are
necessary. The exploration extends to system-of-systems resilience and various
mechanisms, including the integration of AI techniques and non-technical
solutions, like cyber insurance, to achieve comprehensive resilience in control
systems. As we approach 2030, the systems and control community is at the
opportune moment to lay scientific foundations of resilience by bridging
feedback control theory, game theory, and learning theory. Resilient control
systems will enhance overall quality of life, enable the development of a
resilient society, and create a societal-scale impact amid global challenges
such as climate change, conflicts, and cyber insecurity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pre- and Post-Auction Discounts in First-Price Auctions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miguel Alcobendas, Eric Bax
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One method to offer some bidders a discount in a first-price auction is to
augment their bids when selecting a winner but only charge them their original
bids should they win. Another method is to use their original bids to select a
winner, then charge them a discounted price that is lower than their bid should
they win. We show that the two methods have equivalent auction outcomes, for
equal additive discounts and for multiplicative ones with appropriate
adjustments to discount amounts. As a result, they have corresponding
equilibria when equilibria exist. We also show that with the same level of
multiplicative adjustments, bidders with discounts should prefer an augmented
bid to a discounted price. Then we estimate optimal bid functions for valuation
distributions based on data from online advertising auctions, and show how
different discount levels affect auction outcomes for those bid functions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pursuit Winning Strategies for Reach-Avoid <span class="highlight-title">Game</span>s with Polygonal
  Obstacles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06202v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06202v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Yan, Shuai Mi, Xiaoming Duan, Jintao Chen, Xiangyang Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies a multiplayer reach-avoid differential game in the
presence of general polygonal obstacles that block the players' motions. The
pursuers cooperate to protect a convex region from the evaders who try to reach
the region. We propose a multiplayer onsite and close-to-goal (MOCG) pursuit
strategy that can tell and achieve an increasing lower bound on the number of
guaranteed defeated evaders. This pursuit strategy fuses the subgame outcomes
for multiple pursuers against one evader with hierarchical optimal task
allocation in the receding-horizon manner. To determine the qualitative subgame
outcomes that who is the game winner, we construct three pursuit winning
regions and strategies under which the pursuers guarantee to win against the
evader, regardless of the unknown evader strategy. First, we utilize the
expanded Apollonius circles and propose the onsite pursuit winning that
achieves the capture in finite time. Second, we introduce convex goal-covering
polygons (GCPs) and propose the close-to-goal pursuit winning for the pursuers
whose visibility region contains the whole protected region, and the
goal-visible property will be preserved afterwards. Third, we employ Euclidean
shortest paths (ESPs) and construct a pursuit winning region and strategy for
the non-goal-visible pursuers, where the pursuers are firstly steered to
positions with goal visibility along ESPs. In each horizon, the hierarchical
optimal task allocation maximizes the number of defeated evaders and consists
of four sequential matchings: capture, enhanced, non-dominated and closest
matchings. Numerical examples are presented to illustrate the results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Detection of <span class="highlight-title">Review</span>er-Author Collusion Rings From Paper Bidding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07860v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07860v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steven Jecmen, Nihar B. Shah, Fei Fang, Leman Akoglu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A major threat to the peer-review systems of computer science conferences is
the existence of "collusion rings" between reviewers. In such collusion rings,
reviewers who have also submitted their own papers to the conference work
together to manipulate the conference's paper assignment, with the aim of being
assigned to review each other's papers. The most straightforward way that
colluding reviewers can manipulate the paper assignment is by indicating their
interest in each other's papers through strategic paper bidding. One potential
approach to solve this important problem would be to detect the colluding
reviewers from their manipulated bids, after which the conference can take
appropriate action. While prior work has developed effective techniques to
detect other kinds of fraud, no research has yet established that detecting
collusion rings is even possible. In this work, we tackle the question of
whether it is feasible to detect collusion rings from the paper bidding. To
answer this question, we conduct empirical analysis of two realistic conference
bidding datasets, including evaluations of existing algorithms for fraud
detection in other applications. We find that collusion rings can achieve
considerable success at manipulating the paper assignment while remaining
hidden from detection: for example, in one dataset, undetected colluders are
able to achieve assignment to up to 30% of the papers authored by other
colluders. In addition, when 10 colluders bid on all of each other's papers, no
detection algorithm outputs a group of reviewers with more than 31% overlap
with the true colluders. These results suggest that collusion cannot be
effectively detected from the bidding using popular existing tools,
demonstrating the need to develop more complex detection algorithms as well as
those that leverage additional metadata (e.g., reviewer-paper text-similarity
scores).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the tractability of Nash equilibrium 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.05644v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.05644v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ioannis Avramopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a method for solving a PPAD-complete problem
[Papadimitriou, 1994]. Given is the payoff matrix $C$ of a symmetric bimatrix
game $(C, C^T)$ and our goal is to compute a Nash equilibrium of $(C, C^T)$. In
this paper, we devise a nonlinear replicator dynamic (whose right-hand-side can
be obtained by solving a pair of convex optimization problems) with the
following property: Under any invertible $0 < C \leq 1$, every orbit of our
dynamic starting at an interior strategy of the standard simplex approaches a
set of strategies of $(C, C^T)$ such that, for each strategy in this set, a
symmetric Nash equilibrium strategy can be computed by solving the
aforementioned convex mathematical programs. We prove convergence using results
in analysis (the analytic implicit function theorem), nonlinear optimization
theory (duality theory, Berge's maximum principle, and a theorem of Robinson
[1980] on the Lipschitz continuity of parametric nonlinear programs), and
dynamical systems theory (such as the LaSalle invariance principle).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Computing Optimal Commitments to Strategies and Outcome-Conditional
  Utility Transfers <span class="chip">AAMAS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.06626v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.06626v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathaniel Sauerberg, Caspar Oesterheld
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior work has studied the computational complexity of computing optimal
strategies to commit to in Stackelberg or leadership games, where a leader
commits to a strategy which is observed by one or more followers. We extend
this setting to one where the leader can additionally commit to
outcome-conditional utility transfers. We characterize the computational
complexity of finding optimal strategies in normal-form and Bayesian games,
giving a mix of efficient algorithms and NP-hardness results. Finally, we allow
the leader to also commit to a signaling scheme which induces a correlated
equilibrium. In this setting, optimal commitments can be found in polynomial
time for arbitrarily many players.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAMAS 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Probabilistic Neural Circuits <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06235v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06235v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro Zuidberg Dos Martires
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Probabilistic circuits (PCs) have gained prominence in recent years as a
versatile framework for discussing probabilistic models that support tractable
queries and are yet expressive enough to model complex probability
distributions. Nevertheless, tractability comes at a cost: PCs are less
expressive than neural networks. In this paper we introduce probabilistic
neural circuits (PNCs), which strike a balance between PCs and neural nets in
terms of tractability and expressive power. Theoretically, we show that PNCs
can be interpreted as deep mixtures of Bayesian networks. Experimentally, we
demonstrate that PNCs constitute powerful function approximators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the AAAI Conference on Artificial Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep <span class="highlight-title">Reinforcement</span> Learning with Spiking Q-learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.09754v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.09754v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ding Chen, Peixi Peng, Tiejun Huang, Yonghong Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the help of special neuromorphic hardware, spiking neural networks
(SNNs) are expected to realize artificial intelligence (AI) with less energy
consumption. It provides a promising energy-efficient way for realistic control
tasks by combining SNNs with deep reinforcement learning (RL). There are only a
few existing SNN-based RL methods at present. Most of them either lack
generalization ability or employ Artificial Neural Networks (ANNs) to estimate
value function in training. The former needs to tune numerous hyper-parameters
for each scenario, and the latter limits the application of different types of
RL algorithm and ignores the large energy consumption in training. To develop a
robust spike-based RL method, we draw inspiration from non-spiking interneurons
found in insects and propose the deep spiking Q-network (DSQN), using the
membrane voltage of non-spiking neurons as the representation of Q-value, which
can directly learn robust policies from high-dimensional sensory inputs using
end-to-end RL. Experiments conducted on 17 Atari games demonstrate the DSQN is
effective and even outperforms the ANN-based deep Q-network (DQN) in most
games. Moreover, the experiments show superior learning stability and
robustness to adversarial attacks of DSQN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-09T00:00:00Z">2024-03-09</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computer Science and Game Theory
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Flow <span class="highlight-title">Game</span>: Leximin and Leximax Core Imputations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06037v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06037v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohith R. Gangam, Naveen Garg, Parnian Shahkar, Vijay V. Vazirani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently [Vaz24] gave mechanisms for finding leximin and leximax core
imputations for the assignment game and remarked, "Within the area of algorithm
design, the "right" technique for solving several types of algorithmic
questions was first discovered in the context of matching and later these
insights were applied to other problems. We expect a similar phenomenon here."
One of the games explicitly mentioned in this context was the flow game of
Kalai and Zemel [KZ82]. In this paper, we give strongly polynomial time
mechanisms for computing the leximin and leximax core imputations for the flow
game, among the set of core imputations that are captured as optimal solutions
to the dual LP. We address two versions: 1. The imputations are leximin and
leximax with respect to the distance labels of edges. 2. The imputations are
leximin and leximax with respect to the product of capacities of edges and
their distance labels.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mathematics of multi-<span class="highlight-title">agent</span> learning systems at the interface of <span class="highlight-title">game</span>
  theory and artificial intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07017v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07017v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Wang, Feng Fu, Xingru Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evolutionary Game Theory (EGT) and Artificial Intelligence (AI) are two
fields that, at first glance, might seem distinct, but they have notable
connections and intersections. The former focuses on the evolution of behaviors
(or strategies) in a population, where individuals interact with others and
update their strategies based on imitation (or social learning). The more
successful a strategy is, the more prevalent it becomes over time. The latter,
meanwhile, is centered on machine learning algorithms and (deep) neural
networks. It is often from a single-agent perspective but increasingly involves
multi-agent environments, in which intelligent agents adjust their strategies
based on feedback and experience, somewhat akin to the evolutionary process yet
distinct in their self-learning capacities. In light of the key components
necessary to address real-world problems, including (i) learning and
adaptation, (ii) cooperation and competition, (iii) robustness and stability,
and altogether (iv) population dynamics of individual agents whose strategies
evolve, the cross-fertilization of ideas between both fields will contribute to
the advancement of mathematics of multi-agent learning systems, in particular,
to the nascent domain of ``collective cooperative intelligence'' bridging
evolutionary dynamics and multi-agent reinforcement learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Provable <span class="highlight-title">Policy</span> Gradient Methods for Average-Reward Markov Potential
  <span class="highlight-title">Game</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05738v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05738v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Min Cheng, Ruida Zhou, P. R. Kumar, Chao Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study Markov potential games under the infinite horizon average reward
criterion. Most previous studies have been for discounted rewards. We prove
that both algorithms based on independent policy gradient and independent
natural policy gradient converge globally to a Nash equilibrium for the average
reward criterion. To set the stage for gradient-based methods, we first
establish that the average reward is a smooth function of policies and provide
sensitivity bounds for the differential value functions, under certain
conditions on ergodicity and the second largest eigenvalue of the underlying
Markov decision process (MDP). We prove that three algorithms, policy gradient,
proximal-Q, and natural policy gradient (NPG), converge to an $\epsilon$-Nash
equilibrium with time complexity $O(\frac{1}{\epsilon^2})$, given a
gradient/differential Q function oracle. When policy gradients have to be
estimated, we propose an algorithm with
$\tilde{O}(\frac{1}{\min_{s,a}\pi(a|s)\delta})$ sample complexity to achieve
$\delta$ approximation error w.r.t~the $\ell_2$ norm. Equipped with the
estimator, we derive the first sample complexity analysis for a policy gradient
ascent algorithm, featuring a sample complexity of $\tilde{O}(1/\epsilon^5)$.
Simulation studies are presented.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 7 figures, published to AISTAT-24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Machine Learning-Powered Course Allocation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.00954v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.00954v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ermis Soumalias, Behnoosh Zamanlooy, Jakob Weissteiner, Sven Seuken
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the course allocation problem, where universities assign course
schedules to students. The current state-of-the-art mechanism, Course Match,
has one major shortcoming: students make significant mistakes when reporting
their preferences, which negatively affects welfare and fairness. To address
this issue, we introduce a new mechanism, Machine Learning-powered Course Match
(MLCM). At the core of MLCM is a machine learning-powered preference
elicitation module that iteratively asks personalized pairwise comparison
queries to alleviate students' reporting mistakes. Extensive computational
experiments, grounded in real-world data, demonstrate that MLCM, with only ten
comparison queries, significantly increases both average and minimum student
utility by 7%-11% and 17%-29%, respectively. Finally, we highlight MLCM's
robustness to changes in the environment and show how our design minimizes the
risk of upgrading to MLCM while making the upgrade process simple for
universities and seamless for their students.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Markov $α$-Potential <span class="highlight-title">Game</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.12553v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.12553v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Guo, Xinyu Li, Chinmay Maheshwari, Shankar Sastry, Manxi Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a new framework of Markov $\alpha$-potential games to
study Markov games. In this new framework, Markov games are shown to be Markov
$\alpha$-potential games, and the existence of an associated $\alpha$-potential
function is established. Any optimizer of an $\alpha$-potential function is
shown to be an $\alpha$-stationary NE. Two important classes of practically
significant Markov games, Markov congestion games and the perturbed Markov team
games, are studied via this framework of Markov $\alpha$-potential games, with
explicit characterization of an upper bound for $\alpha$ and its relation to
game parameters. Additionally, a semi-infinite linear programming based
formulation is presented to obtain an upper bound for $\alpha$ for any Markov
game. Furthermore, two equilibrium approximation algorithms, namely the
projected gradient-ascent algorithm and the sequential maximum improvement
algorithm, are presented along with their Nash regret analysis, and
corroborated by numerical experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Strategy</span> Synthesis for Zero-Sum Neuro-Symbolic Concurrent Stochastic
  <span class="highlight-title">Game</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.06255v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.06255v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Yan, Gabriel Santos, Gethin Norman, David Parker, Marta Kwiatkowska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neuro-symbolic approaches to artificial intelligence, which combine neural
networks with classical symbolic techniques, are growing in prominence,
necessitating formal approaches to reason about their correctness. We propose a
novel modelling formalism called neuro-symbolic concurrent stochastic games
(NS-CSGs), which comprise two probabilistic finite-state agents interacting in
a shared continuous-state environment. Each agent observes the environment
using a neural perception mechanism, which converts inputs such as images into
symbolic percepts, and makes decisions symbolically. We focus on the class of
NS-CSGs with Borel state spaces and prove the existence and measurability of
the value function for zero-sum discounted cumulative rewards under
piecewise-constant restrictions on the components of this class of models. To
compute values and synthesise strategies, we present, for the first time,
practical value iteration (VI) and policy iteration (PI) algorithms to solve
this new subclass of continuous-state CSGs. These require a finite
decomposition of the environment induced by the neural perception mechanisms of
the agents and rely on finite abstract representations of value functions and
strategies closed under VI or PI. First, we introduce a Borel measurable
piecewise-constant (B-PWC) representation of value functions, extend minimax
backups to this representation and propose a value iteration algorithm called
B-PWC VI. Second, we introduce two novel representations for the value
functions and strategies, constant-piecewise-linear (CON-PWL) and
constant-piecewise-constant (CON-PWC) respectively, and propose
Minimax-action-free PI by extending a recent PI method based on alternating
player choices for finite state spaces to Borel state spaces, which does not
require normal-form games to be solved.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>58 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Performance Analysis of Basin Hopping Compared to Established
  Metaheuristics for Global Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05877v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05877v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Baioletti, Valentino Santucci, Marco Tomassini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  During the last decades many metaheuristics for global numerical optimization
have been proposed. Among them, Basin Hopping is very simple and
straightforward to implement, although rarely used outside its original
Physical Chemistry community. In this work, our aim is to compare Basin
Hopping, and two population variants of it, with readily available
implementations of the well known metaheuristics Differential Evolution,
Particle Swarm Optimization, and Covariance Matrix Adaptation Evolution
Strategy. We perform numerical experiments using the IOH profiler environment
with the BBOB test function set and two difficult real-world problems. The
experiments were carried out in two different but complementary ways: by
measuring the performance under a fixed budget of function evaluations and by
considering a fixed target value. The general conclusion is that Basin Hopping
and its newly introduced population variant are almost as good as Covariance
Matrix Adaptation on the synthetic benchmark functions and better than it on
the two hard cluster energy minimization problems. Thus, the proposed analyses
show that Basin Hopping can be considered a good candidate for global numerical
optimization problems along with the more established metaheuristics,
especially if one wants to obtain quick and reliable results on an unknown
problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long-term Frame-Event Visual Tracking: Benchmark <span class="highlight-title">Dataset</span> and Baseline 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05839v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05839v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Wang, Ju Huang, Shiao Wang, Chuanming Tang, Bo Jiang, Yonghong Tian, Jin Tang, Bin Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current event-/frame-event based trackers undergo evaluation on short-term
tracking datasets, however, the tracking of real-world scenarios involves
long-term tracking, and the performance of existing tracking algorithms in
these scenarios remains unclear. In this paper, we first propose a new
long-term and large-scale frame-event single object tracking dataset, termed
FELT. It contains 742 videos and 1,594,474 RGB frames and event stream pairs
and has become the largest frame-event tracking dataset to date. We re-train
and evaluate 15 baseline trackers on our dataset for future works to compare.
More importantly, we find that the RGB frames and event streams are naturally
incomplete due to the influence of challenging factors and spatially sparse
event flow. In response to this, we propose a novel associative memory
Transformer network as a unified backbone by introducing modern Hopfield layers
into multi-head self-attention blocks to fuse both RGB and event data.
Extensive experiments on both FELT and RGB-T tracking dataset LasHeR fully
validated the effectiveness of our model. The dataset and source code can be
found at \url{https://github.com/Event-AHU/FELT_SOT_Benchmark}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Peer Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ sVAD: A Robust, Low-Power, and Light-Weight Voice Activity Detection
  with Spiking Neural Networks <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05772v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05772v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qu Yang, Qianhui Liu, Nan Li, Meng Ge, Zeyang Song, Haizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech applications are expected to be low-power and robust under noisy
conditions. An effective Voice Activity Detection (VAD) front-end lowers the
computational need. Spiking Neural Networks (SNNs) are known to be biologically
plausible and power-efficient. However, SNN-based VADs have yet to achieve
noise robustness and often require large models for high performance. This
paper introduces a novel SNN-based VAD model, referred to as sVAD, which
features an auditory encoder with an SNN-based attention mechanism.
Particularly, it provides effective auditory feature representation through
SincNet and 1D convolution, and improves noise robustness with attention
mechanisms. The classifier utilizes Spiking Recurrent Neural Networks (sRNN) to
exploit temporal speech information. Experimental results demonstrate that our
sVAD achieves remarkable noise robustness and meanwhile maintains low power
consumption and a small footprint, making it a promising solution for
real-world VAD applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-08T00:00:00Z">2024-03-08</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computer Science and Game Theory
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Online Contention Resolution Schemes for Network Revenue Management and
  Combinatorial Auctions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05378v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05378v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Will Ma, Calum MacRury, Jingwei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the Network Revenue Management (NRM) problem, products composed of up to L
resources are sold to stochastically arriving customers. We take a randomized
rounding approach to NRM, motivated by developments in Online Contention
Resolution Schemes (OCRS). The goal is to take a fractional solution to NRM
that satisfies the resource constraints in expectation, and implement it in an
online policy that satisfies the resource constraints in any state, while
(approximately) preserving all of the sales that were prescribed by the
fractional solution.
  OCRS cannot be naively applied to NRM or revenue management problems in
general, because customer substitution induces a negative correlation in
products being demanded. We start by deriving an OCRS that achieves a guarantee
of 1/(1+L) for NRM with customer substitution, matching a common benchmark in
the literature. We then show how to beat this benchmark for all integers L>1
assuming no substitution, i.e., in the standard OCRS setting. By contrast, we
show that this benchmark is unbeatable using OCRS or any fractional relaxation
if there is customer substitution, for all integers L that are the power of a
prime number. Finally, we show how to beat 1/(1+L) even with customer
substitution, if the products comprise one item from each of up to L groups.
  Our results have corresponding implications for Online Combinatorial
Auctions, in which buyers bid for bundles of up to L items, and buyers being
single-minded is akin to no substitution. Our final result also beats 1/(1+L)
for Prophet Inequality on the intersection of L partition matroids. All in all,
our paper provides a unifying framework for applying OCRS to these problems,
delineating the impact of substitution, and establishing a separation between
the guarantees achievable with vs. without substitution under general resource
constraints parametrized by L.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Elections in the Post-Quantum Era: Is the Complexity Shield Strong
  Enough? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05273v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05273v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Šimon Schierreich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The election, a cornerstone of democracy, is one of the best-recognizable
symbols of democratic governance. Voters' confidence in elections is essential,
and these days, we can watch practically in live broadcast what consequences
distrust in the fairness of elections may have. From the times of the
celebrated Gibbard-Satterthwaite theorem, it is well-known in the social-choice
community that most voting systems are vulnerable to the efforts of various
players to influence elections. Luckily for us, computing such influence to
affect election outcomes is a hard problem from the computational complexity
perspective. This intractability is regarded as a ``complexity shield'' that
secures voting rules against this malicious behavior.
  In this work, we consider quantum computers to be a new threat to the
complexity shield described above, as they break out of standard computing
paradigms and unlock additional computational resources. To this end, we
provide an overview of possible attacks on election, discuss the abilities of
quantum computing, and chart possible directions for future research in this
area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Task-Driven Multi-UAV Coalition Formation Mechanism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05108v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05108v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinpeng Lu, Heng Song, Huailing Ma, Junwu Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid advancement of UAV technology, the problem of UAV coalition
formation has become a hotspot. Therefore, designing task-driven multi-UAV
coalition formation mechanism has become a challenging problem. However,
existing coalition formation mechanisms suffer from low relevance between UAVs
and task requirements, resulting in overall low coalition utility and unstable
coalition structures. To address these problems, this paper proposed a novel
multi-UAV coalition network collaborative task completion model, considering
both coalition work capacity and task-requirement relationships. This model
stimulated the formation of coalitions that match task requirements by using a
revenue function based on the coalition's revenue threshold. Subsequently, an
algorithm for coalition formation based on marginal utility was proposed.
Specifically, the algorithm utilized Shapley value to achieve fair utility
distribution within the coalition, evaluated coalition values based on marginal
utility preference order, and achieved stable coalition partition through a
limited number of iterations. Additionally, we theoretically proved that this
algorithm has Nash equilibrium solution. Finally, experimental results
demonstrated that the proposed algorithm, compared to currently classical
algorithms, not only forms more stable coalitions but also further enhances the
overall utility of coalitions effectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safe Pareto Improvements for Expected Utility Maximizers in Program
  <span class="highlight-title">Game</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05103v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05103v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anthony DiGiovanni, Jesse Clifton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Agents in mixed-motive coordination problems such as Chicken may fail to
coordinate on a Pareto-efficient outcome. Safe Pareto improvements (SPIs) were
originally proposed to mitigate miscoordination in cases where players lack
probabilistic beliefs as to how their delegates will play a game; delegates are
instructed to behave so as to guarantee a Pareto improvement on how they would
play by default. More generally, SPIs may be defined as transformations of
strategy profiles such that all players are necessarily better off under the
transformed profile. In this work, we investigate the extent to which SPIs can
reduce downsides of miscoordination between expected utility-maximizing agents.
We consider games in which players submit computer programs that can condition
their decisions on each other's code, and use this property to construct SPIs
using programs capable of renegotiation. We first show that under mild
conditions on players' beliefs, each player always prefers to use
renegotiation. Next, we show that under similar assumptions, each player always
prefers to be willing to renegotiate at least to the point at which they
receive the lowest payoff they can attain in any efficient outcome. Thus
subjectively optimal play guarantees players at least these payoffs, without
the need for coordination on specific Pareto improvements. Lastly, we prove
that renegotiation does not guarantee players any improvements on this bound.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conjectural Online Learning with First-order Beliefs in Asymmetric
  Information Stochastic <span class="highlight-title">Game</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18781v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18781v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Li, Kim Hammar, Rolf Stadler, Quanyan Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Asymmetric information stochastic games (\textsc{aisg}s) arise in many
complex socio-technical systems, such as cyber-physical systems and IT
infrastructures. Existing computational methods for \textsc{aisg}s are
primarily offline and can not adapt to equilibrium deviations. Further, current
methods are limited to special classes of \textsc{aisg}s to avoid belief
hierarchies. To address these limitations, we propose conjectural online
learning (\textsc{col}), an online method for generic \textsc{aisg}s.
\textsc{col} uses a forecaster-actor-critic (\textsc{fac}) architecture where
subjective forecasts are used to conjecture the opponents' strategies within a
lookahead horizon, and Bayesian learning is used to calibrate the conjectures.
To adapt strategies to nonstationary environments, \textsc{col} uses online
rollout with cost function approximation (actor-critic). We prove that the
conjectures produced by \textsc{col} are asymptotically consistent with the
information feedback in the sense of a relaxed Bayesian consistency. We also
prove that the empirical strategy profile induced by \textsc{col} converges to
the Berk-Nash equilibrium, a solution concept characterizing rationality under
subjectivity. Experimental results from an intrusion response use case
demonstrate \textsc{col}'s superiority over state-of-the-art reinforcement
learning methods against nonstationary attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Strategic Usage in a Multi-Learner Setting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.16422v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.16422v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eliot Shekhtman, Sarah Dean
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world systems often involve some pool of users choosing between a set of
services. With the increase in popularity of online learning algorithms, these
services can now self-optimize, leveraging data collected on users to maximize
some reward such as service quality. On the flipside, users may strategically
choose which services to use in order to pursue their own reward functions, in
the process wielding power over which services can see and use their data.
Extensive prior research has been conducted on the effects of strategic users
in single-service settings, with strategic behavior manifesting in the
manipulation of observable features to achieve a desired classification;
however, this can often be costly or unattainable for users and fails to
capture the full behavior of multi-service dynamic systems. As such, we analyze
a setting in which strategic users choose among several available services in
order to pursue positive classifications, while services seek to minimize loss
functions on their observations. We focus our analysis on realizable settings,
and show that naive retraining can still lead to oscillation even if all users
are observed at different times; however, if this retraining uses memory of
past observations, convergent behavior can be guaranteed for certain loss
function classes. We provide results obtained from synthetic and real-world
data to empirically validate our theoretical findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Linear Programming Based Near-Optimal Pricing for Laminar Bayesian
  Online <span class="highlight-title">Selection</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1807.05477v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1807.05477v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nima Anari, Rad Niazadeh, Amin Saberi, Ali Shameli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Bayesian online selection problem aims to design a pricing scheme for a
sequence of arriving buyers that maximizes the expected social welfare (or
revenue) subject to different structural constraints. Inspired by applications
with a hierarchy of service, this paper focuses on the cases where a laminar
matroid characterizes the set of served buyers. We give the first
Polynomial-Time Approximation Scheme (PTAS) for the problem when the laminar
matroid has constant depth. Our approach is based on rounding the solution of a
hierarchy of linear programming relaxations that approximate the optimum online
solution with any degree of accuracy, plus a concentration argument showing
that rounding incurs a small loss. We also study another variation, which we
call the production-constrained problem. The allowable set of served buyers is
characterized by a collection of production and shipping constraints that form
a particular example of a laminar matroid. Using a similar LP-based approach,
we design a PTAS for this problem, although in this special case the depth of
the underlying laminar matroid is not necessarily a constant. The analysis
exploits the negative dependency of the optimum selection rule in the lower
levels of the laminar family. Finally, to demonstrate the generality of our
technique, we employ the linear programming-based approach employed in the
paper to re-derive some of the classic prophet inequalities known in the
literature -- as a side result.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>conference version of this paper appeared as one-page abstract in ACM
  EC 2019</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The outcomes of generative AI are exactly the Nash equilibria of a
  non-potential <span class="highlight-title">game</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.12321v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.12321v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boualem Djehiche, Hamidou Tembine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this article we show that the asymptotic outcomes of both shallow and deep
neural networks such as those used in BloombergGPT to generate economic time
series are exactly the Nash equilibria of a non-potential game. We then design
and analyze deep neural network algorithms that converge to these equilibria.
The methodology is extended to federated deep neural networks between clusters
of regional servers and on-device clients. Finally, the variational
inequalities behind large language models including encoder-decoder related
transformers are established.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated version 1. 24 pages. Accepted and to appear in: International
  Econometric Conference of Vietnam</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On marginal feature attributions of tree-based models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.08434v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.08434v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khashayar Filom, Alexey Miroshnikov, Konstandinos Kotsiopoulos, Arjun Ravi Kannan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to their power and ease of use, tree-based machine learning models, such
as random forests and gradient-boosted tree ensembles, have become very
popular. To interpret them, local feature attributions based on marginal
expectations, e.g. marginal (interventional) Shapley, Owen or Banzhaf values,
may be employed. Such methods are true to the model and implementation
invariant, i.e. dependent only on the input-output function of the model. We
contrast this with the popular TreeSHAP algorithm by presenting two
(statistically similar) decision trees that compute the exact same function for
which the "path-dependent" TreeSHAP yields different rankings of features,
whereas the marginal Shapley values coincide. Furthermore, we discuss how the
internal structure of tree-based models may be leveraged to help with computing
their marginal feature attributions according to a linear game value. One
important observation is that these are simple (piecewise-constant) functions
with respect to a certain grid partition of the input space determined by the
trained model. Another crucial observation, showcased by experiments with
XGBoost, LightGBM and CatBoost libraries, is that only a portion of all
features appears in a tree from the ensemble. Thus, the complexity of computing
marginal Shapley (or Owen or Banzhaf) feature attributions may be reduced. This
remains valid for a broader class of game values which we shall axiomatically
characterize. A prime example is the case of CatBoost models where the trees
are oblivious (symmetric) and the number of features in each of them is no
larger than the depth. We exploit the symmetry to derive an explicit formula,
with improved complexity and only in terms of the internal model parameters,
for marginal Shapley (and Banzhaf and Owen) values of CatBoost models. This
results in a fast, accurate algorithm for estimating these feature
attributions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Typos corrected. Section 4 is reorganized and new experiments on Owen
  values are added. Minor changes in the Introduction. 30 pages+appendix (64
  pages in total), 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast, Fair and Truthful Distributed Stable Matching for Common
  Preferences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16532v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16532v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juho Hirvonen, Sara Ranjbaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stable matching is a fundamental problem studied both in economics and
computer science. The task is to find a matching between two sides of agents
that have preferences over who they want to be matched with. A matching is
stable if no pair of agents prefer each other over their current matches. The
deferred acceptance algorithm of Gale and Shapley solves this problem in
polynomial time. Further, it is a mechanism: the proposing side in the
algorithm is always incentivised to report their preferences truthfully. The
deferred acceptance algorithm has a natural interpretation as a distributed
algorithm (and thus a distributed mechanism). However, the algorithm is slow in
the worst case and it is known that the stable matching problem cannot be
solved efficiently in the distributed setting. In this work we study a natural
special case of the stable matching problem where all agents on one side share
common preferences. We show that in this case the deferred acceptance algorithm
does yield a fast and truthful distributed mechanism for finding a stable
matching. We show how algorithms for sampling random colorings can be used to
break ties fairly and extend the results to fractional stable matching.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ECToNAS: Evolutionary Cross-Topology Neural Architecture Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05123v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05123v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elisabeth J. Schiessler, Roland C. Aydin, Christian J. Cyron
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present ECToNAS, a cost-efficient evolutionary cross-topology neural
architecture search algorithm that does not require any pre-trained meta
controllers. Our framework is able to select suitable network architectures for
different tasks and hyperparameter settings, independently performing
cross-topology optimisation where required. It is a hybrid approach that fuses
training and topology optimisation together into one lightweight,
resource-friendly process. We demonstrate the validity and power of this
approach with six standard data sets (CIFAR-10, CIFAR-100, EuroSAT, Fashion
MNIST, MNIST, SVHN), showcasing the algorithm's ability to not only optimise
the topology within an architectural type, but also to dynamically add and
remove convolutional cells when and where required, thus crossing boundaries
between different network types. This enables researchers without a background
in machine learning to make use of appropriate model types and topologies and
to apply machine learning methods in their domains, with a computationally
cheap, easy-to-use cross-topology neural architecture search framework that
fully encapsulates the topology optimisation within the training process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 7 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Linearly Constrained Weights: Reducing Activation Shift for Faster
  Training of Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13833v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13833v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takuro Kutsuna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we first identify activation shift, a simple but remarkable
phenomenon in a neural network in which the preactivation value of a neuron has
non-zero mean that depends on the angle between the weight vector of the neuron
and the mean of the activation vector in the previous layer. We then propose
linearly constrained weights (LCW) to reduce the activation shift in both fully
connected and convolutional layers. The impact of reducing the activation shift
in a neural network is studied from the perspective of how the variance of
variables in the network changes through layer operations in both forward and
backward chains. We also discuss its relationship to the vanishing gradient
problem. Experimental results show that LCW enables a deep feedforward network
with sigmoid activation functions to be trained efficiently by resolving the
vanishing gradient problem. Moreover, combined with batch normalization, LCW
improves generalization performance of both feedforward and convolutional
networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards a tailored mixed-precision sub-8-bit quantization scheme for
  Gated Recurrent Units using Genetic Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12263v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12263v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riccardo Miccini, Alessandro Cerioli, Clément Laroche, Tobias Piechowiak, Jens Sparsø, Luca Pezzarossa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the recent advances in model compression techniques for deep neural
networks, deploying such models on ultra-low-power embedded devices still
proves challenging. In particular, quantization schemes for Gated Recurrent
Units (GRU) are difficult to tune due to their dependence on an internal state,
preventing them from fully benefiting from sub-8bit quantization. In this work,
we propose a modular integer quantization scheme for GRUs where the bit width
of each operator can be selected independently. We then employ Genetic
Algorithms (GA) to explore the vast search space of possible bit widths,
simultaneously optimising for model size and accuracy. We evaluate our methods
on four different sequential tasks and demonstrate that mixed-precision
solutions exceed homogeneous-precision ones in terms of Pareto efficiency. In
our results, we achieve a model size reduction between 25% and 55% while
maintaining an accuracy comparable with the 8-bit homogeneous equivalent.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a full paper by the tinyML Research Symposium 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Composite Decomposition Method for Large-Scale Global Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01192v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01192v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maojiang Tian, Minyang Chen, Wei Du, Yang Tang, Yaochu Jin, Gary G. Yen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cooperative co-evolution (CC) algorithms, based on the divide-and-conquer
strategy, have emerged as the predominant approach to solving large-scale
global optimization (LSGO) problems. The efficiency and accuracy of the
grouping stage significantly impact the performance of the optimization
process. While the general separability grouping (GSG) method has overcome the
limitation of previous differential grouping (DG) methods by enabling the
decomposition of non-additively separable functions, it suffers from high
computational complexity. To address this challenge, this article proposes a
composite separability grouping (CSG) method, seamlessly integrating DG and GSG
into a problem decomposition framework to utilize the strengths of both
approaches. CSG introduces a step-by-step decomposition framework that
accurately decomposes various problem types using fewer computational
resources. By sequentially identifying additively, multiplicatively and
generally separable variables, CSG progressively groups non-separable variables
by recursively considering the interactions between each non-separable variable
and the formed non-separable groups. Furthermore, to enhance the efficiency and
accuracy of CSG, we introduce two innovative methods: a multiplicatively
separable variable detection method and a non-separable variable grouping
method. These two methods are designed to effectively detect multiplicatively
separable variables and efficiently group non-separable variables,
respectively. Extensive experimental results demonstrate that CSG achieves more
accurate variable grouping with lower computational complexity compared to GSG
and state-of-the-art DG series designs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Looking for Complexity at Phase Boundaries in Continuous Cellular
  Automata 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17848v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17848v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vassilis Papadopoulos, Guilhem Doat, Arthur Renard, Clément Hongler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One key challenge in Artificial Life is designing systems that display an
emergence of complex behaviors. Many such systems depend on a high-dimensional
parameter space, only a small subset of which displays interesting dynamics.
Focusing on the case of continuous systems, we introduce the 'Phase Transition
Finder'(PTF) algorithm, which can be used to efficiently generate parameters
lying at the border between two phases. We argue that such points are more
likely to display complex behaviors, and confirm this by applying PTF to Lenia
showing it can increase the frequency of interesting behaviors more than
two-fold, while remaining efficient enough for large-scale searches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-07T00:00:00Z">2024-03-07</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computer Science and Game Theory
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stochastic <span class="highlight-title">Game</span>s for Interactive Manipulation Domains <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04910v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04910v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karan Muvvala, Andrew M. Wells, Morteza Lahijanian, Lydia E. Kavraki, Moshe Y. Vardi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As robots become more prevalent, the complexity of robot-robot, robot-human,
and robot-environment interactions increases. In these interactions, a robot
needs to consider not only the effects of its own actions, but also the effects
of other agents' actions and the possible interactions between agents. Previous
works have considered reactive synthesis, where the human/environment is
modeled as a deterministic, adversarial agent; as well as probabilistic
synthesis, where the human/environment is modeled via a Markov chain. While
they provide strong theoretical frameworks, there are still many aspects of
human-robot interaction that cannot be fully expressed and many assumptions
that must be made in each model. In this work, we propose stochastic games as a
general model for human-robot interaction, which subsumes the expressivity of
all previous representations. In addition, it allows us to make fewer modeling
assumptions and leads to more natural and powerful models of interaction. We
introduce the semantics of this abstraction and show how existing tools can be
utilized to synthesize strategies to achieve complex tasks with guarantees.
Further, we discuss the current computational limitations and improve the
scalability by two orders of magnitude by a new way of constructing models for
PRISM-games.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted: ICRA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Winner-Pays-Bid Auctions Minimize Variance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04856v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04856v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Preston McAfee, Renato Paes Leme, Balasubramanian Sivan, Sergei Vassilvitskii
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Any social choice function (e.g the efficient allocation) can be implemented
using different payment rules: first price, second price, all-pay, etc. All of
these payment rules are guaranteed to have the same expected revenue by the
revenue equivalence theorem, but have different distributions of revenue,
leading to a question of which one is best. We prove that among all possible
payment rules, winner-pays-bid minimizes the variance in revenue and, in fact,
minimizes any convex risk measure.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mechanism for Decision-aware Collaborative Federated Learning: A Pitfall
  of Shapley Values 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04753v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04753v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meng Qi, Mingxi Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates mechanism design for decision-aware collaboration via
federated learning (FL) platforms. Our framework consists of a digital platform
and multiple decision-aware agents, each endowed with proprietary data sets.
The platform offers an infrastructure that enables access to the data, creates
incentives for collaborative learning aimed at operational decision-making, and
conducts FL to avoid direct raw data sharing. The computation and communication
efficiency of the FL process is inherently influenced by the agent
participation equilibrium induced by the mechanism. Therefore, assessing the
system's efficiency involves two critical factors: the surplus created by
coalition formation and the communication costs incurred across the coalition
during FL. To evaluate the system efficiency under the intricate interplay
between mechanism design, agent participation, operational decision-making, and
the performance of FL algorithms, we introduce a multi-action collaborative
federated learning (MCFL) framework for decision-aware agents. Under this
framework, we further analyze the equilibrium for the renowned Shapley value
based mechanisms. Specifically, we examine the issue of false-name
manipulation, a form of dishonest behavior where participating agents create
duplicate fake identities to split their original data among these identities.
By solving the agent participation equilibrium, we demonstrate that while
Shapley value effectively maximizes coalition-generated surplus by encouraging
full participation, it inadvertently promotes false-name manipulation. This
further significantly increases the communication costs when the platform
conducts FL. Thus, we highlight a significant pitfall of Shapley value based
mechanisms, which implicitly incentivizes data splitting and identity
duplication, ultimately impairing the overall efficiency in FL systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extensive-Form <span class="highlight-title">Game</span> Solving via Blackwell Approachability on Treeplexes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04680v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04680v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Darshan Chakrabarti, Julien Grand-Clément, Christian Kroer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce the first algorithmic framework for Blackwell
approachability on the sequence-form polytope, the class of convex polytopes
capturing the strategies of players in extensive-form games (EFGs). This leads
to a new class of regret-minimization algorithms that are stepsize-invariant,
in the same sense as the Regret Matching and Regret Matching$^+$ algorithms for
the simplex. Our modular framework can be combined with any existing regret
minimizer over cones to compute a Nash equilibrium in two-player zero-sum EFGs
with perfect recall, through the self-play framework. Leveraging predictive
online mirror descent, we introduce Predictive Treeplex Blackwell$^+$
(PTB$^+$), and show a $O(1/\sqrt{T})$ convergence rate to Nash equilibrium in
self-play. We then show how to stabilize PTB$^+$ with a stepsize, resulting in
an algorithm with a state-of-the-art $O(1/T)$ convergence rate. We provide an
extensive set of experiments to compare our framework with several algorithmic
benchmarks, including CFR$^+$ and its predictive variant, and we highlight
interesting connections between practical performance and the
stepsize-dependence or stepsize-invariance properties of classical algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling reputation-based behavioral biases in school choice 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04616v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04616v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jon Kleinberg, Sigal Oren, Emily Ryu, Éva Tardos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A fundamental component in the theoretical school choice literature is the
problem a student faces in deciding which schools to apply to. Recent models
have considered a set of schools of different selectiveness and a student who
is unsure of their strength and can apply to at most $k$ schools. Such models
assume that the student cares solely about maximizing the quality of the school
that they attend, but experience suggests that students' decisions are also
influenced by a set of behavioral biases based on reputational effects: a
subjective reputational benefit when admitted to a selective school, whether or
not they attend; and a subjective loss based on disappointment when rejected.
Guided by these observations, and inspired by recent behavioral economics work
on loss aversion relative to expectations, we propose a behavioral model by
which a student chooses schools to balance these behavioral effects with the
quality of the school they attend.
  Our main results show that a student's choices change in dramatic ways when
these reputation-based behavioral biases are taken into account. In particular,
where a rational applicant spreads their applications evenly, a biased student
applies very sparsely to highly selective schools, such that above a certain
threshold they apply to only an absolute constant number of schools even as
their budget of applications grows to infinity. Consequently, a biased student
underperforms a rational student even when the rational student is restricted
to a sufficiently large upper bound on applications and the biased student can
apply to arbitrarily many. Our analysis shows that the reputation-based model
is rich enough to cover a range of different ways that biased students cope
with fear of rejection, including not just targeting less selective schools,
but also occasionally applying to schools that are too selective, compared to
rational students.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-District School Choice: Playing on Several Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yannai A. Gonczarowski, Michael Yin, Shirley Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We extend the seminal model of Pathak and S\"onmez (2008) to a setting with
multiple school districts, each running its own separate centralized match, and
focus on the case of two districts. In our setting, in addition to each student
being either sincere or sophisticated, she is also either constrained - able to
apply only to schools within her own district of residence - or unconstrained -
able to choose any single district within which to apply. We show that several
key results from Pathak and S\"onmez (2008) qualitatively flip: A sophisticated
student may prefer for a sincere student to become sophisticated, and a
sophisticated student may prefer for her own district to use Deferred
Acceptance over the Boston Mechanism, irrespective of the mechanism used by the
other district. We furthermore investigate the preferences of students over the
constraint levels of other students. Many of these phenomena appear abundantly
in large random markets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RL-CFR: Improving Action Abstraction for Imperfect Information
  Extensive-Form <span class="highlight-title">Game</span>s with <span class="highlight-title">Reinforcement</span> Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04344v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04344v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boning Li, Zhixuan Fang, Longbo Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective action abstraction is crucial in tackling challenges associated
with large action spaces in Imperfect Information Extensive-Form Games
(IIEFGs). However, due to the vast state space and computational complexity in
IIEFGs, existing methods often rely on fixed abstractions, resulting in
sub-optimal performance. In response, we introduce RL-CFR, a novel
reinforcement learning (RL) approach for dynamic action abstraction. RL-CFR
builds upon our innovative Markov Decision Process (MDP) formulation, with
states corresponding to public information and actions represented as feature
vectors indicating specific action abstractions. The reward is defined as the
expected payoff difference between the selected and default action
abstractions. RL-CFR constructs a game tree with RL-guided action abstractions
and utilizes counterfactual regret minimization (CFR) for strategy derivation.
Impressively, it can be trained from scratch, achieving higher expected payoff
without increased CFR solving time. In experiments on Heads-up No-limit Texas
Hold'em, RL-CFR outperforms ReBeL's replication and Slumbot, demonstrating
significant win-rate margins of $64\pm 11$ and $84\pm 17$ mbb/hand,
respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conflict and Fairness in Resource Allocation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04265v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04265v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Susobhan Bandopadhyay, Aritra Banik, Sushmita Gupta, Pallavi Jain, Abhishek Sahu, Saket Saurabh, Prafullkumar Tale
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the standard model of fair allocation of resources to agents, every agent
has some utility for every resource, and the goal is to assign resources to
agents so that the agents' welfare is maximized. Motivated by job scheduling,
interest in this problem dates back to the work of Deuermeyer et al. [SIAM J.
on Algebraic Discrete Methods'82]. Recent works consider the compatibility
between resources and assign only mutually compatible resources to an agent. We
study a fair allocation problem in which we are given a set of agents, a set of
resources, a utility function for every agent over a set of resources, and a
{\it conflict graph} on the set of resources (where an edge denotes
incompatibility). The goal is to assign resources to the agents such that $(i)$
the set of resources allocated to an agent are compatible with each other, and
$(ii)$ the minimum satisfaction of an agent is maximized, where the
satisfaction of an agent is the sum of the utility of the assigned resources.
Chiarelli et al. [Algorithmica'22] explore this problem from the classical
complexity perspective to draw the boundary between the cases that are
polynomial-time solvable and those that are \NP-hard. In this article, we study
the parameterized complexity of the problem (and its variants) by considering
several natural and structural parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2309.04995</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Large Language Models Replace Economic Choice Prediction Labs? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.17435v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.17435v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eilam Shapira, Omer Madmon, Roi Reichart, Moshe Tennenholtz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Economic choice prediction is an essential challenging task, often
constrained by the difficulties in acquiring human choice data. Indeed,
experimental economics studies had focused mostly on simple choice settings.
The AI community has recently contributed to that effort in two ways:
considering whether LLMs can substitute for humans in the above-mentioned
simple choice prediction settings, and the study through ML lens of more
elaborated but still rigorous experimental economics settings, employing
incomplete information, repetitive play, and natural language communication,
notably language-based persuasion games. This leaves us with a major
inspiration: can LLMs be used to fully simulate the economic environment and
generate data for efficient human choice prediction, substituting for the
elaborated economic lab studies? We pioneer the study of this subject,
demonstrating its feasibility. In particular, we show that a model trained
solely on LLM-generated data can effectively predict human behavior in a
language-based persuasion game, and can even outperform models trained on
actual human data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond discounted returns: Robust Markov decision processes with average
  and Blackwell optimality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.03618v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.03618v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julien Grand-Clement, Marek Petrik, Nicolas Vieille
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust Markov Decision Processes (RMDPs) are a widely used framework for
sequential decision-making under parameter uncertainty. RMDPs have been
extensively studied when the objective is to maximize the discounted return,
but little is known for average optimality (optimizing the long-run average of
the rewards obtained over time) and Blackwell optimality (remaining discount
optimal for all discount factors sufficiently close to 1). In this paper, we
prove several foundational results for RMDPs beyond the discounted return. We
show that average optimal policies can be chosen stationary and deterministic
for sa-rectangular RMDPs but, perhaps surprisingly, that history-dependent
(Markovian) policies strictly outperform stationary policies for average
optimality in s-rectangular RMDPs. We also study Blackwell optimality for
sa-rectangular RMDPs, where we show that {\em approximate} Blackwell optimal
policies always exist, although Blackwell optimal policies may not exist. We
also provide a sufficient condition for their existence, which encompasses
virtually any examples from the literature. We then discuss the connection
between average and Blackwell optimality, and we describe several algorithms to
compute the optimal average return. Interestingly, our approach leverages the
connections between RMDPs and stochastic games.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Stackelberg Strategies for Finitely Repeated <span class="highlight-title">Game</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.04192v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.04192v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Natalie Collina, Eshwar Ram Arunachaleswaran, Michael Kearns
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study Stackelberg equilibria in finitely repeated games, where the leader
commits to a strategy that picks actions in each round and can be adaptive to
the history of play (i.e. they commit to an algorithm). In particular, we study
static repeated games with no discounting. We give efficient algorithms for
finding approximate Stackelberg equilibria in this setting, along with rates of
convergence depending on the time horizon $T$. In many cases, these algorithms
allow the leader to do much better on average than they can in the single-round
Stackelberg. We give two algorithms, one computing strategies with an optimal
$\frac{1}{T}$ rate at the expense of an exponential dependence on the number of
actions, and another (randomized) approach computing strategies with no
dependence on the number of actions but a worse dependence on $T$ of
$\frac{1}{T^{0.25}}$. Both algorithms build upon a linear program to produce
simple automata leader strategies and induce corresponding automata
best-responses for the follower. We complement these results by showing that
approximating the Stackelberg value in three-player finite-horizon repeated
games is a computationally hard problem via a reduction from balanced vertex
cover.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Markov Property of Neural Algorithmic Reasoning: Analyses and
  Methods <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04929v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04929v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Montgomery Bohde, Meng Liu, Alexandra Saxton, Shuiwang Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural algorithmic reasoning is an emerging research direction that endows
neural networks with the ability to mimic algorithmic executions step-by-step.
A common paradigm in existing designs involves the use of historical embeddings
in predicting the results of future execution steps. Our observation in this
work is that such historical dependence intrinsically contradicts the Markov
nature of algorithmic reasoning tasks. Based on this motivation, we present our
ForgetNet, which does not use historical embeddings and thus is consistent with
the Markov nature of the tasks. To address challenges in training ForgetNet at
early stages, we further introduce G-ForgetNet, which uses a gating mechanism
to allow for the selective integration of historical embeddings. Such an
enhanced capability provides valuable computational pathways during the model's
early training phase. Our extensive experiments, based on the CLRS-30
algorithmic reasoning benchmark, demonstrate that both ForgetNet and
G-ForgetNet achieve better generalization capability than existing methods.
Furthermore, we investigate the behavior of the gating mechanism, highlighting
its degree of alignment with our intuitions and its effectiveness for robust
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at ICLR 2024 (Spotlight paper). 17 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lifelong Intelligence Beyond the Edge using Hyperdimensional Computing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofan Yu, Anthony Thomas, Ivannia Gomez Moreno, Louis Gutierrez, Tajana Rosing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  On-device learning has emerged as a prevailing trend that avoids the slow
response time and costly communication of cloud-based learning. The ability to
learn continuously and indefinitely in a changing environment, and with
resource constraints, is critical for real sensor deployments. However,
existing designs are inadequate for practical scenarios with (i) streaming data
input, (ii) lack of supervision and (iii) limited on-board resources. In this
paper, we design and deploy the first on-device lifelong learning system called
LifeHD for general IoT applications with limited supervision. LifeHD is
designed based on a novel neurally-inspired and lightweight learning paradigm
called Hyperdimensional Computing (HDC). We utilize a two-tier associative
memory organization to intelligently store and manage high-dimensional,
low-precision vectors, which represent the historical patterns as cluster
centroids. We additionally propose two variants of LifeHD to cope with scarce
labeled inputs and power constraints. We implement LifeHD on off-the-shelf edge
platforms and perform extensive evaluations across three scenarios. Our
measurements show that LifeHD improves the unsupervised clustering accuracy by
up to 74.8% compared to the state-of-the-art NN-based unsupervised lifelong
learning baselines with as much as 34.3x better energy efficiency. Our code is
available at https://github.com/Orienfish/LifeHD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IPSN'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Memetic Differential Evolution Methods for Semi-Supervised Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04322v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04322v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pierluigi Mansueto, Fabio Schoen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we deal with semi-supervised Minimum Sum-of-Squares Clustering
(MSSC) problems where background knowledge is given in the form of
instance-level constraints. In particular, we take into account "must-link" and
"cannot-link" constraints, each of which indicates if two dataset points should
be associated to the same or to a different cluster. The presence of such
constraints makes the problem at least as hard as its unsupervised version: it
is no more true that each point is associated to its nearest cluster center,
thus requiring some modifications in crucial operations, such as the assignment
step. In this scenario, we propose a novel memetic strategy based on the
Differential Evolution paradigm, directly extending a state-of-the-art
framework recently proposed in the unsupervised clustering literature. As far
as we know, our contribution represents the first attempt to define a memetic
methodology designed to generate a (hopefully) optimal feasible solution for
the semi-supervised MSSC problem. The proposal is compared with some
state-of-the-art algorithms from the literature on a set of well-known
datasets, highlighting its effectiveness and efficiency in finding good quality
clustering solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Noisy Spiking Actor Network for <span class="highlight-title">Exploration</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04162v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04162v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ding Chen, Peixi Peng, Tiejun Huang, Yonghong Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a general method for exploration in deep reinforcement learning (RL),
NoisyNet can produce problem-specific exploration strategies. Spiking neural
networks (SNNs), due to their binary firing mechanism, have strong robustness
to noise, making it difficult to realize efficient exploration with local
disturbances. To solve this exploration problem, we propose a noisy spiking
actor network (NoisySAN) that introduces time-correlated noise during charging
and transmission. Moreover, a noise reduction method is proposed to find a
stable policy for the agent. Extensive experimental results demonstrate that
our method outperforms the state-of-the-art performance on a wide range of
continuous control tasks from OpenAI gym.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep <span class="highlight-title">Reinforcement</span> Learning for Dynamic Algorithm <span class="highlight-title">Selection</span>: A
  Proof-of-Principle Study on Differential Evolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02131v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02131v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongshu Guo, Yining Ma, Zeyuan Ma, Jiacheng Chen, Xinglin Zhang, Zhiguang Cao, Jun Zhang, Yue-Jiao Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evolutionary algorithms, such as Differential Evolution, excel in solving
real-parameter optimization challenges. However, the effectiveness of a single
algorithm varies across different problem instances, necessitating considerable
efforts in algorithm selection or configuration. This paper aims to address the
limitation by leveraging the complementary strengths of a group of algorithms
and dynamically scheduling them throughout the optimization progress for
specific problems. We propose a deep reinforcement learning-based dynamic
algorithm selection framework to accomplish this task. Our approach models the
dynamic algorithm selection a Markov Decision Process, training an agent in a
policy gradient manner to select the most suitable algorithm according to the
features observed during the optimization process. To empower the agent with
the necessary information, our framework incorporates a thoughtful design of
landscape and algorithmic features. Meanwhile, we employ a sophisticated deep
neural network model to infer the optimal action, ensuring informed algorithm
selections. Additionally, an algorithm context restoration mechanism is
embedded to facilitate smooth switching among different algorithms. These
mechanisms together enable our framework to seamlessly select and switch
algorithms in a dynamic online fashion. Notably, the proposed framework is
simple and generic, offering potential improvements across a broad spectrum of
evolutionary algorithms. As a proof-of-principle study, we apply this framework
to a group of Differential Evolution algorithms. The experimental results
showcase the remarkable effectiveness of the proposed framework, not only
enhancing the overall optimization performance but also demonstrating favorable
generalization ability across different problem classes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Systems, Man, and Cybernetics:
  Systems at Thu, Feb 29, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spike Accumulation Forwarding for Effective Training of Spiking Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02772v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02772v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryuji Saiin, Tomoya Shirakawa, Sota Yoshihara, Yoshihide Sawada, Hiroyuki Kusumoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this article, we propose a new paradigm for training spiking neural
networks (SNNs), spike accumulation forwarding (SAF). It is known that SNNs are
energy-efficient but difficult to train. Consequently, many researchers have
proposed various methods to solve this problem, among which online training
through time (OTTT) is a method that allows inferring at each time step while
suppressing the memory cost. However, to compute efficiently on GPUs, OTTT
requires operations with spike trains and weighted summation of spike trains
during forwarding. In addition, OTTT has shown a relationship with the Spike
Representation, an alternative training method, though theoretical agreement
with Spike Representation has yet to be proven. Our proposed method can solve
these problems; namely, SAF can halve the number of operations during the
forward process, and it can be theoretically proven that SAF is consistent with
the Spike Representation and OTTT, respectively. Furthermore, we confirmed the
above contents through experiments and showed that it is possible to reduce
memory and training time while maintaining accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures, Appendix:8 pages, 2 figures, v5:We added
  experimental results and considered the situation the SNN have a feedforward
  or feedback connection</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-06T00:00:00Z">2024-03-06</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computer Science and Game Theory
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ To Spend or to Gain: Online Learning in Repeated Karma Auctions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04057v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04057v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Damien Berriaud, Ezzat Elokda, Devansh Jalota, Emilio Frazzoli, Marco Pavone, Florian Dörfler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have seen a surge of artificial currency-based mechanisms in
contexts where monetary instruments are deemed unfair or inappropriate, e.g.,
in allocating food donations to food banks, course seats to students, and, more
recently, even for traffic congestion management. Yet the applicability of
these mechanisms remains limited in repeated auction settings, as it is
challenging for users to learn how to bid an artificial currency that has no
value outside the auctions. Indeed, users must jointly learn the value of the
currency in addition to how to spend it optimally. In this work, we study the
problem of learning to bid in two prominent classes of artificial currency
auctions: those in which currency, which users spend to obtain public
resources, is only issued at the beginning of a finite period; and those where,
in addition to the initial currency endowment, currency payments are
redistributed to users at each time step. In the latter class, the currency has
been referred to as karma, since users do not only spend karma to obtain public
resources but also gain karma for yielding them. In both classes, we propose a
simple learning strategy, called adaptive karma pacing, and show that this
strategy a) is asymptotically optimal for a single user bidding against
competing bids drawn from a stationary distribution; b) leads to convergent
learning dynamics when all users adopt it; and c) constitutes an approximate
Nash equilibrium as the number of users grows. Our results require a novel
analysis in comparison to adaptive pacing strategies in monetary auctions,
since we depart from the classical assumption that the currency has known value
outside the auctions, and moreover consider that the currency is both spent and
gained in the class of auctions with redistribution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Manuscript submitted for review to the 25th ACM Conference on
  Economics & Computation (EC'24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Empirical <span class="highlight-title">Game</span>-Theoretic Analysis: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04018v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04018v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael P. Wellman, Karl Tuyls, Amy Greenwald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the empirical approach to game-theoretic analysis (EGTA), the model of the
game comes not from declarative representation, but is derived by interrogation
of a procedural description of the game environment. The motivation for
developing this approach was to enable game-theoretic reasoning about strategic
situations too complex for analytic specification and solution. Since its
introduction over twenty years ago, EGTA has been applied to a wide range of
multiagent domains, from auctions and markets to recreational games to
cyber-security. We survey the extensive methodology developed for EGTA over the
years, organized by the elemental subproblems comprising the EGTA process. We
describe key EGTA concepts and techniques, and the questions at the frontier of
EGTA research. Recent advances in machine learning have accelerated progress in
EGTA, and promise to significantly expand our capacities for reasoning about
complex game situations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>72 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fair Artificial Currency Incentives in Repeated Weighted Congestion
  <span class="highlight-title">Game</span>s: Equity vs. Equality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03999v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03999v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonardo Pedroso, Andrea Agazzi, W. P. M. H. Heemels, Mauro Salazar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When users access shared resources in a selfish manner, the resulting
societal cost and perceived users' cost is often higher than what would result
from a centrally coordinated optimal allocation. While several contributions in
mechanism design manage to steer the aggregate users choices to the desired
optimum by using monetary tolls, such approaches bear the inherent drawback of
discriminating against users with a lower income. More recently, incentive
schemes based on artificial currencies have been studied with the goal of
achieving a system-optimal resource allocation that is also fair. In this
resource-sharing context, this paper focuses on repeated weighted congestion
game with two resources, where users contribute to the congestion to different
extents that are captured by individual weights. First, we address the broad
concept of fairness by providing a rigorous mathematical characterization of
the distinct societal metrics of equity and equality, i.e., the concepts of
providing equal outcomes and equal opportunities, respectively. Second, we
devise weight-dependent and time-invariant optimal pricing policies to maximize
equity and equality, and prove convergence of the aggregate user choices to the
system-optimum. In our framework it is always possible to achieve
system-optimal allocations with perfect equity, while the maximum equality that
can be reached may not be perfect, which is also shown via numerical
simulations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Settling the Competition Complexity of Additive Buyers over Independent
  Items 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03937v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03937v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahsa Derakhshan, Emily Ryu, S. Matthew Weinberg, Eric Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The competition complexity of an auction setting is the number of additional
bidders needed such that the simple mechanism of selling items separately (with
additional bidders) achieves greater revenue than the optimal but complex
(randomized, prior-dependent, Bayesian-truthful) optimal mechanism without the
additional bidders. Our main result settles the competition complexity of $n$
bidders with additive values over $m < n$ independent items at
$\Theta(\sqrt{nm})$. The $O(\sqrt{nm})$ upper bound is due to [BW19], and our
main result improves the prior lower bound of $\Omega(\ln n)$ to
$\Omega(\sqrt{nm})$.
  Our main result follows from an explicit construction of a Bayesian IC
auction for $n$ bidders with additive values over $m<n$ independent items drawn
from the Equal Revenue curve truncated at $\sqrt{nm}$ ($\mathcal{ER}_{\le
\sqrt{nm}}$), which achieves revenue that exceeds
$\text{SRev}_{n+\sqrt{nm}}(\mathcal{ER}_{\le \sqrt{nm}}^m)$.
  Along the way, we show that the competition complexity of $n$ bidders with
additive values over $m$ independent items is exactly equal to the minimum $c$
such that $\text{SRev}_{n+c}(\mathcal{ER}_{\le p}^m) \geq
\text{Rev}_n(\mathcal{ER}_{\le p}^m)$ for all $p$ (that is, some truncated
Equal Revenue witnesses the worst-case competition complexity). Interestingly,
we also show that the untruncated Equal Revenue curve does not witness the
worst-case competition complexity when $n > m$: $\text{SRev}_n(\mathcal{ER}^m)
= nm+O_m(\ln (n)) \leq \text{SRev}_{n+O_m(\ln (n))}(\mathcal{ER}^m)$, and
therefore our result can only follow by considering all possible truncations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>50 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Incentivized Learning in Principal-<span class="highlight-title">Agent</span> Bandit <span class="highlight-title">Game</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03811v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03811v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antoine Scheid, Daniil Tiapkin, Etienne Boursier, Aymeric Capitaine, El Mahdi El Mhamdi, Eric Moulines, Michael I. Jordan, Alain Durmus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work considers a repeated principal-agent bandit game, where the
principal can only interact with her environment through the agent. The
principal and the agent have misaligned objectives and the choice of action is
only left to the agent. However, the principal can influence the agent's
decisions by offering incentives which add up to his rewards. The principal
aims to iteratively learn an incentive policy to maximize her own total
utility. This framework extends usual bandit problems and is motivated by
several practical applications, such as healthcare or ecological taxation,
where traditionally used mechanism design theories often overlook the learning
aspect of the problem. We present nearly optimal (with respect to a horizon
$T$) learning algorithms for the principal's regret in both multi-armed and
linear contextual settings. Finally, we support our theoretical guarantees
through numerical experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ To Trust or Not to Trust: Assignment Mechanisms with Predictions in the
  Private Graph Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03725v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03725v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riccardo Colini-Baldeschi, Sophie Klumper, Guido Schäfer, Artem Tsikiridis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The realm of algorithms with predictions has led to the development of
several new algorithms that leverage (potentially erroneous) predictions to
enhance their performance guarantees. The challenge is to devise algorithms
that achieve optimal approximation guarantees as the prediction quality varies
from perfect (consistency) to imperfect (robustness). This framework is
particularly appealing in mechanism design contexts, where predictions might
convey private information about the agents. In this paper, we design
strategyproof mechanisms that leverage predictions to achieve improved
approximation guarantees for several variants of the Generalized Assignment
Problem (GAP) in the private graph model. In this model, first introduced by
Dughmi & Ghosh (2010), the set of resources that an agent is compatible with is
private information. For the Bipartite Matching Problem (BMP), we give a
deterministic group-strategyproof (GSP) mechanism that is $(1
+1/\gamma)$-consistent and $(1 + \gamma)$-robust, where $\gamma \ge 1$ is some
confidence parameter. We also prove that this is best possible. Remarkably, our
mechanism draws inspiration from the renowned Gale-Shapley algorithm,
incorporating predictions as a crucial element. Additionally, we give a
randomized mechanism that is universally GSP and improves on the guarantees in
expectation. The other GAP variants that we consider all make use of a unified
greedy mechanism that adds edges to the assignment according to a specific
order. Our universally GSP mechanism randomizes over the greedy mechanism, our
mechanism for BMP and the predicted assignment, leading to
$(1+3/\gamma)$-consistency and $(3+\gamma)$-robustness in expectation. All our
mechanisms also provide more fine-grained approximation guarantees that
interpolate between the consistency and the robustness, depending on some
natural error measure of the prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Anonymous and Copy-Robust Delegations for Liquid Democracy <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.01174v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.01174v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Markus Utke, Ulrike Schmidt-Kraepelin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Liquid democracy with ranked delegations is a novel voting scheme that unites
the practicability of representative democracy with the idealistic appeal of
direct democracy: Every voter decides between casting their vote on a question
at hand or delegating their voting weight to some other, trusted agent.
Delegations are transitive, and since voters may end up in a delegation cycle,
they are encouraged to indicate not only a single delegate, but a set of
potential delegates and a ranking among them. Based on the delegation
preferences of all voters, a delegation rule selects one representative per
voter. Previous work has revealed a trade-off between two properties of
delegation rules called anonymity and copy-robustness.
  To overcome this issue we study two fractional delegation rules: Mixed Borda
branching, which generalizes a rule satisfying copy-robustness, and the random
walk rule, which satisfies anonymity. Using the Markov chain tree theorem, we
show that the two rules are in fact equivalent, and simultaneously satisfy
generalized versions of the two properties. Combining the same theorem with
Fulkerson's algorithm, we develop a polynomial-time algorithm for computing the
outcome of the studied delegation rule. This algorithm is of independent
interest, having applications in semi-supervised learning and graph theory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Incentive Mechanism for Uncertain Tasks under Differential Privacy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16793v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16793v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xikun Jiang, Chenhao Ying, Lei Li, Boris Düdder, Haiqin Wu, Haiming Jin, Yuan Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mobile crowd sensing (MCS) has emerged as an increasingly popular sensing
paradigm due to its cost-effectiveness. This approach relies on platforms to
outsource tasks to participating workers when prompted by task publishers.
Although incentive mechanisms have been devised to foster widespread
participation in MCS, most of them focus only on static tasks (i.e., tasks for
which the timing and type are known in advance) and do not protect the privacy
of worker bids. In a dynamic and resource-constrained environment, tasks are
often uncertain (i.e., the platform lacks a priori knowledge about the tasks)
and worker bids may be vulnerable to inference attacks. This paper presents
HERALD*, an incentive mechanism that addresses these issues through the use of
uncertainty and hidden bids. Theoretical analysis reveals that HERALD*
satisfies a range of critical criteria, including truthfulness, individual
rationality, differential privacy, low computational complexity, and low social
cost. These properties are then corroborated through a series of evaluations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal Capacity Modification for Many-To-One Matching Problems <span class="chip">AAMAS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.01815v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.01815v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiehua Chen, Gergely Csáji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider many-to-one matching problems, where one side consists of
students and the other side of schools with capacity constraints. We study how
to optimally increase the capacities of the schools so as to obtain a stable
and perfect matching (i.e., every student is matched) or a matching that is
stable and Pareto-efficient for the students. We consider two common optimality
criteria, one aiming to minimize the sum of capacity increases of all schools
(abbrv. as MinSum) and the other aiming to minimize the maximum capacity
increase of any school (abbrv. as MinMax). We obtain a complete picture in
terms of computational complexity: Except for stable and perfect matchings
using the MinMax criteria which is polynomial-time solvable, all three
remaining problems are NP-hard. We further investigate the parameterized
complexity and approximability and find that achieving stable and
Pareto-efficient matchings via minimal capacity increases is much harder than
achieving stable and perfect matchings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended abstract accepted at AAMAS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ordinal Potential-based Player Rating 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05366v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05366v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nelson Vadori, Rahul Savani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It was recently observed that Elo ratings fail at preserving transitive
relations among strategies and therefore cannot correctly extract the
transitive component of a game. We provide a characterization of transitive
games as a weak variant of ordinal potential games and show that Elo ratings
actually do preserve transitivity when computed in the right space, using
suitable invertible mappings. Leveraging this insight, we introduce a new game
decomposition of an arbitrary game into transitive and cyclic components that
is learnt using a neural network-based architecture and that prioritises
capturing the sign pattern of the game, namely transitive and cyclic relations
among strategies. We link our approach to the known concept of sign-rank, and
evaluate our methodology using both toy examples and empirical data from
real-world games.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Guided Automated Reasoning: A Brief <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04017v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04017v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lasse Blaauwbroek, David Cerna, Thibault Gauthier, Jan Jakubův, Cezary Kaliszyk, Martin Suda, Josef Urban
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated theorem provers and formal proof assistants are general reasoning
systems that are in theory capable of proving arbitrarily hard theorems, thus
solving arbitrary problems reducible to mathematics and logical reasoning. In
practice, such systems however face large combinatorial explosion, and
therefore include many heuristics and choice points that considerably influence
their performance. This is an opportunity for trained machine learning
predictors, which can guide the work of such reasoning systems. Conversely,
deductive search supported by the notion of logically valid proof allows one to
train machine learning systems on large reasoning corpora. Such bodies of proof
are usually correct by construction and when combined with more and more
precise trained guidance they can be boostrapped into very large corpora, with
increasingly long reasoning chains and possibly novel proof ideas. In this
paper we provide an overview of several automated reasoning and theorem proving
domains and the learning and AI methods that have been so far developed for
them. These include premise selection, proof guidance in several settings, AI
systems and feedback loops iterating between reasoning and learning, and
symbolic classification problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Architecture Search using Particle Swarm and Ant Colony
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03781v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03781v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Séamus Lankford, Diarmuid Grimes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural network models have a number of hyperparameters that must be chosen
along with their architecture. This can be a heavy burden on a novice user,
choosing which architecture and what values to assign to parameters. In most
cases, default hyperparameters and architectures are used. Significant
improvements to model accuracy can be achieved through the evaluation of
multiple architectures. A process known as Neural Architecture Search (NAS) may
be applied to automatically evaluate a large number of such architectures. A
system integrating open source tools for Neural Architecture Search (OpenNAS),
in the classification of images, has been developed as part of this research.
OpenNAS takes any dataset of grayscale, or RBG images, and generates
Convolutional Neural Network (CNN) architectures based on a range of
metaheuristics using either an AutoKeras, a transfer learning or a Swarm
Intelligence (SI) approach. Particle Swarm Optimization (PSO) and Ant Colony
Optimization (ACO) are used as the SI algorithms. Furthermore, models developed
through such metaheuristics may be combined using stacking ensembles. In the
context of this paper, we focus on training and optimizing CNNs using the Swarm
Intelligence (SI) components of OpenNAS. Two major types of SI algorithms,
namely PSO and ACO, are compared to see which is more effective in generating
higher model accuracies. It is shown, with our experimental design, that the
PSO algorithm performs better than ACO. The performance improvement of PSO is
most notable with a more complex dataset. As a baseline, the performance of
fine-tuned pre-trained models is also evaluated.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Illuminating the property space in crystal structure prediction using
  Quality-Diversity algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03511v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03511v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marta Wolinska, Aron Walsh, Antoine Cully
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The identification of materials with exceptional properties is an essential
objective to enable technological progress. We propose the application of
\textit{Quality-Diversity} algorithms to the field of crystal structure
prediction. The objective of these algorithms is to identify a diverse set of
high-performing solutions, which has been successful in a range of fields such
as robotics, architecture and aeronautical engineering. As these methods rely
on a high number of evaluations, we employ machine-learning surrogate models to
compute the interatomic potential and material properties that are used to
guide optimisation. Consequently, we also show the value of using neural
networks to model crystal properties and enable the identification of novel
composition--structure combinations. In this work, we specifically study the
application of the MAP-Elites algorithm to predict polymorphs of TiO$_2$. We
rediscover the known ground state, in addition to a set of other polymorphs
with distinct properties. We validate our method for C, SiO$_2$ and SiC
systems, where we show that the algorithm can uncover multiple local minima
with distinct electronic and mechanical properties.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparse Spiking Neural Network: Exploiting Heterogeneity in Timescales
  for Pruning Recurrent SNN <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03409v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03409v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Biswadeep Chakraborty, Beomseok Kang, Harshit Kumar, Saibal Mukhopadhyay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recurrent Spiking Neural Networks (RSNNs) have emerged as a computationally
efficient and brain-inspired learning model. The design of sparse RSNNs with
fewer neurons and synapses helps reduce the computational complexity of RSNNs.
Traditionally, sparse SNNs are obtained by first training a dense and complex
SNN for a target task, and, then, pruning neurons with low activity
(activity-based pruning) while maintaining task performance. In contrast, this
paper presents a task-agnostic methodology for designing sparse RSNNs by
pruning a large randomly initialized model. We introduce a novel Lyapunov Noise
Pruning (LNP) algorithm that uses graph sparsification methods and utilizes
Lyapunov exponents to design a stable sparse RSNN from a randomly initialized
RSNN. We show that the LNP can leverage diversity in neuronal timescales to
design a sparse Heterogeneous RSNN (HRSNN). Further, we show that the same
sparse HRSNN model can be trained for different tasks, such as image
classification and temporal prediction. We experimentally show that, in spite
of being task-agnostic, LNP increases computational efficiency (fewer neurons
and synapses) and prediction performance of RSNNs compared to traditional
activity-based pruning of trained dense models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explaining Genetic Programming Trees using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03397v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03397v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paula Maddigan, Andrew Lensen, Bing Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Genetic programming (GP) has the potential to generate explainable results,
especially when used for dimensionality reduction. In this research, we
investigate the potential of leveraging eXplainable AI (XAI) and large language
models (LLMs) like ChatGPT to improve the interpretability of GP-based
non-linear dimensionality reduction. Our study introduces a novel XAI dashboard
named GP4NLDR, the first approach to combine state-of-the-art GP with an
LLM-powered chatbot to provide comprehensive, user-centred explanations. We
showcase the system's ability to provide intuitive and insightful narratives on
high-dimensional data reduction processes through case studies. Our study
highlights the importance of prompt engineering in eliciting accurate and
pertinent responses from LLMs. We also address important considerations around
data privacy, hallucinatory outputs, and the rapid advancements in generative
AI. Our findings demonstrate its potential in advancing the explainability of
GP algorithms. This opens the door for future research into explaining GP
models with LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Two-step interpretable modeling of Intensive Care Acquired Infections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.11146v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.11146v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giacomo Lancia, Meri Varkila, Olaf Cremer, Cristian Spitoni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel methodology for integrating high resolution longitudinal
data with the dynamic prediction capabilities of survival models. The aim is
two-fold: to improve the predictive power while maintaining interpretability of
the models. To go beyond the black box paradigm of artificial neural networks,
we propose a parsimonious and robust semi-parametric approach (i.e., a
landmarking competing risks model) that combines routinely collected
low-resolution data with predictive features extracted from a convolutional
neural network, that was trained on high resolution time-dependent information.
We then use saliency maps to analyze and explain the extra predictive power of
this model. To illustrate our methodology, we focus on healthcare-associated
infections in patients admitted to an intensive care unit.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Prefrontal Cortex-inspired Architecture for Planning in Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00194v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00194v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taylor Webb, Shanka Subhra Mondal, Chi Wang, Brian Krabach, Ida Momennejad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) demonstrate impressive performance on a wide
variety of tasks, but they often struggle with tasks that require multi-step
reasoning or goal-directed planning. To address this, we take inspiration from
the human brain, in which planning is accomplished via the recurrent
interaction of specialized modules in the prefrontal cortex (PFC). These
modules perform functions such as conflict monitoring, state prediction, state
evaluation, task decomposition, and task coordination. We find that LLMs are
sometimes capable of carrying out these functions in isolation, but struggle to
autonomously coordinate them in the service of a goal. Therefore, we propose a
black box architecture with multiple LLM-based (GPT-4) modules. The
architecture improves planning through the interaction of specialized
PFC-inspired modules that break down a larger problem into multiple brief
automated calls to the LLM. We evaluate the combined architecture on three
challenging planning tasks -- graph traversal, Tower of Hanoi, and logistics --
finding that it yields significant improvements over standard LLM methods
(e.g., zero-shot prompting, in-context learning, and chain-of-thought). These
results demonstrate the benefit of utilizing knowledge from cognitive
neuroscience to improve planning in LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-05T00:00:00Z">2024-03-05</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Linear Codes for Hyperdimensional Computing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Netanel Raviv
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperdimensional Computing (HDC) is an emerging computational paradigm for
representing compositional information as high-dimensional vectors, and has a
promising potential in applications ranging from machine learning to
neuromorphic computing. One of the long-standing challenges in HDC is factoring
a compositional representation to its constituent factors, also known as the
recovery problem. In this paper we take a novel approach to solve the recovery
problem, and propose the use of random linear codes. These codes are subspaces
over the Boolean field, and are a well-studied topic in information theory with
various applications in digital communication. We begin by showing that
hyperdimensional encoding using random linear codes retains favorable
properties of the prevalent (ordinary) random codes, and hence HD
representations using the two methods have comparable information storage
capabilities. We proceed to show that random linear codes offer a rich subcode
structure that can be used to form key-value stores, which encapsulate most use
cases of HDC. Most importantly, we show that under the framework we develop,
random linear codes admit simple recovery algorithms to factor (either bundled
or bound) compositional representations. The former relies on constructing
certain linear equation systems over the Boolean field, the solution to which
reduces the search space dramatically and strictly outperforms exhaustive
search in many cases. The latter employs the subspace structure of these codes
to achieve provably correct factorization. Both methods are strictly faster
than the state-of-the-art resonator networks, often by an order of magnitude.
We implemented our techniques in Python using a benchmark software library, and
demonstrated promising experimental results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Author's final version. The article has been accepted for publication
  in Neural Computation (MIT press)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mem-elements based Neuromorphic Hardware for Neural Network Application 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03002v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03002v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ankur Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The thesis investigates the utilization of memristive and memcapacitive
crossbar arrays in low-power machine learning accelerators, offering a
comprehensive co-design framework for deep neural networks (DNN). The model,
implemented through a hybrid Python and PyTorch approach, accounts for various
non-idealities, achieving exceptional training accuracies of 90.02% and 91.03%
for the CIFAR-10 dataset with memristive and memcapacitive crossbar arrays on
an 8-layer VGG network. Additionally, the thesis introduces a novel approach to
emulate meminductor devices using Operational Transconductance Amplifiers (OTA)
and capacitors, showcasing adjustable behavior. Transistor-level simulations in
180 nm CMOS technology, operating at 60 MHz, demonstrate the proposed
meminductor emulator's viability with a power consumption of 0.337 mW. The
design is further validated in neuromorphic circuits and CNN accelerators,
achieving training and testing accuracies of 91.04% and 88.82%, respectively.
Notably, the exclusive use of MOS transistors ensures the feasibility of
monolithic IC fabrication. This research significantly contributes to the
exploration of advanced hardware solutions for efficient and high-performance
machine-learning applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Master's Thesis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evolution Transformer: In-Context Evolutionary Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02985v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02985v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Tjarko Lange, Yingtao Tian, Yujin Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evolutionary optimization algorithms are often derived from loose biological
analogies and struggle to leverage information obtained during the sequential
course of optimization. An alternative promising approach is to leverage data
and directly discover powerful optimization principles via meta-optimization.
In this work, we follow such a paradigm and introduce Evolution Transformer, a
causal Transformer architecture, which can flexibly characterize a family of
Evolution Strategies. Given a trajectory of evaluations and search distribution
statistics, Evolution Transformer outputs a performance-improving update to the
search distribution. The architecture imposes a set of suitable inductive
biases, i.e. the invariance of the distribution update to the order of
population members within a generation and equivariance to the order of the
search dimensions. We train the model weights using Evolutionary Algorithm
Distillation, a technique for supervised optimization of sequence models using
teacher algorithm trajectories. The resulting model exhibits strong in-context
optimization performance and shows strong generalization capabilities to
otherwise challenging neuroevolution tasks. We analyze the resulting properties
of the Evolution Transformer and propose a technique to fully
self-referentially train the Evolution Transformer, starting from a random
initialization and bootstrapping its own learning progress. We provide an open
source implementation under https://github.com/RobertTLange/evosax.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SOFIM: Stochastic Optimization Using Regularized Fisher Information
  Matrix 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02833v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02833v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gayathri C, Mrinmay Sen, A. K. Qin, Raghu Kishore N, Yen-Wei Chen, Balasubramanian Raman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a new stochastic optimization method based on the
regularized Fisher information matrix (FIM), named SOFIM, which can efficiently
utilize the FIM to approximate the Hessian matrix for finding Newton's gradient
update in large-scale stochastic optimization of machine learning models. It
can be viewed as a variant of natural gradient descent (NGD), where the
challenge of storing and calculating the full FIM is addressed through making
use of the regularized FIM and directly finding the gradient update direction
via Sherman-Morrison matrix inversion. Additionally, like the popular Adam
method, SOFIM uses the first moment of the gradient to address the issue of
non-stationary objectives across mini-batches due to heterogeneous data. The
utilization of the regularized FIM and Sherman-Morrison matrix inversion leads
to the improved convergence rate with the same space and time complexities as
stochastic gradient descent (SGD) with momentum. The extensive experiments on
training deep learning models on several benchmark image classification
datasets demonstrate that the proposed SOFIM outperforms SGD with momentum and
several state-of-the-art Newton optimization methods, such as Nystrom-SGD,
L-BFGS, and AdaHessian, in term of the convergence speed for achieving the
pre-specified objectives of training and test losses as well as test accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Control System for Continuous Glucose Monitoring and Maintenance <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13852v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13852v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Azmine Toushik Wasi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precise glucose level monitoring is critical for people with diabetes to
avoid serious complications. While there are several methods for continuous
glucose level monitoring, research on maintenance devices is limited. To
mitigate the gap, we provide a novel neural control system for continuous
glucose monitoring and management that uses differential predictive control.
Our approach, led by a sophisticated neural policy and differentiable modeling,
constantly adjusts insulin supply in real-time, thereby improving glucose level
optimization in the body. This end-to-end method maximizes efficiency,
providing personalized care and improved health outcomes, as confirmed by
empirical evidence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 Pages, 4 figures, ICLR 2024 Tiny Papers Track
  https://openreview.net/forum?id=Te4P3Cn54g</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLaMoCo: Instruction Tuning of Large Language Models for Optimization
  Code Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01131v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01131v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyuan Ma, Hongshu Guo, Jiacheng Chen, Guojun Peng, Zhiguang Cao, Yining Ma, Yue-Jiao Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research explores optimization using large language models (LLMs) by
either iteratively seeking next-step solutions from LLMs or directly prompting
LLMs for an optimizer. However, these approaches exhibit inherent limitations,
including low operational efficiency, high sensitivity to prompt design, and a
lack of domain-specific knowledge. We introduce LLaMoCo, the first
instruction-tuning framework designed to adapt LLMs for solving optimization
problems in a code-to-code manner. Specifically, we establish a comprehensive
instruction set containing well-described problem prompts and effective
optimization codes. We then develop a novel two-phase learning strategy that
incorporates a contrastive learning-based warm-up procedure before the
instruction-tuning phase to enhance the convergence behavior during model
fine-tuning. The experiment results demonstrate that a CodeGen (350M) model
fine-tuned by our LLaMoCo achieves superior optimization performance compared
to GPT-4 Turbo and the other competitors across both synthetic and realistic
problem sets. The fine-tuned model and the usage instructions are available at
https://anonymous.4open.science/r/LLaMoCo-722A.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Symbolic Regression <span class="highlight-title">Dataset</span>s and Benchmarks for Scientific
  Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.10540v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.10540v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoshitomo Matsubara, Naoya Chiba, Ryo Igarashi, Yoshitaka Ushiku
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper revisits datasets and evaluation criteria for Symbolic Regression
(SR), specifically focused on its potential for scientific discovery. Focused
on a set of formulas used in the existing datasets based on Feynman Lectures on
Physics, we recreate 120 datasets to discuss the performance of symbolic
regression for scientific discovery (SRSD). For each of the 120 SRSD datasets,
we carefully review the properties of the formula and its variables to design
reasonably realistic sampling ranges of values so that our new SRSD datasets
can be used for evaluating the potential of SRSD such as whether or not an SR
method can (re)discover physical laws from such datasets. We also create
another 120 datasets that contain dummy variables to examine whether SR methods
can choose necessary variables only. Besides, we propose to use normalized edit
distances (NED) between a predicted equation and the true equation trees for
addressing a critical issue that existing SR metrics are either binary or
errors between the target values and an SR model's predicted values for a given
input. We conduct benchmark experiments on our new SRSD datasets using various
representative SR methods. The experimental results show that we provide a more
realistic performance evaluation, and our user study shows that the NED
correlates with human judges significantly more than an existing SR metric. We
publish repositories of our code and 240 SRSD datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at DMLR. Code and datasets are available at
  https://github.com/omron-sinicx/srsd-benchmark
  https://huggingface.co/datasets/yoshitomo-matsubara/srsd-feynman_easy
  https://huggingface.co/datasets/yoshitomo-matsubara/srsd-feynman_medium
  https://huggingface.co/datasets/yoshitomo-matsubara/srsd-feynman_hard and
  another three sets of SRSD datasets with dummy variables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Unified Framework for Exploratory Learning-Aided Community Detection
  Under Topological Uncertainty <span class="chip">CIKM 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.04497v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.04497v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Hou, Cong Tran, Ming Li, Won-Yong Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In social networks, the discovery of community structures has received
considerable attention as a fundamental problem in various network analysis
tasks. However, due to privacy concerns or access restrictions, the network
structure is often uncertain, thereby rendering established community detection
approaches ineffective without costly network topology acquisition. To tackle
this challenge, we present META-CODE, a unified framework for detecting
overlapping communities via exploratory learning aided by easy-to-collect node
metadata when networks are topologically unknown (or only partially known).
Specifically, META-CODE consists of three iterative steps in addition to the
initial network inference step: 1) node-level community-affiliation embeddings
based on graph neural networks (GNNs) trained by our new reconstruction loss,
2) network exploration via community-affiliation-based node queries, and 3)
network inference using an edge connectivity-based Siamese neural network model
from the explored network. Through extensive experiments on five real-world
datasets including two large networks, we demonstrated: (a) the superiority of
META-CODE over benchmark community detection methods, achieving remarkable
gains up to 151.27% compared to the best existing competitor, (b) the impact of
each module in META-CODE, (c) the effectiveness of node queries in META-CODE
based on empirical evaluations and theoretical findings, (d) the convergence of
the inferred network, and (e) the computational efficiency of META-CODE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 9 figures, 6 tables; its conference version was presented
  at the ACM International Conference on Information and Knowledge Management
  (CIKM 2022)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2024-03-28T05:23:53.770447748Z">
            2024-03-28 05:23:53 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
